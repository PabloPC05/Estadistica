\section{Reducción de Datos}


\begin{definición}[Estadístico suficiente]
	Sea $(\Omega, \mathcal{A}, \mathcal{P})$ el espacio probabilístico asociado a un experimento aleatorio $\mathcal{E}$, una variable aleatoria observable $X: \Omega \longrightarrow \mathbb{R}$ y su modelo estadístico asociado $\left(\chi, \mathcal{B}, F_{\theta}\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}}, \mathcal{B}=\mathcal{B}(\mathbb{R})$, y $\left(X_{1}, \cdots X_{n}\right)$ m.a.s. $(n) \sim X$\\
	$T: \mathbb{R}^n \to \mathbb{R}^m$ es un estadístico suficiente para $\theta$ cuando $\forall (x_1, \ldots, x_n) \in \Omega$ se cumple que: 
	$$P(X_1 = x_1, \ldots, X_n = x_n | T = t) \text{	 no depende de } \theta \quad \forall t \in \mathbb{R}^m$$
\end{definición}


\ejemplo{
	Sabiendo que $X \sim Bin(1, \theta)$, veamos si se cumple que el estadístico $T = \sum_{i = 1}^{n} X_i$ es suficiente para $\theta$\\
	Tenemos que:
	$$X_i \sim Bin(1, \theta) \implies T = \sum_{i = 1}^n X_i \sim Bin(n, \theta)$$
	Para saber si un estadístico es suficiente necesitamos además la funcion de distribucion de la muestra, que en este caso: 
	$$X_i \sim Bin(1, \theta) \equiv Bernoulli(\theta)$$ $$\implies P(X_1 = x_1, \ldots, X_n = x_n) = \prod_{i = 1}^{n} \theta^{x_i} (1 - \theta)^{1 - x_i} = \theta^{\sum_{i = 1}^{n} x_i} (1 - \theta)^{n - \sum_{i = 1}^{n} x_i} \prod_{i = 1}^{n}$$
	Para ver si es suficiente, necesitamos calcular la probabilidad condicionada de la muestra dado el estadístico, que en este caso es:
	$$P(X_1 = x_1, \ldots, X_n = x_n | \sum_{i = 1}^{n} X_i = t) = \frac{P(X_1 = x_1, \ldots, X_n = x_n)}{P(\sum_{i = 1}^{n} X_i = t)} = \frac{\theta^{\sum_{i = 1}^{n} x_i} (1 - \theta)^{n - \sum_{i = 1}^{n} x_i}}{\binom{n}{t} \theta^t (1 - \theta)^{n - t}} = \frac{1}{\binom{n}{t}}$$
}
  
\subsection{Teorema de Factorización de Fisher}
  
\begin{teorema}[de Factorización de Fisher (Caracterización de estadísticos suficientes)]
	$T=T\left(X_{1}, \cdots X_{n}\right): \mathbb{R}^{n} \longrightarrow \mathbb{R}^{m}$ es un estadístico suficiente para $\theta$ sí y sólo sí existen funciones reales positivas $h: \mathbb{R}^{n} \longrightarrow \mathbb{R}$ y $g_{\theta}: \mathbb{R}^{m} \longrightarrow \mathbb{R}$ tales que $f_{\theta}\left(x_{1}, \ldots, x_{n}\right)=h\left(x_{1}, \ldots, x_{n}\right) g_{\theta}\left(T\left(x_{1}, \ldots, x_{n}\right)\right)$, donde $f_{\theta}\left(x_{1}, \ldots, x_{n}\right)$ es la función de densidad o de masa de la muestra
\end{teorema}
  
  
\begin{proof}
	Esta demostración se desarrolla tomando el caso en el que $X$ sea una v.a. discreta. 
	\begin{itemize}
		\item $(\Rightarrow):$ Supongamos que $T$ es suficiente, entonces como la distribución de la muestra condicionada al estadístico no depende de $\theta$, podemos escribir:
		      $$ f_{\theta}(x_1, \ldots, x_n | t) = \frac{f_{\theta}(x_1, \ldots, x_n)}{f_{\theta}(t)} \text{es independiente de } \theta$$
		      $$\implies f_{\theta}(x_1, \ldots, x_n) = f_{\theta}(t) \cdot f_{\theta}(x_1, \ldots, x_n | t) \implies$$
		      Simplemente tomamos $h(x_1, \ldots, x_n) = f_{\theta}(t)$ y $g_{\theta}(t) = f_{\theta}(x_1, \ldots, x_n | t)$
		\item $(\Leftarrow)$: Supongamos que $f_{\theta}(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) \cdot g_{\theta}(T(x_1, \ldots, x_n))$, entonces:\\
		      $$ f_{\theta}(x, \ldots, x_n | t = T(x_1, \ldots, x_n)) = \frac{f_{\theta}(x_1, \ldots, x_n)}{f_{\theta}(t)} = \frac{f_{\theta}(x_1, \ldots, x_n)}{\sum_{(y_1, \ldots, y_m) : T(y_1, \ldots, y_n) = t} f_{\theta}(y_1, \ldots, y_n)} = $$ $$ = \frac{h(x_1, \ldots, x_n) \cdot g_{\theta}(t)}{\sum_{(y_1, \ldots, y_m) : T(y_1, \ldots, y_n) = t} h(y_1, \ldots, y_n) \cdot g_{\theta}(t)} = \frac{h(x_1, \ldots, x_n)}{\sum_{(y_1, \ldots, y_m) : T(y_1, \ldots, y_n) = t} h(y_1, \ldots, y_n)}$$ La cual es una expresión no dependiente de $\theta$, por lo que $T$ es suficiente.
	\end{itemize}
\end{proof}

\begin{proposición}
	Si $T$ es suficiente para $\theta$ y $S$ es una biyección, entonces $S(T)$ es suficiente para $\theta$
\end{proposición}

\begin{proposición}
	Si $T$ es suficiente para $\theta$ y $S$ es una función medible (en estadística-integrable), entonces $S(T)$ es suficiente para $\theta$
\end{proposición}

\begin{proposición}
	Sea $X$ variable aleatoria, con $\sigma$ y $\delta$ parámetros, entonces si $T_1$ es suficiente para $\sigma$ y $T_2$ es suficiente para $\delta$ $\iff$ $T = (T_1, T_2)$ es suficiente para $(\sigma, \delta)$
\end{proposición}

\subsection{Ejemplos de Factorización de Fisher}

\ejemplo{
	Veamos si $\theta$ es un parámetro suficiente para el ejemplo anterior, utilizando la Factorización de Fisher.\\
	Como $X \sim \text{Bin}(1, \theta) \equiv \text{Bernoulli}(\theta)$, tenemos que la función de verosimilitud es:
	\[
		f_{\theta}(x_1, \ldots, x_n) = \prod_{i=1}^{n} \theta^{x_i} (1 - \theta)^{1 - x_i} \cdot I_{\{0,1\}}(x_i),
	\]
	donde $I_{\{0,1\}}(x_i)$ es la función indicadora que asegura que $x_i \in \{0, 1\}$. Esto se puede escribir como:
	\[
		f_{\theta}(x_1, \ldots, x_n) = \theta^{\sum_{i=1}^{n} x_i} (1 - \theta)^{n - \sum_{i=1}^{n} x_i} \prod_{i=1}^{n} I_{\{0,1\}}(x_i).
	\]
	Por lo que tomando $h(x_1, \ldots, x_n) = \prod_{i = 1}^{n} I_{\{0,1\}}(x_i)$ y $g_{\theta}(T(x_1, \ldots, x_n)) = \theta^t (1 - \theta)^{n - t}$, tenemos que $T = \sum_{i=1}^{n} X_i$ es un estadístico suficiente para $\theta$.
}

\ejemplo{
	Sea $X \sim Poisson(\theta)$ veamos si el estadístico $T = \sum_{i = 1}^{n} X_i$ es suficiente para $\theta$\\
	Calculemos primero la función de densidad asociada a la muestra: 
	$$X_i \sim Poisson(\theta) \implies f_{x_i}(x_i) = \frac{e^{-\theta} \theta^{x_i}}{x_i!} \implies f_{\theta}(x_1, \ldots, x_n) = \prod_{i = 1}^{n} \frac{e^{-\theta} \theta^{x_i}}{x_i!} = e^{-n\theta} \theta^{\sum_{i = 1}^{n} x_i} \prod_{i = 1}^{n} \frac{1}{x_i!}$$
	Por lo que tomando $h(x_1, \ldots, x_n) = \prod_{i = 1}^{n} \frac{1}{x_i!}$ y $g_{\theta}(T(x_1, \ldots, x_n)) = e^{-n\theta} \theta^t$ demostramos que $T(x_1, \ldots, x_n) = \sum_{i = 1}^{n}$ es un estadístico suficiente para $\theta$
}

\ejemplo{
	Supongamos que tenemos un estadístico $T$ suficiente para el parámetro $\theta$ y una biyección $S$, demostremos que $S(T)$ también es suficiente para $\theta$\\
	Por el Teorema de Caracterización de Fisher tenemos que: 
	$$f_{\theta}(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) \cdot g_{\theta}(T(x_1, \ldots, x_n))$$ para alguna función $h$ y $g_{\theta}$\\ 
	$$\iff f_{\theta}(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) \cdot g_{\theta}(S^{-1}(S(T(x_1, \ldots, x_n))))$$ $$ \iff f_{\theta}(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) \cdot g'_{\theta}(S(T(x_1, \ldots, x_n))) \implies$$
	Entones por el Teorema de Caracterización de Fisher, $S(T)$ es suficiente para $\theta$
}

\ejemplo{
	Veamos si $T = \bar{X} = \sum_{i = 1}^{n}X_i$ es suficiente para $\mu$ si $X \sim N(\mu, \sigma_0)$ con $\sigma_0$ conocida:\\
	$$X_i \sim N(\mu, \sigma_0) \implies f_{\mu}(x) = \frac{1}{\sigma_0 \sqrt{2\pi}} e^{-\frac{1}{2\sigma_0^2}(x - \mu)^2}$$
	$$\implies T = \bar{X} \implies f_{\mu}(x_1, \ldots, x_n) = \prod_{i = 1}^{n} \frac{1}{\sigma_0 \sqrt{2\pi}} e^{-\frac{1}{2\sigma_0^2}(x_i - \mu)^2} = \left(\frac{1}{\sigma_0 \sqrt{2\pi}}\right)^n e^{-\frac{1}{2\sigma_0^2}\sum_{i = 1}^{n}(x_i - \mu)^2}$$
	$\implies$ Tomando $h(x_1, \ldots, x_n) = \left(\frac{1}{\sigma_0 \sqrt{2\pi}}\right)^n$ y $g_{\mu}(T(x_1, \ldots, x_n)) = e^{-\frac{1}{2\sigma_0^2}\sum_{i = 1}^{n}(x_i - \mu)^2}$, tenemos que $T = \bar{X} = \sum_{i = 1}^{n}X_i$ es suficiente para $\mu$
}

\ejemplo{
	Sea el estadístico $T = \left(\sum_{i = 1}^{n}X_i, \sum_{i = 1}^{n} X_i^2 \right)$ suficiente para $\theta = (\mu, \sigma)$ si $X \sim N(\mu, \sigma)$ con $\mu$ y $\sigma$ desconocidas. Veamos si $\left(\bar{X}, S_n^2\right)$ también es suficiente para $\theta$: \\
	$$\bar{X} = \frac{1}{n} \sum_{i = 1}^{n} X_i \quad S_n^2 = \frac{1}{n - 1} \left(\sum_{i = 1}^{n} X_i^2 - n\bar{X}^2\right) \implies$$
	
	$$\text{Sea la biyección } S: \mathbb{R}^2 \to \mathbb{R}^2 \text{ dada por } S(x,y) = \left(\frac{1}{n}x, \frac{1}{n-1}\left(y - \frac{1}{n}x^2\right)\right) \implies$$ 
	
	
	$$ S(T = \left(\sum_{i = 1}^{n}X_i, \sum_{i = 1}^{n} X_i^2 \right)) = \left(\frac{1}{n}\sum_{i = 1}^{n}X_i, \frac{1}{n-1}\sum_{i = 1}^{n}X_i^2 - \bar{X}^2\right) = \left(\bar{X}, S_n^2 \right)$$
	Entonces por el Teorema de Caracterización de Fisher, $\left(\bar{X}, S_n^2\right)$ es suficiente para $\theta$
}

\ejemplo{
	Si $X \sim U(0, \theta)$, veamos si el estadístico $T = (X_{(1)}, X_{(n)})$ es suficiente para $\theta$:\\
	$$ X \sim U(0, \theta) \implies f_{\theta}(x) = \frac{1}{\theta} \cdot I_{(0, \theta)}(x) \implies f_{\theta}(x_1, \ldots, x_n) = \prod_{i = 1}^{n} \frac{1}{\theta} \cdot I_{(0, \theta)}(x_i) = \frac{1}{\theta^n} \cdot I_{(0, \theta)}(X_{(1)}) \cdot I_{(0, \theta)}(X_{(n)})$$
	$$\implies \text{Tomando } h(x_1, \ldots, x_n) = 1  \text{ y } g_{\theta}(T(x_1, \ldots, X_n)) = \frac{1}{\theta^n} I_{(0, \theta)}(X_{(1)}) \cdot I_{(0, \theta)}(X_{(n)})$$
}

\ejemplo{
	Si $X \sim U(0, \theta)$, veamos si el estadístico $T = X_{(n)}$ es suficiente para $\theta$:\\
	$$ X \sim U(0, \theta) \implies f_{\theta}(x) = \frac{1}{\theta} \cdot I_{(0, \theta)}(x) \implies f_{\theta}(x_1, \ldots, x_n) = \prod_{i = 1}^{n} \frac{1}{\theta} \cdot I_{(0, \theta)}(x_i) = \frac{1}{\theta^n} \cdot I_{(0, \theta)}(X_{(n)})$$
	$$\implies \text{Tomando } h(x_1, \ldots, x_n) = 1  \text{ y } g_{\theta}(T(x_1, \ldots, X_n)) = \frac{1}{\theta^n} I_{(0, \theta)}(X_{(n)})$$
}

\ejemplo{
	Si $X \sim U\left(-\frac{\theta}{2}, \frac{\theta}{2}\right)$, veamos si el estadístico $T = (X_{(1)}, X_{(n)})$ es suficiente para $\theta$:\\
	$$X \sim U(-\frac{\theta}{2}, \frac{\theta}{2}) \implies f_{\theta}(x) = \frac{1}{\theta} \cdot I_{\left(-\frac{\theta}{2}, \frac{\theta}{2}\right)}(x)$$ $$\implies f_{\theta}(x_1, \ldots, x_n) = \prod_{i = 1}^{n} \frac{1}{\theta} \cdot I_{\left(-\frac{\theta}{2}, \frac{\theta}{2}\right)}(x_i) = \frac{1}{\theta^n} \cdot I_{\left(-\frac{\theta}{2}, \frac{\theta}{2}\right)}(X_{(1)}) \cdot I_{\left(-\frac{\theta}{2}, \frac{\theta}{2}\right)}(X_{(n)})$$
	$$\implies \text{Tomando } h(x_1, \ldots, x_n) = 1  \text{ y } g_{\theta}(T(x_1, \ldots, X_n)) = \frac{1}{\theta^n} I_{\left(-\frac{\theta}{2}, \frac{\theta}{2}\right)}(X_{(1)}) \cdot I_{\left(-\frac{\theta}{2}, \frac{\theta}{2}\right)}(X_{(n)})$$
}
  
\subsection{Estadístico minimal suficiente}

\begin{definición}[Órbita]
	Dado un estadístico $T=T\left(X_{1}, \cdots X_{n}\right)$, se define $A_{t}=\left\{\left(x_{1}, \ldots, x_{n}\right) \in \chi^{n}: T\left(x_{1}, \ldots, x_{n}\right)=t\right\}$ cómo la órbita de $ t = 5$\\
\end{definición}

\ejemplo{
	Dado un m.a.s. de tamaño n = 3, definimos el \underline{rango muestral} como $T(x_1, x_2, x_3) = (x_{max}, x_{min})$\\
	Si por ejemplo tomamos el resultado $t = 5$ definimos la órbita de $t = 5$ al conjunto $A_5 = \{(x_1, \ldots, x_n) \in \mathbb{R}^3 : max(x_1, x_2, x_3) - min(x_1, x_2, x_3) = 5\}$ cuyos elementos podrían ser por ejemplo: 
	\begin{itemize}
		\item $(2, 3, 7) \in A_5 \quad (7 - 2 = 5)$
		\item $(10, 12, 15) \in A_5 \quad (15 - 10 = 5)$
		\item $(-1, 4, -5) \notin A_5 \quad (4 - (-6) = 10 \neq 5)$
	\end{itemize}
}
  
\begin{definición}[Partición Inducida]
	Dado un estadístico $T=T\left(X_{1}, \cdots X_{n}\right)$, se define $K_{T}=\left\{A_{t}: t \in \mathbb{R}^{m}\right\}$ cómo la partición inducida por $T$\\
\end{definición}

\ejemplo{
	Sea una muestra aleatoria simple de tamaño $n = 2$ tal que el espacio muestral de cada una de las variables aleatorias es $\chi = \{1, 2, 3\}$ y definimos el estadístico $T(x_1, x_2) = (x_1 + x_2)$\\
	Entonces, podemos deinir algunas órbitas de $T$:
	\begin{itemize}
		\item $A_2 = \{(1, 1), (2, 0), (0, 2)\}$
		\item $A_3 = \{(1, 2), (2, 1)\}$
		\item $A_4 = \{(2, 2)\}$
		\item $A_5 = \{(3, 2), (2, 3)\}$
		\item ...
	\end{itemize}
	Así, una partición inducida por $T$ sería $K_T = \{A_2, A_3, A_4, A_5, ...\}$
}

\begin{proposición}
	Se dice que $\mathcal{K}_{T}$ es suficiente sí y sólo si $T$ es suficiente\\
\end{proposición}

\begin{proposición}
	Dado dos estadísticos $T = T(X_1, \dots, X_n)$ y $S = S(X_1, \dots, X_n)$, se dice que $\mathcal{K}_S$ es una subpartición de $\mathcal{K}_T$ si y solo si:  
	$$ \forall B \in \mathcal{K}_S, \ \exists A \in \mathcal{K}_T \text{ tal que } B \subset A.$$
	En este caso, se dice que $\mathcal{K}_T$ es una partición menos fina que $\mathcal{K}_S$.
\end{proposición}
  
\begin{definición}[Estadístico Minimal Suficiente]
	Un estadístico $T$ es minimal suficiente si su partición asociada es suficiente y es la menos fina $\equiv$ más gruesa entre todos los estadísticos. \\
	\[
		\begin{cases} 
			\text{Más fina} \implies \text{Más clases} \implies \text{Más detallada}           \\ 
			\text{Más gruesa} \implies \text{Menos clases} \implies \text{Agrupa más elementos} 
		\end{cases}
	\]
	    
	Alternativamente, lo podemos definir como que dado un estadístico suficiente $T$, se dice minimal suficiente cuando $\forall T'$ suficiente, $\exists \varphi: \mathbb{R}^n \to \mathbb{R}^n$ medible (integrable) tal que $T' = \varphi(T)$
\end{definición}
  
\begin{proof}
	Demostración de la equivalencia de las definiciones:
	\begin{itemize}
		\item ($\Rightarrow$): Sea $S$ suficiente, si $S(x_1, \ldots, x_n) = S(y_1, \ldots, y_n) = s \implies (x_1, \ldots, x_n), (y_1, \ldots, y_n) \in B_s (\text{Órbita de } S) \implies \exists A_t \in K_T : B_s \subset A_t \implies T(x_1, \ldots, x_n) = T(y_1, \ldots, y_n) = t \implies \exists \psi : \psi(S) = T$ y $T$ es suficiente.
		\item ($\Leftarrow$): Sea $S$ suficiente y $\psi : \psi(S) = T \implies T$ es suficiente y si $S(x_1, \ldots, x_n) = S(y_1, \ldots, y_n) = s \implies T(x_1,\ldots, x_n) = \psi(S(x_1, \ldots, x_n)) = \psi(S) = \psi(S(y_1, \ldots, y_n)) = T(y_1, \ldots, y_n) \implies B_s \subset A_{\psi(S)} \implies K_S$ es una subpartición de $K_T$.
	\end{itemize}
\end{proof}
  
\subsection{Teorema de caracterización de estadísticos minimales suficientes}
\begin{definición}[Relación de equivalencia minimal suficiente]
	Sean $(x_1, \ldots, x_)$ y $(y_1, \ldots, y_n)$ muestras, definimos la relación de equivalencia minimal suficiente como: 
	$$(x_1, \ldots, x_n) R (y_1, \ldots, y_n) \iff \frac{f_{\theta}(x_1, \ldots, x_n)}{f_{\theta}(y_1, \ldots, y_n)} \text{ es independiente de } \theta$$
	Esta definición lo que trata de indicar es que ambas muestras del mismo suceso, aportan la misma información sobre el parámetro $\theta$.
\end{definición}

\begin{teorema}[Teorema de caracterización de estadísticos minimales (Lehmann-Scheffé)]
  A cada clase de equivalencia anterior le asignamos un valor $t$ y definimos un estadístico $T = T(X_1, \ldots, X_n)$ tal que $\frac{f_{\theta}(x_1, \ldots, x_n)}{f_{\theta}(y_1, \ldots, y_n)}$ es independiente de $\theta$ cuando $T(x_1, \ldots, x_n) = T(y_1, \ldots, y_n) = t$. Entonces, $T$ es minimal suficiente para $\theta$.
\end{teorema}


\begin{proof}
  Supongamos la suficiencia de $T$ para demostrar su minimalidad: \\
  Sea $S = S(X_1, \ldots, X_n)$ suficiente y $(x_1, \ldots, x_n), (y_1, \ldots, y_n) \in B_S \implies$ $$\frac{f_{\theta}(x_1, \ldots, x_n)}{f_{\theta}(y_1, \ldots, y_n)} = \frac{h(x_1, \ldots, x_n)g_{\theta}(s)}{h(y_1, \ldots, y_n)g_{\theta}(s)} = \frac{h(x_1, \ldots, x_n)}{h(y_1, \ldots, y_n)} \text{ independiente de } \theta$$ $\implies \exists t : (x_1, \ldots, x_n), (y_1, \ldots, y_n) \in A_t \implies K_S$ es una subpartición de $K_T$.
  \\Ahora demostremos la suficiencia de $T$ en el caso discreto: \\
  Si $T(x_1, \ldots, x_n) = t$, 
  $$f_{\theta}(x_1, \ldots, x_n | t) = \frac{f_{\theta}(x_1, \ldots, x_n)}{f_ {\theta}(t)} = \frac{f_{\theta}(x_1, \ldots, x_n)}{\sum_{(y_1, \ldots, y_n) : T(y_1, \ldots, y_n) = t} f_{\theta}(y_1, \ldots, y_n)} = \frac{1}{\sum_{(y_1, \ldots, y_n) \in A_t} \frac{f_{\theta}(y_1, \ldots, y_n)}{f_{\theta}(x_1, \ldots, x_n)}}$$ que es independiente de $\theta \implies T$ es suficiente.
\end{proof}


\ejemplo{
  Veamos si el estadístico $T = \sum_{i = 1}^{n}X_i$ es minimal suficiente para $\theta$ si $X \sim Bin(1, \theta)$\\
  $$X \sim Bin(1, \theta) \equiv Bernoulli(\theta) \implies f_{\theta}(x) = \theta^x(1 - \theta)^{1 - x} \cdot I_{\{0, 1\}}(x) \implies$$
  $$\implies f_{\theta}(x_1, \ldots, x_n) = \prod_{i = 1}^{n} \theta^{x_i}(1 - \theta)^{1 - x_i} \cdot I_{\{0, 1\}}(x_i) = \theta^{\sum_{i = 1}^{n}x_i}(1 - \theta)^{n - \sum_{i = 1}^{n}x_i} \cdot \prod_{i = 1}^{n}I_{\{0, 1\}}(x_i)$$
  $\implies$ Sean dos muestas m.a.s. de tamaño = $n, (x_1, \ldots, x_n)$ y $(y_1, \ldots, y_n)$, entonces:
  $$\frac{f_{\theta}(x_1, \ldots, x_n)}{f_{\theta}(y_1, \ldots, y_n)} = \left(\frac{\theta}{1 - \theta}\right)^{\sum_{i = 1}^{n}x_i - \sum_{i = 1}^{n}y_i} \text{ es independiente de } \theta \text{ si } \sum_{i = 1}^{n}x_i = \sum_{i = 1}^{n}y_i$$
}

\begin{observación}
  Recordemos que $\forall T' \text{ suficiente } \exists \varphi : \varphi(T) = T'$, por lo que $T$ es minimal suficiente
\end{observación}

\ejemplo{
  Sea $f_{\theta}(x) = e^{-(x + \theta)} \cdot I_{(\theta, \infty)}(x)$  
  $$\implies f_{\theta}(x_1, \ldots, x_n) = e^{-\sum_{i = 1}^{n}(x_i + \theta)} \cdot I_{(\theta, \infty)}(x_1) \cdot \ldots \cdot I_{(\theta, \infty)}(x_n) = e^{-\sum_{i=1}^{n} x_i} e^{-n\theta} \cdot I_{(\theta, \infty)}(x_{(1)})$$
  Entonces, por el Teorema de Factorización de Fisher, tenemos que: 
  $$f_{\theta}(x_1, \ldots, x_n) = h(x_1, \ldots, x_n) \cdot g_{\theta}(T(x_1, \ldots, x_n)) \implies \begin{cases} h(x_1, \ldots, x_n) = e^{-\sum_{i=1}^{n} x_i} \\ g_{\theta}(T(x_1, \ldots, x_n)) = e^{-n\theta} \cdot I_{(\theta, \infty)}(x_{(1)}) \end{cases}$$
  Ahora sean dos muestras m.a.s. de tamaño $n, (x_1, \ldots, x_n)$ y $(y_1, \ldots, y_n)$, entonces, tenemos que ver qué expresión hace que el cociente de las funciones de densidad sea independiente de $\theta$:
  $$\frac{f_{\theta}(x_1, \ldots, x_n)}{f_{\theta}(y_1, \ldots, y_n)} = \frac{e^{-\sum_{i=1}^{n} x_i} e^{-n\theta} \cdot I_{(\theta, \infty)}(x_{(1)})}{e^{-\sum_{i=1}^{n} y_i} e^{-n\theta} \cdot I_{(\theta, \infty)}(y_{(1)})} = \frac{e^{-\sum_{i=1}^{n} x_i}\cdot I_{(\theta, \infty)}(x_{(1)})}{e^{-\sum_{i=1}^{n} y_i}\cdot I_{(\theta, \infty)}(y_{(1)})}$$ $$\implies \text{esta expresión es independiente de } \theta \iff X_{(1)} = Y_{(1)} \implies$$ $$\implies \text{ el estadístico } T = X_{(1)} \text{ es minimal suficiente para } \theta$$
}
  
  
\subsection{Familia exponencial $k$-paramétrica}


\begin{definición}[Familia exponencial k-paramétrica]
  Una distribución $X$ pertence a la familia exponencial $k$-paramétrica si su función de densidad o de masa, se puede expresar como: 
  $$f_{\theta}(x) = c(\theta) \cdot h(x) \cdot e^{\sum_{j = 1}^{k}q_j(\theta)T_j(x)}$$
  donde: 
  \begin{itemize}
    \item $c(\theta)$ es una función sólo de $\theta$
    \item $h(x)$ es una función no negativa sólo de $x$
    \item $q_j(\theta)$ son funciones de $\theta$, conocidas como \underline{parámetros naturales}
    \item $T_j(x)$ son funciones de $x$ llamadas \underline{estadísticos naturales}
    \item $k$ es el número de parámetros de la familia
    \item $\theta$ es el parámetro de la familia
  \end{itemize}
  Esto implica a su vez, que si tenemos una m.a.s., la función de densidad conjunta se puede expresar como: 
  $$f_{\theta}(x_1, \ldots, x_n) = \prod_{i = 1}^{n}f_{\theta(x_i)} = (c(\theta))^n \prod_{i = 1}^{n}h(x_i)e^{\sum_{j = 1}^{k}q_j(\theta)T_j(x_i)}$$
  $$ = (c(\theta))^n \cdot h(x_1, \ldots, x_n) \cdot e^{\sum_{j = 1}^{k}q_j(\theta)(\sum_{i = 1}^{n}T_j(x_i))}$$
\end{definición}

\textbf{Ejemplos de familias exponenciales:}\\
Entre otras, podemos encontrar las siguientes familias exponenciales:
\begin{multicols}{2}
	\begin{enumerate}
		\item Normal
		\item Poisson
		\item Bernoulli
		\item Exponencial
		\item Geométrica
		\item Chi-cuadrado
	\end{enumerate}
	\end{multicols}
Cabe notar que existen distribuciones que son familias exponenciales, pero solamente cuando ciertos parámetros son fijos y se conocen. Por ejemplo:
\begin{enumerate}
	\item Binomial (cuando el numero de ensayos $n$ es fijo)
	\item Multinomial (cuando el número de ensayos $n$ es fijo)
	\item Binomial negativa (cuando el número de fallos $r$ es fijo)
	\item Laplace (cuando la media es conocida)
\end{enumerate}
Ejemplos de distribuciones comunes que no son familias exponenciales son la t de Student, la mayoría de las distribuciones de mezcla e incluso la familia de distribuciones uniformes cuando los límites no son fijos.

Es importante destacar que el soporte de $f_{\theta}(x)$ no puede depender de $\theta$ en una familia exponencial $k$-paramétrica. Esto se debe a que los indicadores no se pueden particionar en una parte que dependa de $\theta$ y otra que no, como por ejemplo ocurre con la distribución binomial con $n$ desconocido, pues el soporte de la distribución depende de $n$.\\

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.5} % Increases row height for readability
    \NiceMatrixOptions{cell-space-limits = 1pt} % Adjusts cell padding for equations
    \begin{NiceTabular}{lcccc}[
        hvlines, % Draws horizontal and vertical lines
        code-before = {\rowcolor{gray!30} 1}, % Applies light gray to the first row
    ]
        \textbf{Distribución} & \textbf{Parámetros} & \textbf{Parámetros naturales} & \textbf{Estadísticos naturales} \\ 
        
        Normal    & \( (\mu, \sigma^2) \) & \( {\displaystyle {\begin{bmatrix}{\dfrac {\mu }{\sigma ^{2}}}\\[1ex]-{\dfrac {1}{2\sigma ^{2}}}\end{bmatrix}}} \) & \( {\displaystyle {\begin{bmatrix}\sum_{i = 1}^{n}X_i\\\sum_{i = 1}^{n}X_i^{2}\end{bmatrix}}} \) \\ 

		Normal &  \(\mu\) & \({\displaystyle {\frac {\mu }{\sigma }}}\) & \(\frac{1}{\sigma} \sum_{i = 1}^{n}X_i\) \\

		Poisson & \(\lambda\) & \({\displaystyle \log \lambda }\) & \({\displaystyle \sum_{i = 1}^{n}X_i}\) \\

		Bernoulli & \(p\) & \({\displaystyle \log \left({\frac {p}{1-p}}\right)}\) & \({\displaystyle \sum_{i = 1}^{n}X_i}\) \\

		Exponencial & \(\lambda\) & \({\displaystyle -\lambda }\) & \({\displaystyle \sum_{i = 1}^{n}X_i}\) \\

		Binomial & \(p\) & \({\displaystyle \log \left({\frac {p}{1-p}}\right)}\) & \({\displaystyle \sum_{i = 1}^{n}X_i}\) \\

		Binomial negativa & \(p\) & \(\log(1-p)\) & \({\displaystyle \sum_{i = 1}^{n}X_i}\) \\

		Chi-cuadrado & \(k\) & \(\frac {k}{2}-1\) & \(\log \left(\sum_{i = 1}^{n}X_i\right)\) \\
        
		Gamma & \((\alpha, \beta)\) & \({\displaystyle {\begin{bmatrix}\alpha -1\\-\beta \end{bmatrix}}}\) & \({\displaystyle {\begin{bmatrix}\log \left(\sum_{i = 1}^{n}X_i\right) \\ \sum_{i = 1}^{n}X_i \end{bmatrix}}}\) \\

		Beta & \((\alpha, \beta)\) & \({\displaystyle {\begin{bmatrix}\alpha -1\\\beta -1\end{bmatrix}}}\) & \({\displaystyle {\begin{bmatrix}\log \left(\sum_{i = 1}^{n}X_i\right) \\\log \left(1-\sum_{i = 1}^{n}X_i\right) \end{bmatrix}}}\) \\

    \end{NiceTabular}
    \label{tabla:distribuciones}
\end{table}

\begin{teorema}
  Sean $\theta_1, \ldots, \theta_k \in \Theta \subset \mathbb{R}^{\ell}$ tales que los vectores $c_r = (q_1(\theta_r), \ldots, q_k(\theta_r))$, $r = 1, \ldots, k$ son linealmente independientes, entonces el estadístico natural suficiente de la familia exponencial $k$-paramétrica es minimal
\end{teorema}

\begin{proof}
  Sean dos muestras $x = (x_1, \ldots, x_n)$ e $y = (y_1, \ldots, y_n)$, entonces:
  $$\frac{f_{\theta}(x_1, \ldots, x_n)}{f_{\theta}(y_1, \ldots, y_n)} = \frac{c(\theta)^n \cdot h(x_1, \ldots, x_n) \cdot e^{\sum_{j = 1}^{k}q_j(\theta)\sum_{i = 1}^{n}T_j(x_i)}}{c(\theta)^n \cdot h(y_1, \ldots, y_n) \cdot e^{\sum_{j = 1}^{k}q_j(\theta)\sum_{i = 1}^{n}T_j(y_i)}} = \frac{h(x_1, \ldots, x_n)}{h(y_1, \ldots, y_n)} \cdot e^{\sum_{j = 1}^{k}q_j(\theta)\left(\sum_{i = 1}^{n}[T_j(x_i) - T_j(y_i)]\right)}$$
  Lo cual es una expresión independiente de $\theta \iff$
  $$\iff \sum_{j = 1}^{k}q_j(\theta)\left(\sum_{i = 1}^{n}[T_j(x_i) - T_j(y_i)]\right) = 0 \iff \sum_{i = 1}^{n}[T_j(x_i) - T_j(y_i)] = 0, \ \forall j = 1, \ldots, k$$
  Lo cual implica que el sistema homogéneo $\sum_{i = 1}^{n}[T_j(x_i) - T_j(y_i)] = 0, \ \forall j = 1, \ldots, k$ sólo admite la solución $T_j(x_i) - T_j(y_i) = 0, \ \forall j = 1, \ldots, k$ $\implies (T_1(x_1), \ldots, T_k(x_n))$ es minimal
\end{proof}

\begin{observación}
	El estadístico natural de una distribución $X$ perteneciente a una familia exponencial $k$-paramétrica con $k=1$ nunca es minimal.
\end{observación}

\ejemplo{
  Veamos si el estadístico $T = \left(\sum_{i = 1}^{n}X_i, \sum_{i = 1}^{n}X_i^2\right)$ es minimal suficiente para $\theta = (\mu, \sigma)$ si $X \sim N(\mu, \sigma)$ con $\mu$ y $\sigma$ desconocidas.\\
  Una distribución perteneciente a la familia 2-paramétrica exponencial es de la forma: 
  $$f_{\theta}(x) = c(\theta)h(x)e^{q_1(\theta) T_1(x) + q_2(\theta)T_2(x)} \implies$$
$$\implies X \sim N(\mu, \sigma) \implies f_{\theta}(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2\sigma^2}(x - \mu)^2} \implies f_{\theta}(x_1, \ldots, x_n) = \left(\frac{1}{\sigma \sqrt{2\pi}}\right)^n e^{-\frac{1}{2\sigma^2}\sum_{i = 1}^{n}(x_i - \mu)^2}$$
$$= \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n e^{-\frac{1}{2\sigma^2}\sum_{i = 1}^{n}x_i^2}e^{\frac{\mu}{\sigma^2}\sum_{i = 1}^{n}x_i} \implies$$
$$\text{ tomando } \begin{cases} c(\theta) = \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^{n} \\ h(x) = 1 \\ q_1(\theta) = -\frac{1}{2\sigma^2} \\ q_2(\theta) = \frac{\mu}{\sigma^2} \\ T_1(x) = \sum_{i = 1}^{n}X_i^2 \\ T_2(x) = 
  \sum_{i = 1}^{n} x_i \end{cases}$$
  La distribución normal pertence a la familia exponencial 2-paramétrica.
  Ahora veamos si el estadístico $T$ es minimal suficiente:\\
  Si tomamos como vectores paramétricos $\theta_1 = (0, 1)$ y $\theta_2 = (1, 1)$, entonces los vectores $c_1 = (q_1(\theta_1), q_2(\theta_1)) = (0, -\frac{1}{2})$ y $c_2 = (q_1(\theta_2), q_2(\theta_2)) = (1, -\frac{1}{2})$ son linealmente independientes $\implies T$ es minimal suficiente
  \\
  Ahora veamos si el estadístico \( T = (\bar{X}, S_n^2) \) también es minimal suficiente:  

  Sabemos que:  
  \[
  S_n^2 = \frac{n}{n-1} \left(\sum_{i = 1}^n X_i^2 - n\bar{X}^2\right)
  \]
  $\implies \text{Sea la transformación }S: \mathbb{R}^2 \to \mathbb{R}^2 \text{ dada por } S(x, y) = \left(\frac{1}{n}x, \frac{1}{n-1}\left(y - \frac{1}{n}x^2\right)\right) \implies$
  
  Analicemos la transformación inversa:  
  \[
  W = nF, \quad Z = \frac{n-1}{n}G + nF^2
  \]
  \[
  J = \begin{vmatrix} n & 0 \\ 2nF & \frac{n-1}{n} \end{vmatrix} = n - 1 \neq 0 \text{ si } n \geq 2
  \]
  Por lo que cómo $S$ es una biyección, y $T$ es minimal suficiente, entonces la imagen de $T$ por $S$ también es minimal suficiente $S(T) = (\bar{X}, S_n^2)$
}

\begin{proposición}
  En las familias k-paramétricas exponenciales, es decir, aquellas de la forma: 
  $$ f_{\theta}(x) = c(\theta)h(x)e^{q(\theta)T(x)}$$
  El estadístico $T(x)$ es suficiente. 
\end{proposición}

  
\subsection{Estadísticos Ancilarios y Completos}
\begin{definición} [Estadístico Ancilario]
	Un estadístico $U(X_1, \ldots, X_n)$ es ancilario para $\theta$ si su distribución en la muestra es independiente de $\theta$
\end{definición}

\ejemplo{
	Sea una población que sigue una distribucion normal $N(\theta, \sigma_0)$ con $\sigma_0$ conocida, tomemos una m.a.s. de tamaño $n$ tal que $U(X_1, \ldots, X_n) = X_1 - X-2 \sim  N(0, \sqrt{2\sigma_0})$ que no depende de $\theta$ por lo que $U$ es un estadístico ancilario para $\theta$.
}

\begin{definición} [Familia Completa]
	La familia $\mathcal{P} = \{f_{\theta}(x_1, \ldots, x_n) : \theta \in \Theta \subset \mathbb{R}^{\ell}\}$ es completa si para cualquier función real $g(X_1, \ldots, X_n$) tal que $E_{\theta}[g(X_1, \ldots, X_n)] = 0, \ \forall \theta \in \Theta \implies g(X_1, \ldots, X_n) \stackrel{c.s.}{=} 0$
\end{definición}

\ejemplo{
	Veamos si la familia de distribuciones $Bin(n, \theta)$ es completa: \\
	Sea $Y \sim Bin(n, \theta)$, entonces:
	$$E_{\theta}[h(Y)] = \sum_{i = 1}^{n}h(i)\binom{n}{i}\theta^i(1 - \theta)^{n - i} = (1 - \theta)^{n}\sum_{i = 1}^{n} h(i) \binom{n}{i}\left(\frac{\theta}{1 - \theta}\right)^i = 0, \ \forall \theta \in (0, 1)$$
	Si definimos la variable $x = \frac{\theta}{1 - \theta} \implies$ $$\sum_{i = 1}^{n}h(i)\binom{n}{i}x^i = 0 \text{ es un polinomio de grado $n$ en $x$} \implies h(i) = 0, \ \forall i = 1, \ldots, n \implies h(Y) \stackrel{c.s.}{=} 0$$
	Por lo que la familia de distribuciones $Bin(n, \theta)$ es completa
}

\ejemplo{
	Veamos si la familia de distribuciones $N(0, \theta)$ es completa: \\
	Sea $Y \sim N(0, \theta)$, entonces:
	Si tomamos la función real identidad, definida por $g(Y) = Y$, entonces: 
	$$E_{\theta}[g(Y)] = E_{\theta}[Y] = 0, \ \forall \theta > 0, \text{ pero } g(Y) = Y \text{ no es idénticamente nula c.s.}$$
	Por lo que la familia de distribuciones $N(0, \theta)$ no es completa
}

\ejemplo{
	Sea $X$ una variable aleatoria discreta $D_x = \{1, 2, 3\}$ comprobemos si $X$ es completa:
	Tenemos que: 
	$\begin{cases}
		P_{\theta}(X = 1) = \theta - 3 \\
		P_{\theta}(X = 2) = 2\theta - 1 \\
		P_{\theta}(X = 3) = 3 - 3\theta
	\end{cases}$
	Tomemos el estadístico $T_1(x_1, \ldots, x_n) = x_1$ y comprobemos si es completo: \\
	Sea $\varphi$ función real que actúa sobre el soporte $D_x$, entonces:
	$$E_{\theta}[\varphi(T_1)] = \varphi(1) \cdot P(X = 1) + \varphi(2) \cdot P(X = 2) + \varphi(3) \cdot P(X = 3)$$
	$$ = \varphi(1)(\theta - 3) + \varphi(2)(2\theta - 1) + \varphi(3)(3 - 3\theta) = \theta(\varphi(1) + 2\varphi(2) - 3\varphi(3)) - 3\varphi(1) - \varphi(2) + 3\varphi(3) = 0$$
	Pero este sistema de ecuaciones es un sistema compatible indeterminado, por lo que no tiene una única solución y por tanto no tiene porqué cumplirse que $\varphi(T_1) = 0$ c.s. $\implies T_1$ no es completo.
}

\ejemplo{
	Sea $X$ una variable aleatoria discreta $D_x = \{1, 2, 3\}$ comprobemos si $X$ es completa:
	Tenemos que: 
	$\begin{cases}
		P_{\theta}(X = 1) = \theta^2 + \theta \\
		P_{\theta}(X = 2) = 1 -2\theta^2 \\
		P_{\theta}(X = 3) = \theta^2 - \theta
	\end{cases}$
	$\implies$
	$$E_{\theta}[\varphi(T)] = \varphi(1) \cdot P(X = 1) + \varphi(2) \cdot P(X = 2) + \varphi(3) \cdot P(X = 3) = $$
	$$ = \varphi(1)(\theta^2 + \theta) + \varphi(2)(1 - 2\theta^2) + \varphi(3)(\theta^2 - \theta) = \theta^2(\varphi(1) -2\varphi(2) + \varphi(3)) + \theta(\varphi(1) - \varphi(3)) + \varphi(2) = 0$$
	Lo cual se trata de un polinomio de grado 2 en $\theta$ por lo que podría tener solución:
	$$\begin{cases}
		\varphi(1) - 2\varphi(2) + \varphi(3) = 0 \\
		\varphi(1) - \varphi(3) = 0 \\
		\varphi(2) = 0
	\end{cases}$$
	$\implies \varphi(1) = \varphi(3) = \varphi(2) = 0 \implies \varphi(T) = 0$ c.s. $\implies T$ es completo
}

\ejemplo{
	Sea $X$ una variable aleatoria discreta con dominio \( D_X = \{-1\} \cup \{0\} \cup \mathbb{N} \). Queremos comprobar si $X$ es completa. Los valores de probabilidad vienen dados por:

	\[
	P(X = -1) = \theta, \quad P(X = x) = \theta^x(1-\theta)^2, \quad \forall x \in \{0\} \cup \mathbb{N}
	\]
	
	Por definición, verificamos si \(E[\varphi(X)] = 0\) implica \(\varphi(X) = 0\) para toda función medible. Calculamos la esperanza:
	
	\[
	E[\varphi(X)] = \varphi(-1) P(X = -1) + \sum_{k = 0}^{+\infty} \varphi(k) P(X = k)
	\]
	
	\[
	= \varphi(-1) \theta + \sum_{k = 0}^{+\infty} \varphi(k) \theta^k (1 - \theta)^2
	\]
	
	Factorizando:
	
	\[
	E[\varphi(X)] = \varphi(-1)\theta + (1-\theta)^2 \sum_{k = 0}^{+\infty} \varphi(k) \theta^k = 0
	\]
	
	Para todo \( \theta \), lo que nos lleva a estudiar la ecuación funcional:
	
	\[
	\varphi(-1)\theta + (1-\theta)^2 \sum_{k = 0}^{+\infty} \varphi(k) \theta^k = 0, \quad \forall \theta
	\]
	
	Esta ecuación es un polinomio en \(\theta\). Para que se cumpla para todo \(\theta\), cada coeficiente debe anularse individualmente. Analizando los coeficientes del desarrollo en serie de potencias:
	
	1. Coeficiente de \(\theta^{-1}\): No hay términos de este tipo, por lo que no impone restricciones.

	2. Coeficiente de \(\theta^0\):
	   \[
	   (1 - \theta)^2 \varphi(0) = 0 \implies \varphi(0) = 0
	   \]
	3. Coeficiente de \(\theta^1\):
	   \[
	   \varphi(-1) + (1-\theta)^2 \varphi(1) = 0
	   \]
	
	   Evaluando en \(\theta = 1\):
	   \[
	   \varphi(-1) + 0 = 0 \implies \varphi(-1) = 0
	   \]
	
	4. Para \( k \geq 1 \), los coeficientes del polinomio también deben anularse:
	   \[
	   \varphi(k) = 0, \quad \forall k \in \mathbb{N}
	   \]
	
	Por lo tanto, se concluye que la única solución posible es la trivial:
	
	\[
	\varphi(k) = 0, \quad \forall k \in D_X
	\]
	
	Conclusión: La variable aleatoria \( X \) es completa.
	
}

\begin{definición} [Estadístico Completo]
	Un estadístico $T$ (no necesariamente suficiente) se dice completo, cuando: 
	$$\forall g \text{-medible(integrable) tal que } E_{\theta}[g(T)] = 0 \implies g(T) \stackrel{c.s.}{=} 0$$
	Es decir, se dice que un estadístico es completo si no existe función $g(x_1, \ldots, x_n)$ tal que $E_{\theta}[g(T)] = 0$ a menos rque $g(X) \stackrel{c.s.}{=} 0$
	\\ Alternativamente, también se puede decir, que el estadístico es completo si y solo sí su distribución en el muestreo es una familia de distribuciones de probabilidad completa
\end{definición}

\ejemplo{

Dado que cada \( X_i \sim \text{Bin}(1, \theta) \), la suma 

\[
T = \sum_{i=1}^{n} X_i
\]

sigue una distribución binomial con parámetros \( n \) y \( \theta \):

\[
T \sim \text{Bin}(n, \theta).
\]

Para demostrar que \( T \) es un estadístico completo, consideremos una función \( g(T) \) tal que:

\[
E_\theta[g(T)] = 0, \quad \forall \theta \in (0,1).
\]

Calculando la esperanza,

\[
E_\theta[g(T)] = \sum_{i=0}^{n} g(i) \binom{n}{i} \theta^i (1 - \theta)^{n - i}.
\]

Factorizando \( (1 - \theta)^n \):

\[
E_\theta[g(T)] = (1 - \theta)^n \sum_{i=0}^{n} g(i) \binom{n}{i} \left( \frac{\theta}{1 - \theta} \right)^i.
\]

Definiendo \( x = \frac{\theta}{1 - \theta} \), obtenemos:

\[
\sum_{i=0}^{n} g(i) \binom{n}{i} x^i = 0, \quad \forall x \in (0, \infty).
\]

Dado que el lado izquierdo es un polinomio de grado \( n \) en \( x \) que se anula para todo \( x \), por el \textbf{teorema fundamental del álgebra}, los coeficientes del polinomio deben ser todos cero:

\[
g(i) \binom{n}{i} = 0, \quad \forall i = 0, \dots, n.
\]

Como los coeficientes binomiales \( \binom{n}{i} \) son distintos de cero, se sigue que:

\[
g(i) = 0, \quad \forall i.
\]

Por lo tanto, \( g(T) = 0 \) casi seguramente, lo que implica que \( T \) es un estadístico \textbf{completo}
}

\begin{proposición}
	Si $T_1$ y $T_2$ son estadísticos completos, entonces $(T_1, T_2)$ también es completo
\end{proposición}

\ejemplo{
	Veamos si dado $X \sim N(\theta, \theta)$ el estadístico $T = \left(\sum_{i = 1}^{n}X_i, \sum_{i = 1}^{n}X_i^2\right)$ es completo:\\
	Por las propiedades de las esperanzas tenemos que:
	$$\sum_{i = 1}^{n}E[X_i] = n\theta, \quad \sum_{i = 1}^{n}E[X_i^2] = n(E[X_i^2]) = n(\theta + \theta^2)$$
	Sea $h(t) = (2(\sum_{i = 1}^{n}X_i)^2 - (n + 1)\sum_{i = 1}^{n}X_i^2)$, entonces:
	$$E_{\theta}[h(T)] = 2E\left[\left(\sum_{i = 1}^{n}X_i \right)^2\right] - (n+1)E\left[\sum_{i = 1}^{n}X_i^2\right] = 2E\left[\left(\sum_{i = 1}^{n}X_i \right)^2\right] - (n+1)\sum_{i = 1}^{n}E[X_i^2] $$
	Recordemos que $Var(Y) = E[Y^2] - (E[Y])^2$, entonces:
	$$E\left[\left(\sum_{i = 1}^{n}X_i \right)^2\right] = Var\left(\sum_{i = 1}^{n}X_i\right) + \left(E\left[\sum_{i = 1}^{n}X_i\right]\right)^2 = n\theta^2 + n^2\theta^2 = n\theta^2(n+1)$$	
	$$E[X_i^2] = Var(X_i) + (E[X_i])^2 = \theta^2 + \theta^2 = 2\theta^2 \implies E_{\theta} \left[\sum_{i = 1}^{n}X_i^2\right] = 2n\theta^2$$
	$$\implies E_{\theta}[h(T)] = 2n\theta^2(n+1) - (n+1)2n\theta^2 \implies E_{\theta}[h(T)] = 0, \ \forall \theta \in \Theta$$
	Pero $h(T) = 2(\sum_{i = 1}^{n}X_i)^2 - (n + 1)\sum_{i = 1}^{n}X_i^2 \neq 0$ por lo que $T$ no es completo
}

\begin{observación}
	Recordemos que: 
	\vspace{-0.5em}
	\begin{itemize}
		\item \textbf{Suficiencia}: Tiene toda la información sobre el parámetro $\theta$ pero puede tener información de más. 
		\item \textbf{Completitud}: No tiene información de más, pero puede no tener toda la información necesaria.
		\item \textbf{Minimalidad}: Tiene toda la información necesaria y no tiene información de más.
	\end{itemize}
\end{observación}

\begin{teorema}[Teorema de Bahadur]
	Un estadístico $T = T(X_1, \ldots, X_n)$ es suficiente y completo $\implies$ es minimal suficiente.
\end{teorema}

\begin{proof}
	Por hipótesis del ejercicio sabemos que el estadístio $T$ es suficiente y completo. Además, supongamos que tenemos otro estadístico $S$ suficiente y definamos un tercero $H = E[T|S]$. 
	\begin{observación}
		Recordemos la propiedad de las esperanzas, \underline{propiedad de la torre de la expectativa} o \uline{propiedad de la iteración de la esperanza} que dice que:
		$$E[E[T|\mathcal{G}]] = E[T]$$
		Donde $\mathcal{G}$ es una $\sigma$-álgebra.\\
		No obstante, dado que mayormente se trabaja con estadísticos, podemos denotar: 
		$$E[E[T|S]] = E[T]$$
		Donde $S$ denota en realidad $\sigma(S)$-álgebra (la menor de todas las generadas por S, en particular).
	\end{observación}
	Definamos ahora, el estadístico $L$ dado por $L = E[H|T] \implies$
	$$\begin{cases}
		H = E[T|S] \implies E[H] = E[E[T|S]] = E[T] \\
		L = E[H|T] \implies E[L] = E[E[H|T]] = E[H]
	\end{cases} \implies E[L] = E[H] = E[T]$$

	Sea la función real $g(T) = T - E[H|T]$, entonces, intentemos calcular su esperanza: 
	$$E[g(T)] = E[T - E[H|T]] = E[T] - E[E[H|T]] = E[T] - E[H] = E[T] - E[T] = 0$$
	Entonces, por la completitud de $T$, ésto implica que: 
	$$g(T) \stackrel{c.s.}{=} 0 \implies T = E[H|T]$$
	Esto nos dice que tras intentar estimar $T$ a través de $H = (T|S$), volvemos a obtener $T$, por lo que contiene toda la información necesaria y no tiene información de más, por lo que es minimal suficiente.
\end{proof}

\begin{observación}
	El recíproco no se cumple, veamos un contraejemplo:\\
	Sean $X \sim N(\mu, \sigma_{X}^2)$ e $Y \sim N(\mu, \sigma_{Y}^2)$ y tomemos el estadístico $T = (\bar{X}, \bar{Y}, S_X^2, S_Y^2)$
	Veamos que el estadístico es minimal suficiente  para $\theta = (\mu, \sigma_X^2,\sigma_Y^2)$, pero no es completo: \\
	Por el Teorema de Caracterización de los estadísticos minimales, tenemos que: 
	$$\frac{f_{\theta}(x_1, \ldots, x_n)}{f_{\theta}(y_1, \ldots, y_n)} = \frac{\left(\frac{1}{\sigma_X\sqrt{2\pi}}\right)^n \cdot e^{-\frac{1}{2}\sum_{i = 1}^{n}(\frac{x_i - \mu}{\sigma})^2}}{\left(\frac{1}{\sigma_Y\sqrt{2\pi}}\right)^n \cdot e^{-\frac{1}{2}\sum_{i = 1}^{n}(\frac{y_i - \mu}{\sigma})^2}} =  \left(\frac{\sigma_Y}{\sigma_X}\right)^n \cdot e^{-\frac{1}{2\sigma_X^2}\sum_{i = 1}^{n}(x_i - \mu)^2 + \frac{1}{2\sigma_Y^2}\sum_{i = 1}^{n}(y_i - \mu)^2} = $$
	Sabiendo que $(x_i - \mu)^2 = (x_i^2 + \mu^2 - 2x_i\mu) \implies \sum(x_i - \mu)^2 = \sum x_i^2 + \sum \mu^2 -\sum 2x_i \mu = \sum x_i^2 + n\mu^2 - 2n\mu\bar{X}$\\
	Ahora, desarrollemos $\sum x_i^2$:\\
	$$\begin{cases} S^2 = \frac{1}{n}\sum(x_i - \bar{X})^2 \\ \bar{X} = \frac{1}{n}\sum x_i \end{cases} \implies \begin{cases} \sum x_i^2 = n(S_X^2 + \bar{X}^2) \\ \sum y_i^2 = n(S_Y^2 + \bar{Y}^2) \end{cases} \implies$$ $$ \implies \begin{cases} n(S_X^2 + \bar{X}^2) + n\mu^2 - 2n\mu \bar{X} \\ n(S_Y^2 + \bar{Y}^2) + n\mu^2 - 2n\mu\bar{Y} \end{cases} \implies \begin{cases} n(S_X^2 + \bar{X}^2 + \mu^2 - 2\mu\bar{X}) \\ n(S_Y^2 + \bar{Y}^2 + \mu^2 - 2\mu\bar{Y}) \end{cases} $$
	$$ \frac{f_{\theta}(\vec{x})}{f_{\theta}(\vec{y})} = \left(\frac{\sigma_Y}{\sigma_X}\right)^n \cdot e^ {-\frac{n}{2\sigma_X^2}(S_X^2 + \bar{X}^2 + \mu^2 - 2\mu\bar{X}) + \frac{n}{2\sigma_Y^2}(S_Y^2 + \bar{Y}^2 + \mu^2 - 2\mu\bar{Y})}$$
	Pero esta expresión no depnde de $\theta$ si y solo si $\bar{X} = \bar{Y}, S_X^2 = S_Y^2$ por lo que el estadístico $T$ es minimal suficiente.\\
	No obstante, si tomamos la función $g(x, y) = x - y$, entonces:
	$$E[g(T)] = E[\bar{X} - \bar{Y}] = E[\bar{X}] - E[\bar{Y}] = \mu - \mu = 0$$
	Sin embargo, 
	$$g(T) = \bar{X} - \bar{Y} \neq 0 \text{ c.s.}$$
	Por lo que el estadístico $T$ no es completo
\end{observación}
  

\begin{teorema}	
	El estadístico natural $(\sum_{i = 1}^{n}T_1(X_i), \ldots \sum_{i = 1}^{n}T_k(X_i))$ de la familia exponencial $k$-paramétrica $$\left\{f_{\theta}(x) = c(\theta)h(x)e^{\sum_{j = 1}^{k}q_j(\theta)T_j(x)}\right\}_{\theta \in \Theta \subset \mathbb{R}^{\ell}}$$ es \textbf{completo} si la imagen de la aplicación $q = (q_1(\theta), \ldots, q_k(\theta)): \Theta \to \mathbb{R}^k$ contiene un rectángulo abierto de $\mathbb{R}^k $ no trivial. 
\end{teorema}

\ejemplo{
	Recordando el ejemplo anterior en el que probabamos que la familia de distribuciones $N(\theta, \theta)$ no era completa, podemos ver que la imagen de la aplicación $q = \left(\frac{1}{\theta}, -\frac{1}{2\theta^2}\right)$ no contiene ningún rectángulo abierto de $\mathbb{R}^2$:
	$$q_2(\theta) = -\frac{1}{2}q_1(\theta)^2, \ \forall \theta > 0$$
	Lo cual es una rama de parábola, y por lo tanto no contiene ningún abierto de $\mathbb{R}^2$
	\begin{center}		
		\begin{tikzpicture}
			\begin{axis}[
				axis lines=middle,
				xlabel={$x$},
				ylabel={$y$},
				samples=100,
				domain=0:2, % Se dibuja solo para x > 0
				xmin=-0.5, xmax=4,
				ymin=-1, ymax=0.25,
				ticks=none
			]
			\addplot[red, thick] {-(1/2)*x^2};
			\node at (axis cs: 2,-0.5) {\small $y = -\frac{1}{2} x^2$};
			\end{axis}
		\end{tikzpicture}
	\end{center}
}

\ejemplo{
	Sea $X \sim Bernoulli(\theta) \implies P(X = x) = \theta^x(1 - \theta)^{1 - x} \implies$
	$$\text{Sea una m.a.s. de tamaño } n \implies P(\vec{X} = \vec{x}) = \theta^{\sum x_i}(1- \theta)^{n - \sum x_i} = \left(\frac{\theta}{1 - \theta}\right)^{\sum x_i} \cdot (1 - \theta)^n = $$
	En este caso, la distribución no tiene exponencial, pero se puede refactrizar para obtenerlo: \\
	Recordemos que $a^b = e^{b \cdot ln(a)}$, entonces, aplicando ésto aquí obtenemos que: 
	$$ =  e^{\sum x_i \cdot ln(\frac{\theta}{1 - \theta})} \cdot (1 - \theta)^n$$
	En donde tenemos por la forma general de las familias exponenciales k-paramétricas, que es de la forma: 
	\begin{itemize}
		\item $T(\vec{x}) = \sum x_i$
		\item $q(\theta) = ln(\frac{\theta}{1 - \theta}) $
		\item $c(\theta) = (1 - \theta)^n $
		\item $h(\vec{x}) = 1$
	\end{itemize}
	Entonces, podemos obtener que como $(X_1 \ldots, X_n)$ pertenece a la famiia exponencial k-paramétrica, obtenemos que su estadístico natural $(T(\vec{x}) = \sum x_i)$ es suficiente. 
	Y como $$g(\theta) = \frac{\theta}{1 - \theta} \neq 0 \implies \text{ el estadístico natural } \sum x_i \text{ es minimal}$$
	Ya que un vector de una dimensión es siempre linealmente independiente si y sólo si no es nulo. \\
	Por último, veamos que el estadístico natural es completo: \\
	Para ello aplicaremos el teorema anterior y para ello analizaremos el conjunto imaagen de $q(\theta)$:
	$$Im(q(\theta)) = \{ln(\frac{\theta}{1 - \theta}) : \theta \in \Theta = (0,1)\} \text{ ya que se trata de una distribución Bernoulli}$$
	Por último dado que la función tiene una gráfica así:
	\begin{center}
	\includegraphics[width=0.5\linewidth]{Graficos/geogebra-export.png}
	\end{center}
	La imagen coincide con los números reales $\mathbb{R}$ por lo que contiene un conjunto abierto no trivial y por tanto el estadístico natural $\sum x_i$ es completo. 
}

Ahora veamos otro ejemplo en el que no se puede provocar la perteniencia a la familia exponencial: 

\ejemplo{
	Sea $X \sim U(\theta, 7\theta) \implies f_{\theta}(x) = \frac{1}{6\theta}\cdot I_{(\theta, 7\theta)}(x) \implies$
	$$f_{\theta}(\vec{x}) = \left(\frac{1}{6\theta}\right)^n \cdot I_{(\theta, 7\theta)}(x_{(1)}) \cdot I_{(\theta, 7\theta)}(x_{(n)})$$
	Entonces por el Teorema de Factorización de Fisher, tenemos que el estadístico $T(\vec{x}) = (x_{(1)}, x_{(n)})$ es suficiente.\\
	\begin{observación}
		Como la función indicadora depende de $\theta$, entonces la distribución no pertenece a la familia exponencial
	\end{observación}
	Recordemos además cuáles son las funciones de densidad de los máximos y lo mínimos (\underline{aunque en la práctica real hace falta demostrarlo siempre}): 
	$\begin{cases}
		f_{\theta}(x_{(n)}) = n(F_X(x))^{n-1} \cdot f_X(x) \\
		f_{\theta}(x_{(1)}) = n(1 - F_X(x))^{n-1} \cdot f_X(x)
	\end{cases} \implies$
	Saquemos primero la esperanza del máximo: \\
	$$E_{\theta}[x_{(n)}] = \int_{\theta}^{7\theta} x\cdot n\left(\frac{x - \theta}{6\theta}\right)^{n-1} \cdot \frac{1}{6\theta}dx =$$
	Para calcularla introduzcamos el cambio de variable, para usar la regla de la multipicación: $\begin{cases} x = u \\ dv = f_{\theta}(x_{(n)}) \end{cases} \implies$
	$$ = \left[x \cdot \left(\frac{x - \theta}{6\theta}\right)^n\right]^{7\theta}_{x = \theta} - \int_{\theta}^{7\theta} \left(\frac{x - \theta}{6\theta}\right)^{n} dx = 7\theta - \int_{y = 0}^{y = 1} y^n \cdot 6\theta dy = 7\theta - 6\theta \cdot \frac{1}{n+1} = \theta\frac{7n+1}{n+1}\theta$$
	Ahora, realicemoslo para el mínimo:\\
	$$E[x_{(1)}] = \int_{\theta}^{7\theta} x\cdot n\left(1 - \frac{x - \theta}{6\theta}\right)^{n-1} \cdot \frac{1}{6\theta}dx = \int^{7\theta}_{\theta} x\cdot n\left(\frac{7\theta - x}{6\theta}\right)^{n-1} \cdot \frac{1}{6\theta}dx =$$
	Al igual que antes introduzcamos un cambio de variable para calcularla: 
	$$ = \left[x \cdot \left(1 - \frac{x - \theta}{6\theta}\right)^n\right]^{7\theta}_{x = \theta} - \int_{\theta}^{7\theta} \left(\frac{7\theta - x}{6\theta}\right)^{n} dx = -\theta - \int_{1}^{0} y^n(-6\theta)dy =  -\theta - 6\theta\left[\frac{y^{n+1}}{n+1}\right]^{1}_{0} = -\theta - 6\theta\frac{1}{n+1}$$
	HAY QUE TERMINAR ESTE EJEMPLO
}


\begin{teorema}[Teorema de Basu]
	Si $T = T(X_1, \ldots, X_n)$ es un estadístico suficiente y completo y $U = U(X_1, \ldots, X_n)$ es un estadístico ancilario, entonces $T$ y $U$ son independientes
\end{teorema}

\begin{proof}
	Sea la muestra $(x_1, \ldots, x_n)$, un parámetro $\theta$ y sea un estadístico $T$ suficiente, entonces por definición tenemos que: 
	$$f(\vec{x} | T = t) \text{es indendiente de } \theta \implies$$
	$$f(U = u | T = t) = \sum_{\vec{x} : U(\vec{x}) = u} f(\vec{x} | T = t) \text{ es independiente de } \theta$$
	Aplicando las propiedades de los condicionales, tenemos que: 
	$$f_{\theta}(u) = f_{\theta}(t)f_{\theta}(u | t) \text{ pero como U es ancilario y T es completo, entonces:} \implies f(u) = f_{\theta}(t)f(u | t)$$

	Tomemos la función $h(t) = f(u | t) - f(u)$, entonces:
	$$E[h(T)] = E[f(u | T) - f(u)] = E[f(u | T)] - E[f(u)] = \int f(u | t)f(t)dt - \int f(u)f(t)dt = f(u) - f(u) = 0$$
	Entones como $T$ es completo, tenemos que $h(T) = 0$ c.s. $\implies f(u | t) = f(u) \implies$
	$$f(u | t) = \frac{f(u, t)}{f(t)} \iff f(t, u) = f(u)f(t) \implies U \text{ y } T \text{ son independientes}$$
\end{proof}

\ejemplo{
	Sea $X \sim U(0, \theta)$, veamos que $X_{(n)}$ y $\frac{X_{(1)}}{X_{(n)}}$ son independientes:\\
	$$ X \sim U(0, \theta) \implies f_{\theta}(x) = \frac{1}{\theta} \cdot I_{(0, \theta)}(x) \implies F_{X_{(n)}}(x) = P(X_{(n)} \leq x) = P(X_1 \leq x, \ldots, X_n \leq x) = $$ $$ = P(X_1 \leq x)\cdot \ldots \cdot P(X_n \leq x) = (F(x))^n = \left(\frac{x}{\theta}\right)^{n} \implies f_{\theta}(x) = n\left(\frac{x}{\theta}\right)^{n-1} \cdot \frac{1}{\theta} = \frac{n}{\theta^n}x^{n-1}$$
	Dado que por el Teorema de Factorización de Fisher, esta distribución es suficiente, consideraremos que $X_{(n)}$ es el estadístico suficiente y completo.\\
	Veamos ahora la función de densidad de $\frac{X_{(1)}}{X_{(n)}}$, para ello calculemos la función de densidad de $X_{(1)}$:
	$$F_{X_{(1)}}(x) = P(X_{(1)} \leq x) = 1 - P(X_{(1)} > x) = 1 - P(X_1 > x, \ldots, X_n > x) = $$ $$ = 1 - P(X_1 > x)\cdot \ldots \cdot P(X_n > x) = 1 - (1 - F(x))^n = 1 - \left(1 - \frac{x}{\theta}\right)^n \implies f_{\theta}(x) = n\left(1 - \frac{x}{\theta}\right)^{n-1} \cdot \frac{1}{\theta} = \frac{n}{\theta^n}(\theta - x)^{n-1}$$
}

\ejemplo{
	Sea una m.a.s. de tamaño $n$ de una población $X \sim N(\theta, \sigma)$ se tiene que: 
	$$\bar{X} \sim N(\mu, \frac{\sigma}{n}) \text{ ya que } \begin{cases} E[\bar{X}] = E\left[\frac{1}{n}\sum_{i = 1}^{n}X_i\right] = \frac{1}{n}\sum_{i = 1}^{n}E[X_i] = \frac{1}{n}n\theta = \theta \\ Var(\bar{X}) = Var\left(\frac{1}{n}\sum_{i = 1}^{n}X_i\right) = \frac{1}{n^2}\sum_{i = 1}^{n}Var(X_i) = \frac{1}{n^2}n\sigma = \frac{\sigma}{n} \end{cases}$$
	Y como se trata de una distribución perteneciente a la familia exponencial 2-paramétrica, entonces $\bar{X}$ es suficiente. Queda demostrar que sea completo, para ello, consideremos la función de densidad de $\bar{X}$:
	$$\bar{X} \sim N(\mu, \frac{\sigma}{n}) \implies f_{\theta}(x) = \sqrt{\frac{n}{2\pi\sigma}}e^{-\frac{n}{2\sigma}(x - \theta)^2}$$
	La completitud se da por definición. No obstante es una integral demasiado dificil de calcular, por lo que se puede deducir que $\bar{X}$ es completo. 
	Por el Teorema de Fisher sabemos que: 
	$$\frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1) \implies S^2 \sim \frac{\sigma^2}{n-1}\chi^2(n-1) \implies S^2 \text{ es un estadístico ancilario} \implies$$
	Por el teorema de Basu, $\bar{X}$ y $S^2$ son independientes
}
    
    
\subsection{Principios de reducción de datos}

\begin{teorema}[Principio de verosimilitud]
	Dada una m.a.s. consideramos que la distribución de la muestra es una función de $\theta$ y no de la muestra en sí, es decir, $L(\theta | x_1, \ldots, x_n) = f(x_1, \ldots, x_n | \theta)$
\end{teorema}

\begin{proposición}
	Sean dos muestras $X = (x_1, \ldots, x_n)$ e $Y = (y_1, \ldots, y_m)$, si $\exists c(x, y)$ tal que $L_1(\theta | x) = c(x, y)L_2(\theta | y)$, entonces la evidencia estadística que suministran ambas muestras es idéntica. Es decir, $Ev(E_1, \vec{x}) = Ev(E_2, \vec{y})$
\end{proposición}

\ejemplo{
	Sea un modelo $Binomial(n, \theta)$ la evidencia que se obtiene del mismo, sobre $\theta$, cuando se observan $t$ éxitos en $n$ repeticiones, es la misma que la obtenida por un modelo $Binomial negativo(t, \theta)$, siempre que se suponga que se han realizado $n$ repeticiones hasta obtener $t$ éxitos.\\
	$$ \text{En el primer caso tenemos que: } 
	f(t|\theta) = \binom{n}{t}\theta^t(1 - \theta)^{n-t} $$ $$\text{ y en el segundo caso: } g(\text{n repeticiones hasta obtener t éxitos} | \theta) = \binom{n-1}{t-1}\theta^t(1 - \theta)^{n-t}$$
	Por tanto, se puede aplicar el principio de verosimilitud, tomando la constante $c(t, n) = \frac{n}{t}$. Es decir, si para estimar la probabilidad con la que sale cara una moneda, se tira ésta $n$ veces,  y es $t$ el numero de cara s que se han presentado (modelo binomial), se obtiene la misma evidencia para $\theta$, que si se repite el experimento hasta observar $t$ caras y para ello ha habido que realizar $n$ repeticiones (modelo binomial negativo)
}
 

\begin{proposición}[Principio de suficiencia]
	En un experimento $E = \left(\chi, \theta, \mathcal{P} = \{f(\vec{x}|\theta) : \theta \in \Theta\} \right)$, si $T = T(\vec{X})$ es un estadístico suficiente para $\theta$ y se tiene que $T(\vec{x}) = T(\vec{y})$, entonces las dos muetras $\vec{x}$ e $\vec{y}$ suministran la misma evidencia estadística, es decir, $Ev(E, \vec{x}) = Ev(E, \vec{y})$
\end{proposición}

\ejemplo{
	En un experimento consistente en la repetición de $n$ pruebas de Bernoulli, la evidencia estadística que se obtiene de dos puntos muestrales con el mismo número de éxitos es la misma.
}

\begin{proposición}[Principio de condicionalidad]
	El principio de condicionalidad dice que si un mecanismo aleatorio no depende del valor a determinar $\theta$, no proporciona evidencia sobre él. Es decir, si $T = T(\vec{X})$ es un estadístico suficiente para $\theta$ y $T(\vec{x}) = T(\vec{y})$, entonces las dos muestras $\vec{x}$ e $\vec{y}$ suministran la misma evidencia estadística, es decir, $Ev(E, \vec{x}) = Ev(E, \vec{y})$
\end{proposición}

\ejemplo{
	Dados dos experimentos $E_1 = (\chi^n, f_1(\vec{x}|\theta))_{\theta \in \Theta \subset \mathbb{R}^{\ell}}$ y $E_2 = (\chi^m, f_2(\vec{y}|\theta))_{\theta \in \Theta \subset \mathbb{R}^{\ell}}$ y el lanzamiento de una moneda al aire representado por la v.a. $J$ tal que $P(J = 1) = P(J = 2) = \frac{1}{2}$, si $E = (\chi^n \cup \chi^m \times \{1, 2\}, f(\vec{x}, j|\theta))_{\theta \in \Theta \subset \mathbb{R}^{\ell}}$ es el experimento mixto representado por la v.a. $(Z, J)$ tal que $Z = \begin{cases} X & \text{ si } J = 1 \\ Y & \text{ si } J = 2 \end{cases}$, entonces $Ev(E, (\vec{x}, 1)) = Ev(E_1, \vec{x})$ y $Ev(E, (\vec{y}, 1)) = Ev(E_2, \vec{y})$
}
  
\begin{teorema}[Teorema de Birnbaum]
	El principio de verosimilitud es equivalente a los principios de suficiencia y condicionalidad.
\end{teorema}  

\subsection{Ejercicios}
\begin{problem}{2.1}
	Sea $X$ muestra de una población $N(0, \sigma)$, ¿Es $T(X) = |X|$ un estadístico suficiente?
\end{problem}
\begin{sol}
	Recordemos la definición de estadístico suficiente por definición: Dado un estadístico $T$ en este caso $T(X) = |X|$ y una muestra $\vec{x}$ se dice que $T$ es suficiente si $f_{\theta}(\vec{X} = \vec{x} | T = t)$ \\
	Empecemos definiendo la distribución que sigue el estadístico $T$: 
	$$P(|X| \leq a) = P(-a \leq X \leq a) = P(X \leq a) - P(X \leq -a) = F(a) - F(-a) = 2F(a) = 2\int_{0}^{a}f_{X}(x)dx$$
	$$\implies f_{|X|}(x) = 2f_{X}(x) = 2\cdot \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x - \mu)^2}{2\sigma^2}} \implies \text{ Si } X \sim N(0, \sigma) \implies f_{|X|}(x) = \frac{\sqrt{2}}{\sigma\sqrt{\pi}}\cdot e^{-\frac{x^2}{2\sigma^2}}$$
	Entonces veamos el cociente de las dos funciones de densidad:
	$$P(X = x | T = t) = \frac{f_{\sigma}(x)}{f_{\sigma}(t)} = \frac{\frac{1}{\sigma\sqrt{2\pi}}\cdot e^{-\frac{x^2}{2\sigma^2}}}{\frac{\sqrt{2}}{\sigma\sqrt{\pi}} \cdot e^{-\frac{x^2}{2\sigma^2}}} = \frac{1}{2}$$
	Lo cual no depende del parámetro $\sigma$ y por tanto por definición, el estadístico es suficiente. 
\end{sol}

\begin{problem}{2.2}
	Encuentra un estadístico suficiente para una m.a.s. de tamaño $n$ de cada una de las siguientes poblaciones: 
	\begin{enumerate}
		\item Beta($\alpha, \beta$) : $\alpha > 0, \beta > 0$
		\item Gamma($a, p$) : $a > 0, p > 0$
		\item $p_{N_1 N_2}(x) = \frac{1}{N_2 - N_1} \cdot I_{N_1 + 1, N_1 +2 \ldots, N_2}(x)$ : $N_1 < N_2$
		\item $f_{\theta}(x) = e^{-x + \theta}\cdot I_{\theta, \infty}(x)$
		\item $f_{\mu, \sigma}(x) = \frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(\ln(x - \mu))^2}\cdot I_{0, \infty}$
		\item $p_{\theta, p}(x) = (1 - p)p^{x - \theta} \cdot I_{\theta, \theta + 1, \ldots }(x)$
	\end{enumerate}
\end{problem}
\begin{sol}
	\begin{enumerate}
		%Primer caso%
		\item Sea $X \sim Beta(\alpha, \beta) \implies$ $$f_{\alpha, \beta}(x) = \frac{x^{\alpha -1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)} \implies f_{\alpha, \beta}(x_1, \ldots, x_n)(x) = \prod_{i = 1}^{n}f_{\alpha, \beta}(x_i) = \frac{(\prod_{i = 1}^{n}x_i)^{\alpha -1}(\prod_{i = 1}^{n}(1-x))^{\beta -1}}{B(\alpha, \beta)^n}$$
		Entonces por el Teorema de Factorización de Fisher, tenemos que $$f_{\theta}(x) = h(x) \cdot g_{\theta}(T(x)) \implies T(\vec{x}) = (\prod_{i = 1}^{n}x_i, \prod_{i = 1}^{n}(1 - x_i))$$
		%Segundo caso%
		\item Sea $X \sim Gamma(\alpha, \beta) \implies$ $$f_{\alpha, \beta}(x) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1} \cdot e^{-x\beta} \implies f_{\alpha, \beta}(x_1, \ldots, x_n) = \prod_{i = 1}^{n}f_{\alpha, \beta}(x_i) = \frac{\beta^{n\alpha}}{\Gamma(\alpha)}(\prod_{i = 1}^{n}x_i)^{\alpha-1} \cdot e^{-\beta \sum_{i = 1}^{n}x_i}$$
		Entonces por el Teorema de Factorización de Fisher: $$f_{\theta} = h(x) \cdot g_{\theta}(T(x)) \implies T(\vec{x}) = (T_1(\vec{x}), T_2(\vec{x})) = (\sum_{i = 1}^{n}x_i, \prod_{i = 1}^{n}x_i)$$
		%Tercer caso%
		\item Sea $X \sim \text{Uniforme Discreta}(N_1 , N_2) \implies$
		$$ f_{(N_1, N_2)}(x) = \frac{1}{N_2 - N_1} \cdot I_{N_1 + 1, N_1 +2 \ldots, N_2}(x) \implies f_{(N_1, N_2)}(x_1, \ldots, x_n) = \prod_{i = 1}^{n}f_{(N_1, N_2)}(x_i) =$$ $$ = \frac{1}{(N_2 - N_1)^n} \cdot I_{\{N_1 + 1, N_1 +2, \ldots, +\infty\}}(x_{(1)}) \cdot I_{\{-\infty, \ldots, N_2 - 1, N_2\}}(x_{(n)})$$
		Por el Teorema de Factorización de Fisher, tenemos que: 
		$$f_{\theta}(x) = h(x) \cdot g_{\theta}(T(x)) \implies T(\vec{x}) = (x_{(1)}, x_{(n)})$$
		%Cuarto caso%
		\item Sea una variable aleatoria con función de densidad $f_{\theta}(x) = e^{-x + \theta}\cdot I_{\theta, \infty}(x) \implies$
		$$ f_{\theta}(x_1, \ldots, x_n) = \prod_{i = 1}^{n}f_{\theta}(x_i) = \prod_{i = 1}^{n}e^{-x_i + \theta}\cdot I_{\theta, \infty}(x_i) = e^{-\sum_{i = 1}^{n}x_i + n\theta}\cdot I_{\theta, \infty}(x_{(1)})$$
		Por el Teorema de Factorización de Fisher, tenemos que:
		$$f_{\theta}(x) = h(x) \cdot g_{\theta}(T(x)) \implies T(\vec{x}) = (x_{(1)})$$
		%Quinto caso%
		\item Sea una varible aleatoria $X$ cuya función de densidad es: 
		$$f_{(\mu, \sigma)}(x) = \frac{1}{x\sigma\sqrt{2\pi}}\cdot e^{-\frac{1}{2\sigma^2}(ln(x) - \mu)^2}\cdot I_{0, \infty}(x) \implies f_{(\mu, \sigma)}(\vec{x}) = \prod_{i = 1}^{n}\frac{1}{x_i} \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n \cdot e^{-\frac{1}{2\sigma}\sum_{i = 1}^{n}(\ln(x_i) - \mu)^2}$$
		Desarrollando el cuadrado: $(ln(x_i) - \mu)^2 = ln^2(x_i) - 2\mu ln(x_i) + \mu^2$ y conociendo que la forma general de las funciones de densidad de las familias exponenciales 2-paramétricas es: 
		$$f_{(\mu, \sigma)}(x) = c(\mu, \sigma) \cdot h(x) \cdot e^{q_1(\mu, \sigma)T_1(x) + q_2(\mu, \sigma)T_2(x)} \implies f_{(\mu, \sigma)}(\vec{x}) = c(\mu, \sigma)^n \cdot h(\vec{x}) \cdot e^{q_1(\mu, \sigma)\sum T_1(x_i) + q_2(\mu, \sigma)\sum T_2(x_i)}$$
		Podemos ver que $T_1(\vec{x}) = \sum_{i = 1}^{n}ln(x_i)$ y $T_2(\vec{x}) = \sum_{i = 1}^{n}ln^2(x_i)$
		\begin{observación}
			Esta distribución en particular se la conoce como log-normal o distribucion de Tinaut y surge de tomar una distribución normal $N(\mu, \sigma)$ y aplicarle la función logarítmica, es decir: $Y = ln(X) \sim N(\mu, \sigma)$
		\end{observación}
		%Sexto caso%
		\item Sea una variable aleatoria $X$ cuya función de densidad es: 
		$$f_{(\theta, p)}(x) = (1 - p)p^{x - \theta} \cdot I_{\theta, \theta + 1, \ldots }(x) \implies f_{(\theta, p)}(\vec{x}) = (1-p)^n \cdot p^{\sum_{i = 1}^{n}x_i - n\theta} \cdot I_{\theta, \theta + 1, \ldots }(x_{(1)})$$
		Entonces por el Teorema de Factorización de Fisher, tenemos que: 
		$$ f_{\theta}(x) = h(x) \cdot g_{\theta}(T(x)) \implies T(\vec{x}) = (\sum_{i = 1}^{n}x_i, x_{(1)})$$
	\end{enumerate}
\end{sol}
\begin{problem}{2.3}
	Sea una m.a.s. de tamaño $n$ de una población con función de distribución $X \sim Uniforme(\theta, 4\theta)$ pruébese que $(X_{(1)}, X_{(n)})$ es un estadístico suficiente para $\theta$ pero no es completo. 
\end{problem}

\begin{sol}
	$$X \sim U(\theta, 4\theta) \implies f_{\theta}(x) = \frac{1}{3\theta} \cdot I_{(\theta, 4\theta)}(x) \implies f_{\theta}(x_1, \ldots, x_n) = \frac{1}{(3\theta)^n} \cdot I_{(\theta, 4\theta)}(x_{(1)}) \cdot I_{(\theta, 4\theta)}(x_{(n)})$$
	Entonces, por el Teorema de Factorización de Fisher, tenemos que: $f_{\theta}(x) = h(x) \cdot g_{\theta}(T(x)) \implies T(\vec{x}) = (x_{(1)}, x_{(n)})$ 
	Ahora veamos que el estadístico no es completo: 
	Para ello tenemos que sacar cuales son las distribuciones del mínimo y el máximo:
	\begin{enumerate}
		%El máximo%
		\item $$f_{\theta}(x_{(n)}) = n(F(x))^{n-1} \cdot f_{\theta}(x_{(n)}) = n\left(\frac{x - \theta}{3\theta}\right)^{n-1} \cdot \frac{1}{3\theta} = \frac{n}{3\theta^n}(x - \theta)^{n-1} \implies$$
		$$E[T_1] = \int_{\theta}^{4\theta}x \cdot n\left(\frac{x - \theta}{3\theta}\right)^{n-1} \cdot \frac{1}{3\theta}dx = \left[x\cdot \left(\frac{x - \theta}{3\theta}\right)^n\right]_{\theta}^{4\theta} - \int_{\theta}^{4\theta} \left(\frac{x - \theta}{3\theta}\right)^n dx = 4\theta - 3\theta\int_{0}^{1}y^n dy = $$ $$ = 4\theta - 3\theta\frac{1}{n+1} = \theta\frac{4n + 1}{n+1}$$
		%El minimo%
		\item $$f_{\theta}(x_{(1)}) = n(1 - F(x))^{n-1} \cdot f_{\theta}(x) = n\left(1 - \frac{x - \theta}{3\theta}\right)^{n-1} \cdot \frac{1}{3\theta} = \frac{n}{3\theta^n}(4\theta - x)^{n-1} \implies$$
		$$E[T_2] = \int_{\theta}^{4\theta} x \cdot n\left(1 - \frac{x - \theta}{3\theta}\right)^{n-1} \cdot \frac{1}{3\theta}dx = \left[x\cdot -\left(1 - \frac{x - \theta}{3\theta}\right)^n\right]_{\theta}^{4\theta} - \int_{\theta}^{4\theta} \left(\frac{4\theta - x}{3\theta}\right)^n dx = $$ $$ = \theta - 3\theta\int_{0}^{1}y^n dy = \theta - 3\theta\frac{1}{n+1} = \theta\frac{4n + 1}{n+1}$$
	\end{enumerate} 
\end{sol}

\begin{problem}{2.4}
	Sea una m.a.s. de una población que sigue una variable aleatoria $X$ con función de densidad: 
	$$f_{\theta}(x) = e^{-(x - \theta)} \cdot I_{(\theta, +\infty)}(x)$$
	\begin{enumerate}
		\item Pruébese que $T(\vec{X}) = X_{(1)}$ es un estadístico suficiente y completo para $\theta$
		\item Mediante el Teorema de Basu, pruébese que $X_{(1)}$ y $S^2$ son independientes. 
	\end{enumerate} 
\end{problem}
\begin{sol}
	Antes de comenzar con la resolución del problema, calculemos la función de densidad de la muestra: 
	$$f_{\theta}(\vec{x}) = e^{-\sum_{i = 1}^{n}x_i + n\theta} \cdot I_{\theta, +\infty}(x_{(1)})$$
	\begin{enumerate}
		\item Para demostrar la suficiencia usaremos el Teorema de Factorización de Fisher con: $f_{\theta}(\vec{x}) = h(\vec{x}) \cdot g_{\theta}(T(\vec{x})) = e^{-\sum_{i = 1}^{n}x_i + n\theta} \cdot I_{\theta, +\infty}(x_{(1)})$ y $T(\vec{x}) = (x_{(1)}, \sum x_i)$\\
		Ahora veamos la completitud: Sea una función real $g(x)$ que cumpla que: $$E[g(x_{(1)})] = 0 = \int_{\theta}^{+\infty}g(x)f_{\theta}(x)dx = \int_{\theta}^{+\infty}g(x)e^{-x + \theta}dx$$
		Si tomamos cómo función $H(x) = \int_{t > 0}^{+\infty} e^{-y}$ sabemos que dicha función es positiva en todo su dominio, por lo que la únia manera de que la integral anterior sea nula es que $g(x) = 0$ c.s. $\implies$ el estadístico es completo.
		\item El Teorema de Basu, nos dice que si $T$ es un estadístico suficiente y completo y $U$ es un estadístico ancilario, entonces $T$ y $U$ son independientes. En este caso, sabemos que $X_{(1)}$ es completo y suficiente, y por el Teorema de Basu, sabemos que $X_{(1)}$ y $S^2$ son independientes.
		Entonces debemos comprobar que $S^2$ es ancilario: 
		$$ f_{\theta} = e^{-(x - \theta)} \implies X \sim Exponencial(1) + \theta \implies S^2 = \frac{1}{n-1}\sum_{i = 1}^{n}(X_i - \bar{X})^2 =$$  $$= \frac{1}{n-1}\sum_{i = 1}^{n} (X_i - \theta + \theta - \bar{X})^2 = \frac{1}{n-1}\sum_{i = 1}^{n} (X_i - \theta - (\bar{X} - \theta)^2) = \frac{1}{n-1}\sum_{i = 1}^{n} (Z_i - \bar{Z})^2 \implies $$ $$\implies Z \sim Exponencial(1) \text{ no depende de } \theta \implies S^2 \text{ es ancilario}$$
		Por tanto por el Teorema de Basu, ambos estadísticos son independientes.
	\end{enumerate}
\end{sol}
\begin{problem}{2.5}
	Sea una población que sigue una variable aleatoria $X$ con fucniones de masa: 
	$$P_{\theta}(X = -1) = \theta \quad P_{\theta}(X = x) = (1 - \theta)^2\cdot\theta^x\cdot I_{0 \cup \mathbb{N}}(x)$$
	Con $\theta \in (0, 1)$	y el tamaño de la muestra $n = 1$ ¿El estadístico $X$ es completo?
\end{problem}
\begin{sol}
	$$E[X] = \sum_{i = 0}^{+\infty} P(X = i) \cdot i = -1\cdot P(X = -1) + \sum_{i = 1}^{+\infty} P_{\theta}(X = i) \cdot i = -1 \cdot \theta + \sum_{i = 1}^{n} (1 - \theta)^2 \cdot i\theta^{i} = $$ $$ = -\theta + (1 - \theta) \sum_{i = 1}^{n}i\theta^{i} = -\theta + (1 - \theta)t'(x) \text{ donde } t(x) = \sum_{i = 1}^{\infty}\theta^i = \frac{\theta}{1 - \theta} \implies t'(x) = \frac{1}{(1-\theta)^2}$$ $$\implies E[X] = -\theta + (1 - \theta)^2\theta(\frac{1}{(1 - \theta)^2}) = -\theta + \theta = 0$$
	Entonces, como $E[X] = 0$, $X$ no es completo.
\end{sol}
\begin{problem}{2.6}
	Sea m.a.s. de tamaño $n$ de una población $N(\alpha\sigma, \sigma) : \alpha \in \mathbb{R}$ veamos si el estadístico $T(\vec{X}) = (\sum_{i = 1}^{n}X_i, \sum_{i = 1}^{n}X_i^2)$ es suficiente y completo para $\sigma$.
\end{problem}
\begin{sol}
	$$X \sim N(\alpha\sigma, \sigma) \implies f_{\sigma}(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x - \alpha\sigma)^2}{2\sigma^2}} \implies f_{\sigma}(\vec{x}) = \prod_{i = 1}^{n}f_{\sigma}(x_i) = \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n \cdot e^{-\frac{1}{2\sigma^2}\sum_{i = 1}^{n}(x_i - \alpha\sigma)^2}$$
	Entonces descomoponiendo: $(x_i - \alpha\sigma)^2 = x_i^2 + \alpha^2\sigma^2 - 2\alpha\sigma x_i \implies$
	$$f_{\sigma}(\vec{x}) = \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n \cdot e^{-\frac{1}{2\sigma^2}\sum_{i = 1}^{n}x_i^2 + \frac{\alpha}{\sigma}\sum_{i = 1}^{n}x_i  - \frac{n\alpha^2}{2}} $$
	Entonces por el Teorema de Factorización de Fisher, tenemos que el estadístico $T(\vec{x}) = (\sum_{i = 1}^{n}x_i, \sum_{i = 1}^{n}x_i^2)$ es suficiente. \\
	Ahora veamos si es completo:\\
	$$E\left[\sum_{i = 1}^{n}X_i\right] = n\alpha\sigma \quad E\left[\sum_{i = 1}^{n}X_i^2\right] = n\sigma^2 + n\alpha^2\sigma^2 = (1 + n\alpha^2)n\sigma^2$$
	Pero la función 
	$$g(\vec{T}) = \frac{1}{1 + n\alpha^2}\left(\sum_{i = 1}^{n}X_i\right)^2 - \frac{1}{1 + \alpha^2}\sum_{i = 1}^{n}X_i^2 \implies E[g(\vec{T})] = 0$$
	Pero la función no es nula en casi todo punto, por lo que no es completo. 
\end{sol}
\begin{problem}{2.7}
	Sea una población con variable aleatoria $X \sim Uniforme(\theta, \theta + 1)$ y una m.a.s. de tamaño $n$ de dicha población. Entonces, 
	\begin{enumerate}
		\item Encuentra un estadístico suficiente para $\theta$
		\item Comprueba si el recorrido muestral $R = X_{(n)} - X_{(1)}$ es un estadístico ancilario
		\item Si la función de de distribución sufriera un desplazamiento de $F_{\theta}(x) = F(x - \theta)$ Comprueba si se conserva la propiedad del apartado anterior
	\end{enumerate}
\end{problem}
\begin{sol}1
	\begin{enumerate}
		\item $X \sim Uniforme(\theta, \theta + 1) \implies f_{\theta}(x) = \frac{1}{\theta + 1 - \theta} \cdot I_{(\theta, \theta + 1)} = 1 \cdot I_{(\theta, \theta +1)}(x) \implies f_{\theta}(\vec{x}) = I_{(\theta, \theta + 1)}(x{(1)})\cdot I_{(\theta, \theta +1)}(x_{(n)})  \implies$ por el Teorema de Factorización de Fisher, el estadístico $T(\vec{x}) = (x_{(1)}, x_{(n)})$ es suficiente.
		\item Calculemos las funciones de distribución del máximo y el mínimo: 
		\begin{itemize}
			\item $$f_{\theta}(x_{(n)}) = n(F(x))^{n-1} \cdot f_{\theta}(x_{(n)}) = n\left(\frac{x - \theta}{1}\right)^{n-1} \cdot 1 = n(x - \theta)^{n-1}$$ 
			\item $$f_{\theta}(x_{(1)}) = n(1 - F(x))^{n-1} \cdot f_{\theta}(x_{(1)}) = n\left(1 - \frac{x - \theta}{1}\right)^{n-1} \cdot 1 = n(\theta + 1 - x)^{n-1} \implies$$
			Cuando intentamos calcular la probabilidad $P(R \leq r)$ estamos calculando la probabilidad de que el máximo y el mínimo estén a menos de una distancia $r$, entonces: 
			$$P(R \leq r) = \int_{\theta}^{\theta +1} f_{X_{(n)}}(x) \cdot P(x - X_{(1)} \leq r)dx$$
			Intuitivamente, esta expresión nos dice la probabilidad de que un punto $x$ del intervalo sea el máximo por la probabilidad de que la distancia de ese número menos el mínimo esté a una distancia menor que $r \implies$
			$$P(R \leq r) = \int_{\theta}^{\theta +1} n(x - \theta)^{n-1} \cdot P(x - X_{(1)} \leq r)dx = \int_{\theta}^{\theta +1} n(x - \theta)^{n-1} \cdot P(X_{(1)} \geq x - r)dx =$$ $$ = \int_{\theta}^{\theta +1} n(x - \theta)^{n-1} \cdot (1 - P(X_{(1)} \leq x - r))dx = \int_{\theta}^{\theta +1} n(x - \theta)^{n-1} \cdot (\theta + 1 - x + r)^n dx = $$ $$ = \int_{0}^{1} n y^{n-1}\cdot (1 + r - y)^{n}dy$$
			Donde $y = x - \theta$ y $r = r - \theta$ y dado que es una expresión que no depende de $\theta$ llegamos a que el estadístico es ancilario. 
			\item Si tenemos en cuenta el caso general del desplazamiento en una distribución uniforme: $$F_{\theta}(x) = F(x - \theta)  \implies P:{\theta}(r) = P(X_{(n)} - X_{(1)} \leq r) \implies$$
			Si hacemos el cambio de varibale $Z_i = X_i - \theta$ obtenemos que $Z_{(n)} - Z_{(1)} = X_{(n)} - X_{(1)}$ y por tanto, el recorrido muestral sigue siendo un estadístico ancilario.  
		\end{itemize}
	\end{enumerate}
\end{sol}
\begin{problem}{2.8}
	Dada una m.a.s. de tamaño $n$ de una población donde se sigue la siguiente función de densidad: 
	$$f_{\theta}(x) = \frac{2x}{\theta^2}\cdot I_{(0, \theta)}(x)$$
	Entonces: 
	\begin{enumerate}
		\item Comprueba si $T(\vec{X}) = (X_{(n)}, \prod_{i = 1}^{n}X_i)$ es suficiente para $\theta$ y si además es minimal. 
		\item Comprueba también si $T_2(\vec{x}) = X_{(n)}$ es un estadístico completo. 
	\end{enumerate}
\end{problem}
\begin{sol}
	\begin{enumerate}
		\item $$f_{\theta}(x) = \frac{2x}{\theta^2} \cdot I_{(0, \theta)}(x) \implies f_{\theta}(\vec{x}) = \prod_{i = 1}^{n}f_{\theta}(x_i) = \frac{2^n}{\theta^{2n}} \cdot \prod_{i = 1}^{n}x_i \cdot I_{(0, \theta)}(x_{(n)})$$
		Entonces por el Teorema de Factorización de Fisher, el estadístico $T(\vec{x}) = (X_{(n)}, \prod_{i = 1}^{n}X_i)$ es suficiente. Para comprobar si es minimal, veamos que: \\
		Dadas dos muestras $\vec{x}$ y $\vec{y} \implies$ $$\frac{f_{\theta}(\vec{x})}{f_{\theta}(\vec{y})} = \frac{\frac{2^n}{\theta^{2n}} \cdot \prod_{i = 1}^{n}x_i \cdot I_{(0, \theta)}(x_{(n)})}{\frac{2^n}{\theta^{2n}} \cdot \prod_{i = 1}^{n}y_i \cdot I_{(0, \theta)}(y_{(n)})} = \frac{\prod_{i = 1}^{n}x_i \cdot I_{(0, \theta)}(x_{(n)})}{\prod_{i = 1}^{n}y_i \cdot I_{(0, \theta)}(y_{(n)})}$$
		Entonces, el cociente no depende de $\theta \iff T(\vec{x}) = T(\vec{y})$ por lo que el estadístico es minimal.
		\item Comprobemos que el estadístico $T_2(\vec{x}) = X_{(n)}$ es completo: \\
		Cómo es un máximo la función de densidad sigue una fórmula: 
		$$f_{X_{(n)}} = n(F(x))^{n-1} \cdot f_{X}(x) = n\left(\frac{x^2}{\theta^2}\right)^{n-1} \cdot \frac{2x}{\theta^2} = 2n\left(\frac{x^{2n-1}}{\theta^{2n}}\right)$$
		Entonces, para una función real $g(x)$ queremos ver si que $E[g(X_{(n)})] = 0$ implica que $g(x) = 0$ c.s.\\
		$$E[g(X_{(n)})] = \int_{0}^{\theta}g(x)2n\left(\frac{x^{2n-1}}{\theta^{2n}}\right)dx = \frac{2n}{\theta^{2n}}\int_{0}^{\theta}g(x) \cdot x^{2n-1}dx = 0 \implies$$
		Aplicando la Regla de Leibniz para derivadas de integrales: 
		$$\frac{d}{d\theta}\int_{a(\theta)}^{b(\theta)} f(x, \theta)dx = f(b(\theta), \theta) \cdot \frac{d}{d\theta}(b(\theta)) - f(a(\theta), \theta) \cdot \frac{d}{d\theta}(a(\theta)) + \int_{a(\theta)}^{b(\theta)} \frac{\partial}{\partial \theta}f(\theta, x) dx$$
		En nuestro caso, podemos sustituir usando: 
		\begin{itemize}
			\item $f(x, \theta) = g(x)\cdot x^{2n-1}$
			\item $b(\theta) = \theta$
			\item $a(\theta) = 0$
		\end{itemize}
		Entones tenemos que:
		$$\frac{d}{d\theta}\int_{0}^{\theta}g(x) \cdot x^{2n-1}dx = g(\theta)\cdot \theta^{2n-1} - g(0)\cdot 0^{2n-1} + \int_{0}^{\theta} \frac{\partial}{\partial \theta}(g(x) \cdot x^{2n-1})dx = $$ $$ = g(\theta)\cdot \theta^{2n-1} = 0 \implies g(\theta) = 0 \implies g(x) = 0 \text{ c.s.}$$
		Entones el estadístico $T_2(\vec{x}) = X_{(n)}$ es completo.
	\end{enumerate}
\end{sol}