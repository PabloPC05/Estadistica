
\section{Muestreo}

\subsection{Modelos estadísticos}

\label{Modelo estidístico}
\begin{definición}[Modelo Estadístico]
Sea $(\omega, \mathcal{A}, \mathbb{P})$ un espacio de probabilidad asociado a un experimento aleatorio $\mathcal{E}$, una variable aleatoria observable $X: \omega \to \mathbb{R}$ y su espacio medible asociado $(\chi, \mathcal{B})$.
\\Un \textbf{modelo estadístico} es una terna $(\chi, \mathcal{B}, F)$, donde:
\vspace{-\topsep}
\begin{itemize}
	\item $\chi$ es el espacio donde la variable aleatoria $X$ toma valores.
	\item $\mathcal{B}$ es la $\sigma$-álgebra asociada a $\chi$. Generalmente se toma que $\mathcal{B} = \mathcal{B}(\mathbb{R})$.
	\item $F$ es el conjunto de todas las posibles funciones de distribución que podemos considerar sobre $\mathcal{B}$.
\end{itemize}
\end{definición}

\begin{definición}[Modelo Estadístico Paramétrico]
Un \textbf{modelo estadístico paramétrico} es, al igual que el anteior, una terna $(\chi, \mathcal{B}, F)$, pero en este caso $F$ depende de un parámetro $\theta$ desconocido. \\
Se define $F = \{F_{\theta} : \theta \in \Theta\}$, donde $\Theta$ es el espacio paramétrico, es decir, el conjunto de todos los posibles valores que puede tomar $\theta$ para que $F_\theta$ sea una función de distribución.
De forma general se tiene que $\theta \in \Theta \subseteq \mathbb{R}^{k}$. \\
Además se tienen dos enfoques según cómo se tome el comportamiento de $\theta$:
\vspace{-\topsep}
\begin{itemize}
	\item \textbf{Enfoque frecuentista:} $\theta$ es un valor fijo pero desconocido.
	\item \textbf{Enfoque bayesiano:} $\theta$ es una variable aleatoria.
\end{itemize}
\end{definición}

\ejemplo{
	\begin{enumerate}
		\item Sea $X$ una variable aleatoria con distribución $N(\theta, 1)$, donde $\theta$ es un parámetro desconocido. Entonces, el modelo estadístico asociado a este experimento es $(\mathbb{R}, \mathcal{B}, F_{\theta}, \theta)$, donde $F_{\theta}(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(x-\theta)^2}$. En este caso $\Theta = \mathbb{R}$.
		\item Sea $X$-variable aleatoria con distribución $Bernouilli$, de parámetro $\theta$. Entonces en este caso, el modelo estadístico asociado es $(\{0,1\}, \mathcal{B}, F_{\theta}, \theta)$, donde $F_{\theta}(x) = \theta^x (1-\theta)^{1-x}$. En este caso $\Theta = [0,1]$.
	\end{enumerate}
}

\begin{definición}[Muestra aleatoria simple]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ un conjunto de variables aleatorias independientes e idénticamente distribuidas (i.i.d.) entonces a dicho conjunto se le conoce como \textbf{muestra aleatoria simple} de tamaño n.
\\ Por tanto tenemos que el modelo estadístico asociado es una terna $(\chi, \mathcal{B}, F)$, donde:
\vspace{-\topsep}
\begin{itemize}
	\item $\chi = \mathbb{R}^{n}$.
	\item $\mathcal{B} = \mathcal{B}(\mathbb{R}^{n})$.
	\item $F = \{F_{\theta}^n : \theta \in \Theta\}$, donde $\Theta$ es el espacio paramétrico, donde:
	      \[F_{\theta}^n(x_1, \ldots, x_n) = \prod_{i=1}^{n} F_{\theta}(x_i)\]
\end{itemize}
\end{definición}

\begin{definición}[Función de Distibución Empírica]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ m.a.s. $(n) \sim X$ y denotemos por $\left(X_{(1)}, \ldots, X_{(n)}\right)$ a la muestra ordenada de menor a mayor. $\forall x \in \mathbb{R}$ fijo definimos la función distribución empírica como la variable aleatoria $$F_{n}(x)=\frac{1}{n} \sum_{i=1}^{n} I_{(-\infty, x]}\left(X_{i}\right)$$

Observemos que

$$
	F_{n}(x)= \begin{cases}0 & \text { si } x<X_{(1)} \\ \frac{k}{n} & \text { si } X_{(k)}<x<X_{(k+1)}, k=1, \ldots, n-1 \\ 1 & \text { si } x \geq X_{(n)}\end{cases}
$$

Dada una realización particular de la muestra $\left(x_{1}, \ldots,
	x_{n}\right), F_{n}(x)$ es una función de distribución asociada a una variable
aleatoria discreta que toma valores $\left(x_{(1)}, \ldots, x_{(n)}\right)$ con
función de masa $\left(\frac{1}{n}, \ldots, \frac{1}{n}\right)$
\end{definición}

\ejemplo{
	Supongamos que tenemos una muestra aleatoria ordenada de tamaño $n = 5: \\
		x_{(1)} = 2, x_{(2)} = 3, x_{(3)} = 5, x_{(4)} = 7 x_{(5)} = 9$. \\
	Entones, por como se define la FDE $F_n(x) = \frac{1}{5} \sum_{i=1}^{5} I_{(-\infty, x]}(X_i)$, tenemos que:
	\[
		F_n(x) = \begin{cases}
			0           & \text{si } x \geq 2     \\
			\frac{1}{5} & \text{si } 2 \geq x < 3 \\
			\frac{2}{5} & \text{si } 3 \geq x < 5 \\
			\frac{3}{5} & \text{si } 5 \geq x < 7 \\
			\frac{4}{5} & \text{si } 7 \geq x < 9 \\
			1           & \text{si } x \geq 9
		\end{cases}
	\]
	De manera que cada vez que $x$ alcanza un valor de la muestra, la función de distribución empírica aumenta en $\frac{1}{n} =0,2$.
}

\begin{proposición}[Propiedades de la Función de Distribución Empírica]
Sea una muestra aleatoria \( X_1, X_2, \dots, X_n \) de una variable aleatoria \( X \) con función de distribución \( F(x) \). Definimos la función de distribución empírica como:

$$
	F_n(x) = \frac{1}{n} \sum_{i=1}^{n} \chi_{(-\infty,x]}(X_i)
$$

donde \( \chi_{(-\infty,x]}(X_i) \) es la función indicadora que toma el valor \( 1 \) si \( X_i \leq x \) y \( 0 \) en caso contrario.

\begin{enumerate}
	\item \textbf{Interpretación probabilística:}
	      La función indicadora \( \chi_{(-\infty,x]}(X_i) \) sigue una distribución Bernoulli con parámetro \( F(x) \), es decir:

	      $$
		      \chi_{(-\infty,x]}(X_i) \sim \text{Bernoulli}(F(x))
	      $$

	      Por tanto también podemos afirmar que: $$\chi_{(-\infty,x]}(X_i) \sim \text{Bin}(1, F(x))$$

	      Además, la suma de estas variables sigue una distribución binomial:

	      $$ \sum_{i=1}^{n} \chi_{(-\infty,x]}(X_i) \sim \text{Bin}(n, F(x)) $$
	\item \textbf{Esperanza y varianza:}
	      Para un valor fijo de \( x \), se cumple que:

	      $$ E[F_n(x)] = F(x) $$

	      lo que indica que \( F_n(x) \) es un estimador insesgado de \( F(x) \).
	      La varianza está dada por:

	      $$ V[F_n(x)] = \frac{F(x)(1 - F(x))}{n} \underset{n \to \infty}{\longrightarrow} 0 $$

	\item \textbf{Convergencia:}
	      \begin{enumerate}
		      \item \textbf{Convergencia casi segura:}

		            $$ F_n(x) \underset{n \to \infty}{\xrightarrow{c.s.}} F(x) $$

		      \item \textbf{Convergencia en distribución:}
		            Se cumple la normalidad asintótica:

		            $$ \frac{F_n(x) - F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}} \underset{n \to \infty}{\xrightarrow{d}} N(0,1) $$

	      \end{enumerate}

	\item \textbf{Intervalo de confianza para \( F(x) \):}
	      Dada una realización particular de la muestra \( (x_1, \dots, x_n) \), se puede construir un intervalo de confianza asintótico para \( F(x) \) de nivel \( 1 - \alpha \):

	      $$ IC_{1-\alpha}(F(x)) = \left( F_n(x) - \frac{z_{\alpha/2}}{2\sqrt{n}}, F_n(x) + \frac{z_{\alpha/2}}{2\sqrt{n}} \right) $$

	      donde \( z_{\alpha/2} \) es el cuantil de la distribución normal estándar.

\end{enumerate}
\end{proposición}

\begin{teorema}[de Glivenko-Cantelli]
	Sea $\left(X_{1}, \ldots, X_{n}\right)$ m.a.s.$(n) \sim X$ con función de distribucióm empírica $F_{n}(x)$ y sea $F(x)$ la función de distribución de $X$, es decir, de la población total. Entonces se cumple que:
	\[\lim _{n \rightarrow \infty} P\left(w: \operatorname{Sup}_{x}\left|F_{n}(x)-F(x)\right|<\epsilon\right)=1, \ \forall \epsilon>0\]
	Es decir, $\lVert F_{n}-F \rVert_{\infty} = \operatorname{Sup}_{x}\left|F_{n}(x)-F(x)\right| \xrightarrow[n \rightarrow \infty]{c.s.} 0$
\end{teorema}

\begin{proof}
	Para simplicidad, consideremos el caso de una variable aleatoria continua \( X \). Fijemos \( -\infty = x_0 < x_1 < \cdots < x_{m-1} < x_m = \infty \) tal que 

	\[
	F(x_j) - F(x_{j-1}) = \frac{1}{m} \quad \text{para } j = 1, \dots, m.
	\]
	
	Ahora, para todo \( x \in \mathbb{R} \), existe \( j \in \{1, \dots, m\} \) tal que \( x \in [x_{j-1}, x_j] \).
	
	\[
	F_n(x) - F(x) \leq F_n(x_j) - F(x_j) + \frac{1}{m},
	\]
	
	\[
	F_n(x) - F(x) \geq F_n(x_{j-1}) - F(x_{j-1}) - \frac{1}{m}.
	\]
	
	Por lo tanto,
	
	\[
	\|F_n - F\|_{\infty} = \sup_{x \in \mathbb{R}} |F_n(x) - F(x)| \leq \max_{j \in \{1, \dots, m\}} |F_n(x_j) - F(x_j)| + \frac{1}{m}.
	\]
	
	Dado que 
	
	\[
	\max_{j \in \{1, \dots, m\}} |F_n(x_j) - F(x_j)| \to 0 \quad \text{c.s.}
	\]
	
	por la ley fuerte de los grandes números, podemos garantizar que para cualquier \( \varepsilon > 0 \) y cualquier entero \( m \) tal que \( 1/m < \varepsilon \), podemos encontrar \( N \) tal que para todo \( n \geq N \),
	
	\[
	\max_{j \in \{1, \dots, m\}} |F_n(x_j) - F(x_j)| \leq \varepsilon - 1/m \quad \text{c.s.}
	\]
	
	Combinando con el resultado anterior, esto implica además que 
	
	\[
	\|F_n - F\|_{\infty} \leq \varepsilon \quad \text{c.s.},
	\]
	
	lo que es la definición de convergencia casi segura.
\end{proof}

\begin{corolario}
	El Teorema de Glivenko-Cantelli permite realizar una técnica estadística denominda \textbf{método de sustitución (Plug-In)} la cual se basa en la sustitución de parámetros desconocidos por sus estimaciones sobre una muestra. Por ejemplo:
	\begin{enumerate}
		\item Se puede estimar la media poblacional $\mu$ por la media muestral $\bar{X} = \int x\partial{F_n(x)} = \frac{1}{n}\sum_{i = 1}^{n}x_i $.
		\item Se puede estimar la varianza poblacional $\sigma^2$ por la varianza muestral $S^2 = \int (x - \bar{x})dF_n(x) = \frac{1}{n}\sum_{i = 1}^{n}(x_i - \bar{x})^2 = \sigma_n^2$.
	\end{enumerate}
\end{corolario}

\subsection{Estadísticos muestrales}

\begin{definición}[Estadístico muestral]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ m.a.s.$(n) \sim X$ y sea $T: \mathbb{R}^{n} \longrightarrow \mathbb{R}^{m}$ medible (integrable), bien definida y no dependiente de parámetros desconocidos, se le llama \textbf{estadístico muestral}
\end{definición}

\ejemplo{
	Desacamos los siguientes estadísticos muestrales:
	\begin{enumerate}
		\item Media muestral $T\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{n} \sum_{i=1}^{n}
			      X_{i}=\bar{X}$
		\item  Cuasivarianza muestral $T\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{n-1}
			      \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=S_{n}^{2}$
		\item $T\left(X_{1}, \ldots, X_{n}\right)=\left(\bar{X}, S_{n}^{2}\right)$
	\end{enumerate}
}
\subsection{Momentos}

Sea $\left(X_{1}, \cdots X_{n}\right)$ m.a.s.(n) de $X, \ \mu=E[X]$ y
$\sigma=\sqrt{V(X)}$.
\begin{definición}[Momento Muestral]
Se define el momento muestral de orden $k$ respecto al origen como
\[a_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}\]
y el momento muestral de orden $k$ respecto a la media como
\[b_{k}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{k}\]
\end{definición}

\begin{observación}
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\begin{itemize}
	\item El momento muestral de orden 1 respecto al origen es la media muestral $(a_1 =
		      \bar{X})$.
	\item El momento muestral de orden 2 respecto a la media es la varianza muestral
	      $(b_2 = \sigma_n^2)$.
\end{itemize}
\end{observación}

\begin{definición}[Momento Poblacional]
Se define el momento poblacional de orden $k$ respecto al origen como
\[\alpha_{k}=E\left[X^{k}\right]\]
y el momento poblacional de orden $k$ respecto a la media como
\[\beta_{k}=E\left[(X-\mu)^{k}\right]\]
\end{definición}

\begin{observación}
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\begin{itemize}
	\item El momento poblacional de orden 1 respecto al origen es la media poblacional
	      $(\alpha_1 = \mu)$.
	\item El momento poblacional de orden 2 respecto a la media es la varianza
	      poblacional $(\beta_2 = \sigma^2)$.
\end{itemize}
\end{observación}

\begin{proposición}[Propiedades asintóticas de los momentos muestrales]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ m.a.s.$(n)$ de $X \sim N(\mu, \sigma)$, entonces se cumple que:
\begin{enumerate}
	\item Momentos muestrales respecto al origen:
	      \begin{enumerate}
		      \item $$    a_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k} \xrightarrow[n \rightarrow \infty]{\text{c.s.}} \alpha_{k}=E\left[X^{k}\right]$$
		      \item $$     \bar{X} \xrightarrow[n \rightarrow \infty]{\text{c.s.}} \mu \quad \text{(Ley Fuerte de Kintchine, } \mu<\infty \text{)}$$
	      \end{enumerate}
	\item Momentos muestrales respecto a la esperanza/media:
	      \begin{enumerate}
		      \item  $$b_{k}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{k} \xrightarrow[n \rightarrow \infty]{\text{c.s.}} \beta_{k}=E\left[(X-\mu)^{k}\right]$$
		      \item $$ \sigma_n^2 = \frac{1}{n}\sum_{i = 1}^{n}(X_i - \bar{X})^2 \underset{n \to \infty}{\xrightarrow{c.s.}} \sigma^2 $$
		      \item $$ S_n^2 = \frac{1}{n-1}\sum_{i = 1}^{n}(X_i - \bar{X})^2\underset{n \to \infty}{\xrightarrow{c.s.}} \sigma^2 $$
	      \end{enumerate}
\end{enumerate}
\end{proposición}


\textbf{Propiedades asintóticas de los momentos muestrales}\\

\[
	\sqrt{n} \frac{a_k-\alpha_k}{\sqrt{a_{2k}-a_k^2}} \xrightarrow[n \rightarrow \infty]{d} Z \sim N(0,1)
\]

\[
	\sqrt{n} \frac{\bar{X}-\mu}{\sigma} \xrightarrow[n \rightarrow \infty]{d} Z \sim N(0,1), \quad \sigma=\sqrt{E\left[X^{2}\right]-\mu^{2}}
\]
\[
	\text{(Teorema Central del Límite de Levy-Lindeberg, } \mu<\infty, \sigma<\infty \text{)}
\]

\[
	\sqrt{n} \frac{b_{k}-\beta_{k}}{\sqrt{\beta_{2k}-\beta_{k}^{2}-2k \beta_{k-1} \beta_{k+1}+k^{2} \beta_{k-1}^{2} \beta_{2}}} \xrightarrow[n \rightarrow \infty]{d} Z \sim N(0,1)
\]

\[
	\sqrt{n} \frac{\sigma_n^2-\sigma^2}{\sqrt{\beta^4-\sigma^4}} \xrightarrow[n \rightarrow \infty]{d} Z \sim N(0,1)
\]

\subsection{Resultados de convergencias}

\begin{teorema}[de Slutsky]
	Si $X_{n} \xrightarrow[n \rightarrow \infty]{d} X$ y $Y_{n} \xrightarrow[n \rightarrow \infty]{P}$ a entonces
	\begin{enumerate}
		\item $Y_{n} X_{n} \xrightarrow[n \rightarrow \infty]{d} a X$
		\item $X_{n}+Y_{n} \xrightarrow[n \rightarrow \infty]{d} X+a$
		\item $\frac{X_{n}}{Y_{n}} \xrightarrow[n \rightarrow \infty]{d} \frac{X}{a}$ siempre que $a \neq 0$
	\end{enumerate}
\end{teorema}

\begin{lema}
	Si $\left\{a_{n}\right\}$ es una sucesión de constantes con $\lim _{n \rightarrow \infty} a_{n}=+\infty$ y $a$ es un número fijo tal que
	\[a_{n}\left(X_{n}-a\right) \xrightarrow[n \rightarrow \infty]{d} X\]
	entonces para cualquier función $g: \mathbb{R} \to \mathbb{R}$ con derivada
	continua y no nula en a se tiene que
	$a_{n}\left(g\left(X_{n}\right)-g(a)\right) \underset{n \rightarrow
			\infty}{\stackrel{d}{\longrightarrow}} g^{\prime}(a) X$
\end{lema}

\ejemplo{
Sea $\left(X_{1}, \cdots X_{n}\right)$ m.a.s. $(n)$ de $X \sim N(\mu, \sigma)$, se pide calcular la distribución de la media muestral:\\
Tenemos que $\varphi_{X}(t)=e^{i t \mu-\frac{1}{2} t^{2} \sigma^{2}} \implies$\\
$E\left[e^{\bar{X}}\right] = \varphi_{\bar{X}}(t)=E\left[e^{it \frac{1}{n} \sum_{i=1}^{n}
				X_{i}}\right]=\varphi_{\sum_{i=1}^{n}
		X_{i}}\left(\frac{t}{n}\right)=\left(\varphi_{X}\left(\frac{t}{n}\right)\right)^{n}=e^{i
	t \mu-\frac{1}{2} \frac{\sigma^{2}}{n} t^{2}}$.\\ Por lo tanto $\bar{X}
	\sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right)$
}
\leavevmode

\ejemplo{
	Sea \( X_1, \dots, X_n \) una muestra aleatoria simple de una variable aleatoria \( X \sim N(0, \sigma) \). La función de densidad de \( X \) es:

	\[
		f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2\sigma^2} x^2}
	\]

	Calculemos la distribución de \( a_2 = \frac{1}{n} \sum_{i=1}^{n} X_i^2 \)

	Definimos la variable estandarizada:

	\[
		Z_i = \frac{X_i}{\sigma} \sim N(0,1).
	\]

	Entonces, la suma de los cuadrados sigue una distribución chi-cuadrado:

	\[
		\sum_{i=1}^{n} Z_i^2 \sim \chi_n^2.
	\]

	La distribución chi-cuadrado con \( n \) grados de libertad es un caso particular de la distribución Gamma:

	\[
		\chi_n^2 \sim \text{Gamma} \left( a = \frac{1}{2}, p = \frac{n}{2} \right).
	\]

	Donde:
	- \( a = \frac{1}{2} \) es el **parámetro de forma**.
	- \( p = \frac{n}{2} \) es el **parámetro de escala**.

	La función de densidad de la suma de cuadrados es:

	\[
		f_{\sum_{i=1}^{n} Z_i^2}(y) = \frac{1}{2^{n/2} \Gamma(n/2)} y^{n/2-1} e^{-y/2}.
	\]

	Como \( X_i^2 = \sigma^2 Z_i^2 \), al tomar la media muestral obtenemos:

	\[
		a_2 = \frac{1}{n} \sum_{i=1}^{n} X_i^2 = \sigma^2 \frac{1}{n} \sum_{i=1}^{n} Z_i^2.
	\]

	Sustituyendo la distribución Gamma de la suma de \( Z_i^2 \), se tiene:

	\[
		a_2 \sim \text{Gamma} \left( a = \frac{n}{2\sigma^2}, p = \frac{n}{2} \right).
	\]

	Para muestras grandes, usando el **Teorema Central del Límite**, la variable estandarizada:

	\[
		\sigma^2 \sqrt{\frac{n}{2\sigma^2}} (a_2 - \sigma^2)
	\]

	converge en distribución a una normal estándar:

	\[
		N(0,1).
	\]

	Este resultado es fundamental en inferencia estadística, ya que muestra que la varianza muestral puede aproximarse por una normal para muestras grandes.

}

\begin{lema}
	Si \( Y \sim \operatorname{Gamma}(a, p) \), entonces \( T = 2a Y \sim \operatorname{Gamma}\left(\frac{1}{2}, p\right) \).
\end{lema}

\begin{proof}
	Sabemos que la función de densidad de probabilidad de una variable aleatoria \( Y \) que sigue una distribución Gamma con parámetros \( a \) y \( p \) es:

	\[
		f_{Y}(y) = \frac{a^{p}}{\Gamma(p)} e^{-a y} y^{p-1}, \quad y \geq 0.
	\]

	Ahora, definimos la variable \( T \) como \( T = 2a Y \), lo que implica que \(
	Y = \frac{1}{2a} T \). Usamos el cambio de variable para encontrar la función
	de densidad de probabilidad de \( T \). El jacobiano de este cambio es:

	\[
		J = \left| \frac{dY}{dT} \right| = \frac{1}{2a}.
	\]

	Por lo tanto, la función de densidad de probabilidad de \( T \) se obtiene
	sustituyendo en la fórmula general para el cambio de variable:

	\[
		f_{T}(t) = f_{Y}\left(\frac{1}{2a} t\right) \cdot \frac{1}{2a}.
	\]

	Sustituyendo la expresión de \( f_Y(y) \), obtenemos:

	\[
		f_{T}(t) = \frac{a^{p}}{\Gamma(p)} e^{-a \left(\frac{1}{2a} t\right)} \left( \frac{1}{2a} t \right)^{p-1} \cdot \frac{1}{2a}.
	\]

	Simplificando, tenemos:

	\[
		f_{T}(t) = \frac{\left( \frac{1}{2} \right)^{p}}{\Gamma(p)} e^{-\frac{1}{2} t} t^{p-1}, \quad t \geq 0.
	\]

	Esta es precisamente la función de densidad de una distribución Gamma con
	parámetros \( \left( \frac{1}{2}, p \right) \), lo que demuestra que \( T \sim
	\operatorname{Gamma}\left( \frac{1}{2}, p \right) \).
\end{proof}

\ejemplo{
Sea $\left(X_{1}, \dots, X_{n}\right)$ una m.a.s.(n) de $X \sim N(0, \sigma)$.
Entonces, la función de densidad es:

\[
	f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{x^2}{2\sigma^{2}}}
\]

Se pide calcular la distribución de

\[
	a_{2}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}
\]

Tipificando las variables aleatorias \( X_i \) como:

\[
	Z_{i}=\frac{X_{i}}{\sigma} \sim N(0,1),
\]

se tiene que

\[
	\sum_{i=1}^{n} Z_{i}^{2} \sim \chi_{n}^{2} \equiv \operatorname{Gamma} \left(a=\frac{1}{2}, p=\frac{n}{2} \right)
\]

La función de densidad de esta suma es:

\[
	f_{\sum_{i=1}^{n} Z_{i}^{2}}(y) = \frac{1}{2^{\frac{n}{2}} \Gamma\left(\frac{n}{2}\right)} e^{-\frac{y}{2}} y^{\frac{n}{2}-1}
\]

Dado que

\[
	a_{2} = \frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} = \frac{\sigma^{2}}{n} \sum_{i=1}^{n} Z_{i}^{2},
\]

definimos el cambio de variable

\[
	J = \frac{n}{\sigma^{2}}.
\]

Por lo tanto, la función de densidad de $a_2$ es:

\[
	f_{a_{2}}(t) = f_{\sum_{i=1}^{n} Z_{i}^{2}} \left( \frac{n}{\sigma^{2}} t \right) \frac{n}{\sigma^{2}} = \frac{\left(\frac{n}{2\sigma^{2}}\right)^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}\right)} e^{-\frac{n}{2\sigma^{2}} t} t^{\frac{n}{2}-1}.
\]

Por lo tanto,

\[
	a_{2} = \frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} \sim \operatorname{Gamma} \left(a=\frac{2\sigma^{2}}{n} , p=\frac{n}{2}\right).
\]

Finalmente, bajo el límite

\[
	\sigma^{2} \sqrt{\frac{n}{2}}\left(a_{2}-\sigma^{2}\right) \xrightarrow{d} N(0,1) \quad \text{cuando } n \to \infty.
\]
}

\begin{teorema}[de Fisher]
	Sea $\left(X_{1}, \dots, X_{n}\right)$ una m.a.s. $(n)$ de $X \sim N(\mu, \sigma)$, con $\mu$ y $\sigma$ desconocidos. Se cumple que:

	\begin{enumerate}

		\item La media muestral y la varianza muestral:

		      \[
			      \bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_{i}, \quad S^{2} = \frac{1}{n-1} \sum_{i=1}^{n} \left(X_{i} - \bar{X} \right)^{2}
		      \]

		      son variables aleatorias independientes.

		\item Sus distribuciones son:

		      \[
			      \bar{X} \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right),
		      \]

		      \[
			      \frac{(n-1) S^{2}}{\sigma^{2}} \sim \chi_{n-1}^{2}.
		      \]

		\item La siguiente variable aleatoria sigue una distribución t de Student:

		      \[
			      \frac{\bar{X} - \mu}{\frac{S}{\sqrt{n}}} \sim t_{n-1}.
		      \]

	\end{enumerate}
\end{teorema}

\begin{proof}
	\leavevmode
	\begin{enumerate}
		%Demostración del primer punto%
		\item $$ \bar{X} = \frac{1}{n}\sum_{i = 1}^{n}X_i \text{ y } S^2 = \frac{1}{n-1}\sum_{i = 1}^{n}(X_i -\bar{X})^2 \text{ son independientes} \iff X_i - \bar{X} \text{ y } \bar{X} \text{ son independientes}$$
		      $ \implies$ Veamos la independencia a través de la covarianza: $Cov(X_i -\bar{X}, \bar{X}) = 0$?\\
		      $$ Cov(X_i - \bar{X}, \bar{X}) = E[(X_i - \bar{X})\bar{X}] - E[X_i- \bar{X}]E[\bar{X}] = E[(X_i - \bar{X}\bar{X})] - (E[X_i] - E[\bar{X}])E[\bar{X}] = $$ $$ = E[(X_i - \bar{X})\bar{X}] - (\mu - \mu)\mu = E[X_i \cdot \bar{X}] - E[\bar{X}^2] = E[X_i\cdot \bar{X}] - (V[X]+ E[\bar{X}]^2) = E[X_i \cdot \bar{X}] - (\frac{\sigma^2}{n} + \mu^2) = $$ $$ = E[\frac{1}{n}X_1\cdot X_i + \ldots + \frac{1}{n}X_i^2 + \ldots + \frac{1}{n}X_n\cdot X_i] - (\frac{\sigma^2}{n} - \mu^2) = $$ $$ = \frac{1}{n}(E[X_1X_i] + \ldots + E[X_nX_i]) +\frac{1}{n}E[X_i^2] - (\frac{\sigma^2}{n} + \mu^2) = $$ $$ = \frac{n-1}{n}(\mu^2) + \frac{1}{n}(\sigma^2 + \mu^2) - (\frac{\sigma^2}{n} + \mu^2) = $$ $$ = \frac{n-1}{n}\mu^2 + \frac{\mu^2}{n} - \mu^2 + \frac{\sigma^2}{n} - \frac{\sigma^2}{n} = 0 \implies$$ $$ Cov(X_i - \bar{X}, \bar{X}) = 0 \implies \text{son independientes}$$.\\

		      %Demostracion del segundo punto%
			  \item Denotemos por  
			  $$ S_{n+1}^{2} = \frac{1}{n} \sum_{i=1}^{n+1} \left(X_{i} - \bar{X}_{n+1} \right)^{2}. $$  
			  Demostremos que:
			  $$
			  n S_{n+1}^{2} = (n-1) S_{n}^{2} + \left(X_{n+1} - \bar{X}_{n} \right)^{2} \frac{n}{n+1}.
			  $$
		  
			  Desarrollando la expresión:
		  
			  \begin{align*}
				  n S_{n+1}^{2} &= \sum_{i=1}^{n+1} \left(X_{i} - \bar{X}_{n+1} \right)^{2} \\
								&= \sum_{i=1}^{n} \left(\left(X_{i} - \bar{X}_{n} \right) + \left(\bar{X}_{n} - \bar{X}_{n+1} \right)\right)^{2} + \left(X_{n+1} - \bar{X}_{n+1} \right)^{2} \\
								&= (n-1) S_{n}^{2} + n \left(\bar{X}_{n} - \bar{X}_{n+1} \right)^{2} + \left(X_{n+1} - \bar{X}_{n+1} \right)^{2} \\
								&\quad + \sum_{i=1}^{n} 2 \left(X_{i} - \bar{X}_{n} \right) \left(\bar{X}_{n} - \bar{X}_{n+1} \right).
			  \end{align*}
		  
			  Como $\sum_{i=1}^{n} \left(X_{i} - \bar{X}_{n} \right) = 0$, el término cruzado se anula y obtenemos:
		  
			  \begin{align*}
				  n S_{n+1}^{2} &= (n-1) S_{n}^{2} + n \left(\bar{X}_{n} - \bar{X}_{n+1} \right)^{2} + \left(X_{n+1} - \bar{X}_{n+1} \right)^{2}.
			  \end{align*}
		  
			  En este último paso, se desarrollan los cuadrados y se aplica la definición de la media muestral:
		  
			  $$
			  \bar{X}_{n} = \frac{1}{n} \sum_{i=1}^{n} X_{i} \quad \Longrightarrow \quad n \bar{X}_{n} = \sum_{i=1}^{n} X_{i}.
			  $$
		  
			  Ahora, utilizando la relación:
		  
			  $$
			  \bar{X}_{n+1} = \frac{n \bar{X}_{n} + X_{n+1}}{n+1} \quad \Longrightarrow \quad X_{n+1} - \bar{X}_{n+1} = \frac{(n+1)X_{n+1} - n\bar{X}_{n} - X_{n+1}}{n+1},
			  $$
		  
			  obtenemos:
		  
			  \begin{align*}
				  n S_{n+1}^{2} &= (n-1) S_{n}^{2} + \left(X_{n+1} - \bar{X}_{n} \right)^{2} \frac{n}{(n+1)^2} + \left(X_{n+1} - \bar{X}_{n} \right)^{2} \frac{n^2}{(n+1)^2} \\
								&= (n-1) S_{n}^{2} + \left(X_{n+1} - \bar{X}_{n} \right)^{2} \frac{n}{n+1}.
			  \end{align*}
		  
			  Así, hemos obtenido el resultado deseado.
		  
			  **Distribución de la razón estandarizada:**  
			  Ahora veamos que:
			  $$
			  \left(\frac{X_{n+1} - \bar{X}_{n}}{\sigma \sqrt{\frac{n+1}{n}}} \right)^2 \sim \chi_1^2.
			  $$
		  
			  Usamos las siguientes propiedades:
		  
			  \begin{align*}
				  X_{n+1} &\sim N(\mu, \sigma), \\
				  \bar{X}_n &\sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right), \\
				  X_{n+1} - \bar{X}_n &\sim N\left(0, \sigma \sqrt{\frac{n+1}{n}} \right).
			  \end{align*}
		  
			  En este desarrollo se ha usado que en las distribuciones normales, se restan las medias y se suman las varianzas.
			  Por lo tanto, la variable estandarizada es:
		  
			  $$
			  \left(\frac{X_{n+1} - \bar{X}_n}{\sigma \sqrt{\frac{n+1}{n}}} \right) \sim N(0,1) \sim \chi_1^2.
			  $$
		  
			  **Prueba por inducción:**  
			  Consideremos el caso base, con n = 2:
		  
			  \begin{align*}
				  \frac{S_{2}^{2}}{\sigma^{2}} &= \frac{1}{\sigma^{2}} \left( (X_{1} - \bar{X}_{2})^{2} + (X_{2} - \bar{X}_{2})^{2} \right) \\
											   &= \frac{1}{\sigma^{2}} \left( \left( X_{1} - \frac{X_1 + X_2}{2} \right)^{2} + \left( X_{2} - \frac{X_1 + X_2}{2} \right)^{2} \right) \\
											   &= \frac{1}{\sigma^{2}} \left( \frac{1}{4} (X_{1} - X_{2})^{2} + \frac{1}{4} (X_{2} - X_{1})^{2} \right) \\
											   &= \frac{1}{\sigma^{2}} \frac{1}{2} (X_{2} - \bar{X_{1}})^{2}.
			  \end{align*}
		  
			  Como hemos demostrado antes,
		  
			  $$
			  \left(\frac{X_{2} - \bar{X_{1}}}{\sigma\sqrt{2}} \right)^{2} \sim \chi_{1}^{2}.
			  $$
		  
			  **Hipótesis de inducción:**  
			  Para n - 1 supongamos que
		  
			  $$
			  \frac{(n-1) S_{n}^{2}}{\sigma^{2}} \sim \chi_{n-1}^{2}.
			  $$
		  
			  Aplicando la fórmula demostrada:
		  
			  $$
			  \frac{n S_{n+1}^{2}}{\sigma^{2}} = \frac{(n-1) S_{n}^{2}}{\sigma^{2}} + \left(\frac{X_{n+1} - \bar{X}_{n}}{\sigma \sqrt{\frac{n+1}{n}}} \right)^{2}.
			  $$
		  
			  Como los dos términos a la derecha son independientes, y
		  
			  $$
			  \frac{X_{n+1} - \bar{X}_{n}}{\sigma \sqrt{\frac{n+1}{n}}} \sim \chi_1^2,
			  $$
		  
			  obtenemos la suma de dos variables chi-cuadrado:
		  
			  $$
			  \chi_{n-1}^{2} + \chi_1^2 \sim \chi_n^2.
			  $$
		  
			  **Conclusión:**  
			  Por inducción, se ha demostrado que
		  
			  $$
			  \frac{n S_{n+1}^{2}}{\sigma^{2}} \sim \chi_n^2.
			  $$
		  
		      % Demostración del tercer punto%
		\item Se deduce de los anteriores
	\end{enumerate}
\end{proof}

\subsection{Estadísticos ordenados}
\begin{definición}[Estadísticos ordenados]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ una m.a.s. $(n)$ de $X$. Podemos ordenar los valore de menor a mayor. A éstos se les llama estadísticos ordenados y se denotan por $X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(n)}$.
Sus funciones de ditribución y densidad son:
$$F_{X_{(n)}}(x)=F(x)^{n} \implies f_{X_{(n)}}(x)= \frac{\partial}{\partial{x}}(F(x)^n) = n F(x)^{n-1} f(x)$$
$$F_{X_{(1)}}(x)=1-(1-F(x))^{n} \implies f_{X_{(1)}}(x)= \frac{\partial}{\partial{x}}(1-(1-F(x))^n) = n(1-F(x))^{n-1} f(x)$$
$$F_{X_{(r)}}(x) = P\left(\sum_{i=1}^{n} I_{(-\infty, x]}(X_i) \geq r\right) = P(\operatorname{Bin}(n, F(x)) \geq r) = \sum_{j=r}^{n} \binom{n}{j} F(x)^j(1-F(x))^{n-j} \implies$$  $$\\ f_{X_{(r)}}(x) = \binom{n}{r} r F(x)^{r-1}(1-F(x))^{n-r} f(x)$$
$$f_{\left(X_{(r)}, X_{(s)}\right)}(x, y) = \frac{n!}{(r-1)!(s-r-1)!(n-s)!} F(x)^{r-1}(F(y)-F(x))^{s-r-1}(1-F(y))^{n-s} f(x) f(y)$$
$$f_{\left(X_{(1)}, \ldots, X_{(n)}\right)}(y_1, \ldots, y_n) = n! \prod_{j=1}^{n} f(y_j) \text{ si } y_1 < \ldots < y_n$$
\end{definición}

\ejemplo{
	Sea $X_1, X_2, \dots, X_n$ una muestra aleatoria simple de una distribución uniforme en $(0,1)$, es decir:

	$$ X_1, X_2, \dots, X_n \overset{i.i.d}{\sim} U(0,1) $$

	La función de distribución acumulada de una variable uniforme en $(0,1)$ es:

	$$ F(x) =
		\begin{cases}
			0, & x \leq 0  \\
			x, & 0 < x < 1 \\
			1, & x \geq 1
		\end{cases} $$

	Ordenando la muestra de menor a mayor, los estadísticos ordenados se denotan como $X_{(1)} \leq X_{(2)} \leq \dots \leq X_{(n)}$. Se quiere encontrar la función de densidad de estos valores ordenados.

	Para el máximo, $X_{(n)}$, se tiene que su función de distribución es:

	$$ P(X_{(n)} \leq x) = P(X_1 \leq x, X_2 \leq x, \dots, X_n \leq x) $$

	Dado que los datos son independientes, esto se factoriza como:

	$$ P(X_{(n)} \leq x) = F(x)^n = x^n, \quad 0 < x < 1 $$

	Derivando se obtiene la función de densidad:

	$$ f_{X_{(n)}}(x) = \frac{d}{dx} x^n = n x^{n-1}, \quad 0 < x < 1 $$

	Es decir, $X_{(n)} \sim Beta(n,1)$.

	Para el mínimo, $X_{(1)}$, la función de distribución se obtiene como:

	$$ P(X_{(1)} \leq x) = 1 - P(X_{(1)} > x) = 1 - P(X_1 > x, X_2 > x, \dots, X_n > x) $$

	Por independencia,

	$$ P(X_1 > x, X_2 > x, \dots, X_n > x) = (1 - F(x))^n = (1-x)^n $$

	Por lo que,

	$$ P(X_{(1)} \leq x) = 1 - (1-x)^n, \quad 0 < x < 1 $$

	Derivando se obtiene la densidad:

	$$ f_{X_{(1)}}(x) = n (1-x)^{n-1}, \quad 0 < x < 1 $$

	De manera general, para el $r$-ésimo estadístico ordenado, su densidad es:

	$$ f_{X_{(r)}}(x) = \frac{n!}{(r-1)! (n-r)!} x^{r-1} (1-x)^{n-r}, \quad 0 < x < 1 $$

	lo que implica que $X_{(r)} \sim Beta(r, n-r+1)$.

	La densidad conjunta del mínimo y el máximo de la muestra es:

	$$ f_{X_{(1)}, X_{(n)}}(x,y) = n(n-1)(y-x)^{n-2}, \quad 0 < x < y < 1 $$

	Esto muestra cómo la distribución de los estadísticos ordenados sigue distribuciones beta en función de la posición del orden estadístico dentro de la muestra.
	\\\\
	Con lo obtenido, calculemos la ditribución del rango muestral: $R = X_{(n)} - X_{(1)}$: \\
	Sea el cambio de variable $$	\left.\left.\begin{array}{l}
		R=X_{(n)}-X_{(1)} \\
		H=\frac{X_{(n)}+X_{(1)}}{2}
	\end{array}\right\} \begin{array}{c}
		X_{(n)}=H+\frac{R}{2} \\
		X_{(1)}=H-\frac{R}{2}
	\end{array}\right\}$$ 
	Dado que estamos intentando calcular un cambio de variable aleatoria, necesitamos calcular el jacobiano de la transformación. En este caso, el jacobiano es:
\[
J=
\begin{vmatrix}
\frac{\partial{X_{(1)}}}{\partial{H}} & \frac{\partial{X_{(1)}}}{\partial{R}} \\ 
\frac{\partial{X_{(n)}}}{\partial{H}} & \frac{\partial{X_{(n)}}}{\partial{R}} 
\end{vmatrix}
=
\begin{vmatrix}
1 & -\frac{1}{2} \\ 
1 & \frac{1}{2}
\end{vmatrix}
= 1.
\]

	Además, dado que $X_{(i)}$ son variables aleatorias uniformes en $(0,1)$, tenemos que: \\
	$$ 0 < X_{(1)} = H - \frac{R}{2} < 1 \quad 0 < X_{(n)} = H + \frac{R}{2} < 1 \implies \\ \frac{R}{2} < H < 1 - \frac{R}{2}$$
	Entonces por el Teorema de la Transformación de Variables, la densidad conjunta de $R$ y $H$ es:
	$$ g_{(R,H)}(r,h)=f_{(X_{(1)},X_{(n)})}\left(h-\frac{r}{2},h+\frac{r}{2}\right)=1-n(n-1)r^{n-2} $$
	Para obtener la densidad de $R$, se integra la densidad conjunta respecto a $H$:
	$$ f_{R}(r)=\int_{0}^{1}g_{(R,H)}(r,h)dh=\int_{0}^{1}1-n(n-1)r^{n-2}dh=h-n(n-1)r^{n-2}h\Big|_{0}^{1}=1-n(n-1)r^{n-2} $$
	Entonces, la densidad de $R$ es:
	$$ f_{R}(r)=1-n(n-1)r^{n-2} \sim Beta(n-1,2) $$
}


\vspace{1em} % Adds vertical space of 1em

\ejemplo{
	Sea $\left(X_{1}, \dots, X_{n}\right)$ una muestra aleatoria simple (m.a.s.) de tamaño $n$ de una variable aleatoria $X$, cuya función de distribución es  
	\[
	F(x) = P(X \leq x).
	\]
	
	Se desea calcular la distribución de las variables  
	\[
	U = F(X) \quad \text{y} \quad U_{R} = F\left(X_{(r)}\right).
	\]
	
	Para la variable $U$, tenemos:  
	\[
	G_{U}(u) = P(U \leq u) = P(F(X) \leq u) = P\left(X \leq F^{-1}(u)\right).
	\]
	Utilizando la propiedad de la función de distribución inversa, se obtiene:  
	\[
	F\left(F^{-1}(u)\right) = u, \quad 0 < u < 1.
	\]
	Por lo tanto,  
	\[
	U = F(X) \sim U(0,1).
	\]

	Ahora, para la distribución de $U_R$:  
	\[
	G_{U_{R}}(u) = P\left(U_{R} \leq u\right) = P\left(F\left(X_{(r)}\right) \leq u\right) = P\left(X_{(r)} \leq F^{-1}(u)\right).
	\]
	Esto es equivalente a la función de distribución del estadístico ordenado:  
	\[
	F_{X_{(r)}}\left(F^{-1}(u)\right) = \sum_{j=r}^{n} \binom{n}{j} F\left(F^{-1}(u)\right)^{j} \left(1 - F\left(F^{-1}(u)\right)\right)^{n-j}.
	\]
	Reescribiendo en términos de $u$:  
	\[
	G_{U_{R}}(u) = \sum_{j=r}^{n} \binom{n}{j} u^{j} (1-u)^{n-j}.
	\]
	
	Por lo tanto, el estadístico ordenado de orden $r$ asociado a la muestra aleatoria sigue la distribución  
	\[
	U_{R} = F\left(X_{(r)}\right),
	\]
	donde la población original $U = F(X)$ sigue una distribución uniforme en el intervalo $(0,1)$.
}


\subsection{Ejercicios}
\begin{problem}{2.1}
	Sea $X$ una población de $Bernouilli(p = \frac{1}{2})$ y se consideran todas las m.a.s. posibles de tamaño 3. Para cada muestra calcúlese $\bar{X}, s^2, \mu y S^2$ y determínese sus distribuciones en el muestreo. 
\end{problem}
\begin{sol}
	\begin{center}
		\begin{tabular}{c c c c}
			\toprule
			\textbf{Muestras} & $\bar{X}$ & $s^2$ & P \\
			\midrule
			(0,0,0)   & 0     & 0     & 1/8 \\
			(0,0,1)   & 1/3   & 1/3   & 1/8 \\
			(0,1,0)   & 1/3   & 1/3   & 1/8 \\
			(0,1,1)   & 2/3   & 1/3   & 1/8 \\
			(1,0,0)   & 1/3   & 1/3   & 1/8 \\
			(1,0,1)   & 2/3   & 1/3   & 1/8 \\
			(1,1,0)   & 2/3   & 1/3   & 1/8 \\
			(1,1,1)   & 1     & 0     & 1/8 \\
			\bottomrule
		\end{tabular}
	\end{center}
	
	\vspace{0.1cm} % Espacio entre la tabla grande y el texto
	
	\textbf{Distribución de} $\bar{X}$ \textbf{y} $s^2$:
	
	\vspace{0cm} % Espacio para separar más el texto de las tablas pequeñas
	
	\begin{center}
		\begin{tabular}{c|cccc} 
			$\bar{x}$ & 0 & $1/3$ & $2/3$ & 1 \\
			\hline
			$P$ & $1/8$ & $3/8$ & $3/8$ & $1/8$ \\
		\end{tabular}
		\hspace{1cm} % Espacio entre tablas pequeñas
		\begin{tabular}{c|cc} 
			$s^2$ & 0 & $1/3$  \\
			\hline
			$P$ & $1/4$ & $3/4$ \\
		\end{tabular}
	\end{center}
\end{sol}
\begin{problem}{2.2}
	De una población con media $\mu$ desconocida y varianza 1, se toma una m.a.s. de tamaño n. ¿Cuál debe ser éste para que la media muestral diste en valor absoluto de la media de la población menos que 0,5, con una probabilidad mayor o igual que 0,95?
\end{problem}
\begin{sol}
	\\El enunciado nos pide es averiguar la n suficiente para que se cumpla que $P(|\bar{X}-\mu|<0.5)\geq 0.95 \iff P( -0.5 < \bar{X} - \mu < 0.5) \geq 0.95$.\\
	Sabemos que $\bar{X} \sim N(\mu, \frac{1}{n})$. Por lo que 
	para solucionarlo, haremos uso de la Desigualdad de Chebushev, la cual afirma que: $$ P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2} \quad \forall k > 0$$
	Aplicando la desigualdad a nuestro caso, obtenemos que $k = 0.5$ y $\sigma = \frac{1}{\sqrt{n}}$. Por lo que:
	$$P(|\bar{X} - \mu| \leq 0'5) \geq 0'95 \iff P(|\bar{X} - \mu| > 0'5) \leq 0'05 \implies \begin{cases} \frac{1}{k^2} = 0'05 \\ \frac{k}{\sqrt{n}} = 0,5 \end{cases} \implies n \geq 80$$
\end{sol}
\begin{problem}{2.3}
	Dada una m.a.s. de tamaño n, calcúlese la distribución de la media muestral $\bar{X}$ cuando la población es: 
	\begin{enumerate}
		\item Bernouilli
		\item Gamma
		\item Exponencial
	\end{enumerate}	
\end{problem}
\begin{sol}
	\begin{enumerate}
		\item \textbf{Bernouilli:} Si $X \sim Bernouilli(p) \implies \sum_{i = 1}^{n}X_i = S_n \sim Bin(n, p) \implies \bar{X} = \frac{S_n}{n} \implies P(\bar{X} = w) = P(S_n = nw) = \binom{n}{nw}p^{nw}(1-p)^{n(1-w)}$
		\item \textbf{Gamma:} Si $X \sim Gamma(a, b) \implies \varphi_{X}(t) = (1 - \frac{it}{a})^{-b} \implies \varphi_{S_n}(t) = E[e^{i \cdot \frac{t}{n}\sum_{i = 1}^{n}X_i}] = E[e^{i\frac{t}{n}X_1}] \ldots E[e^{i\frac{t}{n}X_n}] = \left[\varphi(\frac{t}{n})\right]^n = (1- \frac{it}{an})^{-nb} \implies \bar{X} \sim Gamma(na,nb)$
		\item \textbf{Exponencial:} Es un caso particular del apartado anterior dado que $X \sim Exponencial(\theta) = Gamma(\theta, 1) \implies \bar{x} \sim Gamma(n\theta, n)$.
	\end{enumerate}
\end{sol}
\begin{problem}{2.4}
	Dada una sucesion $\{X_n\}$ de variables aleatorias independientes con distribución $N(0,1)$ y un entero positivo $k$ se define: 
	$$F_{k,m} = \left(\frac{1}{k}\sum_{i = 1}^k X_i^2 \right) / \left(\frac{1}{m}\sum_{i = k+1}^{k+m}X_i^2\right)$$
	Pruébese que
	\[
	F_{k,m} \xrightarrow[m \to \infty]{\mathcal{L}} \frac{1}{k} X
	\]
	donde \( X \sim \chi^2_k \). 
\end{problem}
\begin{sol}
	Por las propiedades de las distribuciones normales, tenemos que: $$X_i \sim N(0,1) \implies X_i^2 \sim \chi^2_1 \implies \sum_{i = 1}^{n} X_i^2 \sim \chi_n^2$$
	De esta manera podemos ver los sumandos como el calculo de la media muestral: $$\bar{X} = \frac{1}{m} \sum_{i = k+1}^{k+m} \chi^2_1 \implies $$ Por la Ley Fuerte de los Grandes Números tenemos que $$\bar{X} \xrightarrow[m \to \infty]{\mathcal{L}} E[\chi^2_1] = 1$$ 
	Finalmente como el segundo sumatorio $\sum_{i = 1}^{k} X_i^2$ se mantiene constante(pues sólo se está modificando $k$) tenemos que, por el Teorema de Slutsky, $$F_{k,m} \xrightarrow[m \to \infty]{\mathcal{L}} \frac{1}{k} \chi_k^2$$
\end{sol}
\begin{problem}{2.5}
	Demuéstrese que para una m.a.s. de tamaño $n$ tales que $$P(X = x_i) = p_i \forall i \in \N$$ se cumple que la distribución del estadístico ordenado $X_{(k)}$ es discreta y viene dada por: 
	$$P(X_{(k)} \leq x_i) = \sum_{j = k}^{n} \binom{n}{j} (F(x_j))^j(1-F(x_j))^{n-j}$$ 
	\\
	$$P(X_{(k)} = x_i) = \sum_{j = k}^{n} \binom{n}{j}[(F(x_j))^j(1 - F(x_i))^{n-j} - F(x_{i-1})^j(1 - F(x_{i-1}))^{n-j}]$$	
\end{problem}
\begin{sol}
	Sea una m.a.s. $\left(X_1, X_2, \ldots, X_n\right)$. \\Sea entonces el conjunto de variables aleatorias $$I_j = \begin{cases} 1, \quad \text{si } X_j \leq x \\ 0, \quad \text{si } X_j > x \end{cases} \implies I_j \sim Bernouilli$$ con una probabilidad $P(I_j = 1) = P(X_j \leq x) = F_j(x)$\\ Al ser variables aleatorias independientes con distirbución Bernouilli, se cumple que $$\sum_{j = 1}^{n} I_j = S \sim Bin(n, F(x))$$\\
	Nos piden calcular que $P(X_{(k)} \leq x_i)$, lo cual es equivalente a que nos pregunten la probabilidad de que haya al menos $k$ valores menores o iguales a $x_i$: $P(S \geq k)$. Por lo que podemos escribir: 
	$$P(S \geq k) = P(X_{(k)} \leq x_i) = \sum_{j = k}^{n} \binom{n}{j}(F(X_i))^j(1 - F(X_i))^{n-j}$$
	Finalmente, nos queda sólo calcular la probabilidad de que $P(X_{(k)} = x_i)$ pero esto es igual a hacer: 
	$$P(X_{(k)} = x_i) = P(X_{(k)} \leq x_i) - P(X_{(k)} \leq x_{i-1})$$
\end{sol}