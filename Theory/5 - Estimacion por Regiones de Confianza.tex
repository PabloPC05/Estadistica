\section{Estimación por Regiones de Confianza}

\ejemplo{
    Tomemos como ejemplo de entrada a este tema un caso en el que un lanzador de jabalina quiere estimar la distancia real promedio, es decir, su media, $\mu$. Para ello realizamos varias mediciones (muestras) y medimos las distancias. Estos datos se supone que son una m.a.s. de tamaño $n$ de una distribución normal $X \sim N(\theta, \sigma^2)$ con $\sigma$ conocida. Parece normal suponer que cada $x_i$ distará de $\mu$ una cantidad aaleatoria con distribución $Normal(\theta, \sigma^2) - \theta \equiv N(0, \sigma^2)$. \\
    Queremos estimar la media (poblacional) por lo que debemos saber que la media muestral se rige por la distribución $N(\theta, \frac{\sigma^2}{n})$. De manera que si queremos saber en qué intervalo está la media poblacional $\mu$ con una probbilidad del $95\%$. \\
    Antes de ello, simplifiquemos un poco el trabajo y tomemos $Z = \frac{\bar{X} - \theta}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1)$. Ahora tengamos en cuenta que lo que queremos es calcular un intervalo de manera que: 
    $$P\{-x \leq Z \leq x\} = 0.95 \iff x \equiv 1.96 (\text{mirando la tabla de la normal})$$ $$\implies P\{-1.96 \leq Z \leq 1.96\} = 0.95 \iff P\{-1.96 \leq \frac{\bar{X} - \theta}{\frac{\sigma}{\sqrt{n}}} \leq 1.96\} = 0.95 \iff $$
    $$ \iff P\{\bar{X} - \frac{\sigma}{\sqrt{n}} \cdot 1.96 < \theta < \bar{X} + \frac{\sigma}{\sqrt{n}\cdot 1.96}\} = 0.95 \implies$$
    Cualquier valor de $\theta$ que esté dentro del intervalo aleatorio $(\bar{X} - \frac{\sigma}{\sqrt{n}} \cdot 1.96, \bar{X} + \frac{\sigma}{\sqrt{n}\cdot 1.96})$ tiene una probabilidad del $95\%$ de contener el valor real de la media poblacional $\mu$. \\ \\
    Veamos su aplicación a un caso real: \\
    Pongamos que el lanzados lanza la jabalina $6$ veces $\implies n = 6$ y que salen los siguientes valores: 
    $$89.90 \quad 90.02 \quad 89.92 \quad 91.10 \quad 88.71 \quad 90.51 \implies \begin{cases}
    \bar{X} = 90.03 \\
    \sigma = 0.8
    \end{cases}$$
    Todos estos datos nos dan lugar a que el intervalo de confianza sea: 
    $$(\bar{X} - \frac{\sigma}{\sqrt{n}} \cdot 1.96, \bar{X} + \frac{\sigma}{\sqrt{n}\cdot 1.96}) = \left(90.03 - \frac{0.8}{\sqrt{6}}\cdot 1.96, 90.03 + \frac{0.8}{\sqrt{6}\cdot 1.96}\right) = (89.39, 90.67)$$
    Hay que tener en cuenta, que una vez calculado el intervalo, éste ya es fijo y no cambia, de manera que el parámetro $\theta$ también lo es y sólo hay dos posibilidades, que esté dentro o que esté fuera y es cómo una moneda ideal, la probabilidad es del $50\%$. La probabilidad del $95\%$ es que si tomamos varias muestras y sus intervalos correspondientes, el $95\%$ de ellos contendrán el valor real del parámetro $\theta$.
}

\subsection{Intervalos de confianza}
\begin{definición}[Región de confianza]
    Sea $C(X_{1}, \ldots, X_{n}) \subset \Theta$ una región aleatoria del espacio paramétrico tal que: 
    $$P_{\theta}\left\{\theta \in C(X_{1}, \ldots, X_{n})\right\} \geq 1-\alpha, \forall \theta \in \Theta$$
    Entones para cada $x_{1}, \ldots, x_{n} \in \chi^{n}$, $C(x_{1}, \ldots, x_{n})$ se denomina región de confianza para $\theta$ de nivel $1-\alpha$
\end{definición}

\ejemplo{
    Sea por ejemplo $X \sim N(\theta, \sigma^2)$ con $\sigma_0$ conocida y $\left(\bar{X} - \frac{\sigma_0}{\sqrt{n}}\mathcal{z_{\frac{\alpha}{2}}}, \bar{X} + \frac{\sigma_0}{\sqrt{n}}\mathcal{z}_{\frac{\alpha}{2}}\right)$ es un intervalo de grado de confianza $1-\alpha$ para media $\theta$ y con $z_{\frac{\alpha}{2}}$ tal que $\Phi(z_{\frac{\alpha}{2}}) = 1 - \frac{\alpha}{2}$, donde $\Phi$ es la función de distribución de la normal estándar $N(0,1)$
}

\begin{definición}[Intervalos de confianza]
Sea $h: \Theta \rightarrow \mathbb{R}, \alpha \in(0,1)$ y $T_{1}=T_{1}\left(X_{1}, \ldots, X_{n}\right): \chi^{n} \rightarrow \mathbb{R}$ y $T_{2}=T_{2}\left(X_{1}, \ldots, X_{n}\right): \chi^{n} \rightarrow \mathbb{R}, T_{1} \leq T_{2}$, dos estadísticos unidimensionales tales que
$$P_{\theta}\left\{T_{1}\left(X_{1}, \ldots, X_{n}\right) \leq h(\theta) \leq T_{2}\left(X_{1}, \ldots, X_{n}\right)\right\} \geq 1-\alpha, \forall \theta \in \Theta$$
Entonces, para cada $\left(x_{1}, \ldots, x_{n}\right) \in \chi^{n},\left(T_{1}\left(x_{1}, \ldots, x_{n}\right), T_{2}\left(x_{1}, \ldots, x_{n}\right)\right)$ se denomina intervalo de confianza para $h(\theta)$ de nivel $1-\alpha$
\end{definición}

\begin{observación}[Explicación de la definición]
    \vspace{-\topsep}
    \vspace{-\topsep}
    \begin{itemize}
        \item $h(\theta)$ es una función del parámetro $\theta$ que se desea estimar, por ejemplo la media poblacional $\mu$
        \item $T_{1}$ y $T_{2}$ son dos estadísticos dependientes de la muestra
        \item El intervalo $(T_{1}(\vec{x}), T_{2}(\vec{x}))$ cambia cada vez que tomas una muestra diferente
        \item La probabilidad que ese intervalo contenga el valor real de $h(\theta)$ debe ser al menos $1-\alpha$
    \end{itemize}
    Se construye un intervalo usando una muestra tomada, de modo que sin importar el valor real del parámetro $\theta$, hay algmenos una probabilidad de $1-\alpha$ de probabilidad de que el alor real esté dentro del intervalo.
\end{observación}

\begin{observación}
Siempre es deseable hacer que la medida de la región de confianza sea mínima (en términos de medida-longitud), entre todas las del mismo grado de confianza $1-\alpha$
\end{observación}

\ejemplo{
    Veamos un ejemplo explicativo de la observación anterior: \\
    En el primer ejemplo de este tema, vemos cómo calcular un intervalo que contiene el $95\%$ del área bajo la curva de la normal, no obstante el intervalo anterior se obtuvo de forma arbitraria, pero haciendo uso de la simetría de la normal. Si quisieramos, por ejemplo, podríamos tomar que un intervalo que deja fuera a la izquierda un $1\%$ y a la derecha un $4\%$, de manera que dan un intervalo de aproximadamente $(-2.33, 1.75)$. \\
    No obsante podemos comparar la longitud de los intervalos para entender la observación: 
    $$\begin{cases}
    \text{Intervalo 1:} & (-1.96, 1.96) \implies \text{Longitud} = 3.92 \\
    \text{Intervalo 2:} & (-2.33, 1.75) \implies \text{Longitud} = 4.08
    \end{cases}$$
}

\subsection{Métodos de obtención de intervalos de confianza}
\subsubsection{Método de la cantidad pivotal}
\begin{definición}[Cantidad pivotal]
    Una cantidad pivotal es una función de la muestra y del parámetro desconocido, que tiene una distribución conocida independiente del valor del parámwtro. Es decir, 
    $$Q(X_1, \ldots, X_n; \theta)$$ 
    Donde $Q$ es a cantidada pivotal, $X_1, \ldots, X_n$ son los datos de la muestra y $\theta$ es el parámetro desconocido que queremos estimar.
\end{definición}

\begin{teorema}[Método de la cantidad pivotal]
    Si $T = T(X_1, \ldots, X_n; \theta)$ es una cantidad pivotal monótonamente creciente $\forall \vec{x} \in \chi^n$, fijado cualquier nivel de confianza 1 - $\alpha$, $\alpha \in (0,1)$ se pueden determinar dos contantes $c_1(\alpha)$ y $c_2(\alpha) \in \mathbb{R}$ (que no son únicas), tales que:
    $$P_\theta\{c_1(\alpha) \leq T(X_1, \ldots, X_n; \theta) \leq c_2(\alpha)\} \geq 1 - \alpha, \forall \theta \in \Theta$$
    Si para cada $\left(x_1, \ldots, x_n\right) \in \chi^n$, $c_1(\alpha) \leq T(x_1, \ldots, x_n; \theta) \leq c_2(\alpha)$ $\Leftrightarrow T_1(x_1, \ldots, x_n; \alpha) \leq h(\theta) \leq T_2(x_1, \ldots, x_n; \alpha)$, entonces $\left(T_1(x_1, \ldots, x_n; \alpha), T_2(x_1, \ldots, x_n; \alpha)\right)$ es un intervalo de confianza para $h(\theta)$ de nivel $1 - \alpha$
\end{teorema}

\ejemplo{
    Para una m.a.s.(n) de $X \sim N(\mu, \sigma), \sigma$ conocida y  $\mu$ desconocida
    $$I C_{1-\alpha}(\mu)=\left(\bar{x}-z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}, \bar{x}+z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right)$$
    es un intervalo de confianza para $\mu$ de nivel $1-\alpha, \alpha \in(0,1)$\\
    Para demostrar esto, utilizaremos el teorema de fisher, segun el cual $\overline{X} \sim N(\mu, \frac{\sigma}{\sqrt{n}})$, entonces:
    $$ \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1)$$
    $$ P(z_{\frac{\alpha}{2}} \leq \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \leq z_{\frac{\alpha}{2}}) \geq 1-\alpha \implies P(\bar{X}-z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}+z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}) \geq 1-\alpha$$
    Y asi finalmente se deduce el intervalo dado.
}

\ejemplo{
    Para una m.a.s.(n) de $X \sim N(\mu, \sigma), \sigma$ desconocida y $\mu$ desconocida.
    En este caso tenemos que el estadístico $\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}$ no puede usarse ya que tiene dos parámetros desconocidos, por lo que usaremos los estimadores insesgados de los parámetros $\bar{X}$ y $S^2$: 
    $$ \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1) \quad \quad \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1} \implies t_{n-1} = \frac{N(0,1)}{\sqrt{\chi^2_{n-1}}} \iff $$
    $$\iff \frac{\frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}}}{\sqrt{\chi^2_{n-1}}}{\sqrt{\frac{\frac{(n-1)S^2}{\sigma^2}}{n-1}}} = \frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}$$
    En este nuevo caso, el estadístico sólo tiene un parámetro, pero se distribuye con una distribución t de Student que es independiente del mismo. Los intervalos de confianza ahora se calculan de la forma: 
    $$P\left\{-t_{\frac{\alpha}{2}} \leq \frac{\bar{X} - \mu}{\frac{S^2}{\sqrt{n}}} \leq t_{\frac{\alpha}{2}}\right\} = 1 - \alpha \iff$$
    $$\iff P\left\{-t_{\frac{\alpha}{2}} \cdot \frac{S}{\sqrt{n}} \leq \bar{X} - \mu \leq t_{\frac{\alpha}{2}} \cdot \frac{S}{\sqrt{n}}\right\} \iff$$
    $$ \iff P\left\{\bar{X} - t_{\frac{\alpha}{2}} \cdot \frac{S}{\sqrt{n}} \leq \mu \leq \bar{X} + t_{\frac{\alpha}{2}} \cdot \frac{S}{\sqrt{n}}\right\} = 1 - \alpha$$
    \\
    Dado que este es un caso especial en el que ambos parámetros son desconocidos, el intervalo de confianza anterior es el de $\mu$ pero podríamos calcular también el de $\sigma^2$, para ello y al contrario que antes, podemos usar el estimador $T \equiv \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$ ya que sólo depende de un parámetro y es independiente del mismo, cosa que no ocurría en el caso de $\mu$. Por lo que el intervalo de confianza para $\sigma^2$ sería:
    $$P\left\{\chi^2_{\frac{\alpha}{2}} \leq \frac{(n-1)S^2}{\sigma^2} \leq \chi^2_{1 - \frac{\alpha}{2}}\right\} = 1 - \alpha \iff$$
    $$\iff P\left\{\frac{1}{\chi^2_{\frac{\alpha}{2}}} \leq \frac{\sigma^2}{(n-1)S^2} \leq \frac{1}{\chi^2_{1 - \frac{\alpha}{2}}}\right\}\iff$$
    $$\iff P\left\{\frac{(n-1)S^2}{\chi^2_{1 - \frac{\alpha}{2}}} \leq \sigma^2 \leq \frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2}}}\right\} = 1 - \alpha$$

    \begin{observación}
        Dada la independencia del estimador $T \equiv \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$ de otros parámetros, el caso en el que $\mu$ es conocida y $\sigma^2$ no lo es, se puede resolver de la misma manera.
    \end{observación}
} 

\ejemplo{
    Sean dos variables aleatorias: $\begin{cases}
    X \sim N(\mu_x, \sigma_x^2) \\
    Y \sim N(\mu_y, \sigma_y^2)
    \end{cases} \implies \begin{cases}
    \bar{X} \sim N(\mu_x, \frac{\sigma_x^2}{n}) \\
    \bar{Y} \sim N(\mu_y, \frac{\sigma_y^2}{m})
    \end{cases}$
    Calculemos el intervalo de confianza para $\mu_x - \mu_y$: 
    NO ENTIENDO CÓMO TERMINARLO
}
\ejemplo{
    Pongámonos en el caso anterior, con dos variables aleatorias $X$ e $Y$ independientes con $\sigma_x^2$ y $\sigma_y^2$ desconocidas y queremos calcular el intervalo de confianza para $\frac{\sigma_x^2}{\sigma_y^2}$.
    Para ello tenemos que:
    $$\frac{\sigma_y^2}{\sigma_x^2} \cdot \frac{S_{n-1}^2}{S_{m-1}^2} = \frac{\frac{(n-1)S_{n-1}^2}{\frac{\sigma_x^2}{n-1}}}{\frac{(m-1)S_{m-1}^2}{\frac{\sigma_y^2}{m-1}}} \sim F_{n-1, m-1} \implies$$
    $$P\{f_{n-1, m-1;\frac{\alpha}{2}} \leq \frac{\sigma_y^2}{\sigma_x^2} \cdot \frac{S_{n-1}^2}{S_{m-1}^2} \leq f_{n-1, m-1; 1 - \frac{\alpha}{2}}\} = 1- \alpha \implies$$
    $$IC_{1 - \alpha}(\frac{\sigma_x^2}{\sigma_y^2}) = \left(\frac{S_{n-1}^2}{S_{m-1}^2} \cdot f_{n-1, m-1;\frac{\alpha}{2}}, \frac{S_{n-1}^2}{S_{m-1}^2} \cdot f_{n-1, m-1; 1 - \frac{\alpha}{2}}\right)$$

}

\ejemplo{
    Para una m.a.s. ( $n$ ) de de $X$, si $\theta \in \mathbb{R}$ y la función de distribución de la población $F_{\theta}(x)$, como función en $x$ es continua y estrictamente monótona $\forall \theta$, y como función de $\theta$ es continua y estrictamente monótona $\forall x$, entonces $T=-2 \sum_{i=1}^{n} \ln F_{\theta}\left(X_{i}\right) \sim \chi_{2 n}^{2}$ constituye una cantidad pivotal y permite obtener un intervalo de confianza para $\theta$\\
    Para ver este primero fijemonos en que como $F_\theta (x)$ es continua y estrictamente monótona, entonces:
    $Y = F(X|\theta) \sim U(0,1)$. Ahora hagamos el cambio:
    $$z=-2\ln(F(X | \theta)) \implies y=e^{-z/2}, \quad \left| \frac{dy}{dz} \right| = \frac{1}{2} e^{-z/2} \implies g(z) = \frac{1}{2} e^{-z/2}$$
    Que coincide con la densidad de una variable aleatoria $\chi^2$ con 2 grados de libertad.\\
    Asi, sumandolas:
    $$ T = -2 \sum_{i=1}^{n} \ln F\left(X|\theta\right) \sim \chi_{2 n}^{2}$$
    Ahora podemos utilizar los cuantiles para obtener el intervalo de confianza:
    $$ P(\chi_{2 n; 1- \frac{\alpha}{2}} \leq T \leq \chi_{2 n;\frac{\alpha}{2}}) \geq 1-\alpha$$
    Por tanto utilizando la monotonia estricta en $\theta$ de $F_\theta(x)$, y la invertibilidad de las ecuaciones, podemos obtener:
    $$\chi_{2 n; 1- \frac{\alpha}{2}} = T(x_1, \ldots ,x_n; \underline{\theta}(x_1, \ldots ,x_n))$$
    $$\chi_{2 n;\frac{\alpha}{2}} = T(x_1, \ldots ,x_n; \overline{\theta}(x_1, \ldots ,x_n))$$
    Finalmente obtenemos el intervalo de confianza:
    $$\left(\underline{\theta}(x_1, \ldots ,x_n), \overline{\theta}(x_1, \ldots ,x_n)\right)$$
}

\ejemplo{
    Construir un intervalo de confianza para $\theta$ por el método de la
    cantidad pivotal basado en una m.a.s. $(n)$ de $f_{\theta}(x)=\theta x^{\theta-1} I_{(0,1)}(x)$, con $\theta>0$\\
    Utilizando el ejemplo anterior, y como:
    $$F_{\theta}(x)=
    \begin{cases}
        0 & x \leq 0 \\
        x^{\theta} & 0<x<1 \\
        1 & x \geq 1
    \end{cases}
    $$
    Es una función continua y estrictamente monótona, entonces:
    $$P\left(\chi^2_{2n;1-\frac{\alpha}{2}} < -2\theta \sum_{i=1}^{n} \ln X_i < \chi^2_{2n;\frac{\alpha}{2}} \middle| \theta \right) = 1 - \alpha \quad \forall \theta \in \Theta$$
    Y por tanto:
    $$
        P\left\{
        \frac{\chi^2_{2n;1-\frac{\alpha}{2}}}{-2 \sum_{i=1}^{n} \ln X_i}
        < \theta <
        \frac{\chi^2_{2n;\frac{\alpha}{2}}}{-2 \sum_{i=1}^{n} \ln X_i}
        \middle|
        \theta
        \right\}
        = 1 - \alpha \quad \forall \theta \in \Theta
        $$
}

\subsubsection{Método de Neyman}
\begin{teorema}[Método de Neyman]
    Sea una m.a.s. de tamaño $n$. Si para cualquier $\alpha \in [0,1]$ se puedne encontrar dos funciones $\gamma_1(\theta, \alpha)$ y $\gamma_2(\theta, \alpha)$ y un estadístico $T(\vec{X})$ tales que: 
    $$P\{\gamma_1(\theta, \alpha) < T(\vec{X}) < \gamma_2(\theta, \alpha) | \theta\} \geq 1 - \alpha$$
    y si además las fuciones $\gamma_1$ y $\gamma_2$ son estrictamente monótonas en $\theta$ del mismo sentido y las ecuaciones: 
    $$\begin{cases}
        \gamma_1(\theta, \alpha) = T(\vec{X}) \\
        \gamma_2(\theta, \alpha) = T(\vec{X})
    \end{cases}$$
    se pueden invertir para resolverlas en $\theta$ en función de $T(\vec{x}) = t$ entonces se puede construir un intervalo para $\theta$ de grado de confianza $1 - \alpha$, de la forma: 
    $$\left(\gamma_1^{-1}(t, \alpha), \gamma_2^{-1}(t, \alpha)\right)$$
\end{teorema}

\ejemplo{
    Construir por el método de Neyman un intervalo de confianza de longitud esperada mínima para $\theta$ basado en una m.a.s. $(n)$ de $X \sim U(0, \theta)$, con $\theta>0$. Indicación: utilizar $T=T\left(X_{1}, \ldots, X_{n}\right)=X_{(n)}$\\
    Para ello utilizaremos la función de distribución del estadistico suficiente $T = X_{(n)}$:
    $$F(t | \theta) = \begin{cases}
        0 & t \leq 0 \\
        \left(\frac{t}{\theta}\right)^{n} & 0<t<\theta \\
        1 & t \geq \theta
        \end{cases}
    $$
    El metodo de Neyman nos dice que tenemos que dos funciones $\gamma_1(\theta \text{;} \alpha)$ y $\gamma_2(\theta \text{;} \alpha)$ tales que:
    $$P(\gamma_1(\theta \text{;} \alpha) < T < \gamma_2(\theta \text{;} \alpha) | \theta) \geq 1 - \alpha, \quad \alpha \in [0,1]$$
    Asi definimos las siguientes constantes: 
        $$F(\gamma_1(\theta \text{;} \alpha) | \theta) = \alpha_1 \quad \quad 1 - F(\gamma_2(\theta \text{;} \alpha) | \theta) = \alpha - \alpha_1$$
    Donde $\alpha_1 \in [0,\alpha]$
    Despejando obtenemos que ambas funciones deben ser:
    $$\gamma_1(\theta \text{;} \alpha) = \theta \left(\alpha_1\right)^{\frac{1}{n}} \quad \quad \gamma_2(\theta \text{;} \alpha) = \theta \left(1 - (\alpha - \alpha_1)\right)^{\frac{1}{n}}$$
    Finalmente, volviendo a la probabilidad y teniendo ya nuestras funciones y nuestro estadistico:
    $$P\left(\theta \left(\alpha_1\right)^{\frac{1}{n}} < X_{(n)} < \theta \left(1 - (\alpha - \alpha_1)\right)^{\frac{1}{n}} | \theta\right) \geq 1 - \alpha \quad \implies$$
    $$P\left(\frac{X_{(n)}}{\left(1 - (\alpha - \alpha_1)\right)^{\frac{1}{n}}} < \theta < \frac{X_{(n)}}{\left(\alpha_1\right)^{\frac{1}{n}}} | \theta\right) \geq 1 - \alpha$$
    Obteniendo asi el intervalo de confianza:
    $$\left(\frac{X_{(n)}}{\left(1 - (\alpha - \alpha_1)\right)^{\frac{1}{n}}}, \frac{X_{(n)}}{\left(\alpha_1\right)^{\frac{1}{n}}}\right)$$
    El cual claramente se minimiza en $\alpha_1 = \alpha$, obteniendo finalmente el intervalo de confianza:
    $$IC_{1-\alpha}(\theta) = \left(X_{(n)}, X_{(n)} \alpha^{-\frac{1}{n}}\right)$$
}

\begin{definición}[Intervalos de confianza para muestras grandes]
        Si $T_{n}=T\left(X_{1}, \ldots, X_{n}\right)$ es un estimador de $h(\theta)$ tal que

    $$ \frac{T_{n}-h(\theta)}{\sigma_{n}(\theta)} \underset{n \rightarrow \infty}{d} N(0,1) \quad \quad
            P_{\theta}\left(-z_{\alpha / 2} \leq \frac{T_{n}-h(\theta)}{\sigma_{n}(\theta)} \leq z_{\alpha / 2}\right) \underset{n \rightarrow \infty}{\longrightarrow} 1-\alpha
    $$
    Por lo tanto, si puede invertirse la desigualdad anterior, despejando $h(\theta)$, se puede obtener un intervalo de confianza para $h(\theta)$, de nivel aproximado $1-\alpha$, cuando el tamaño muestral es suficientemente grande 
\end{definición}



\begin{observación}
Si se cumplen todas las condiciones de regularidad y la ecuación de verosimilitud tiene una única raíz, $\hat{\theta}_{n} \xrightarrow[n \rightarrow \infty]{\text { c.s. }} \theta$, puede tomarse $T_{n}=\hat{\theta}_{n}$ y $h(\theta)=\theta$, y como

$$ \frac{\hat{\theta}_{n}-\theta}{\sqrt{\frac{1}{n l_{1}(\theta)}}} \xrightarrow[n \rightarrow \infty]{\stackrel{d}{\longrightarrow}} N(0,1) $$

entonces $\sigma_{n}(\theta)=\sqrt{\frac{1}{n I_{1}(\theta)}}$, que si es una
función continua puede ser aproximada por
$\sigma_{n}\left(\hat{\theta}_{n}\right)$, lo que facilita la inversión

$$ I C_{1-\alpha}(\theta)=\hat{\theta}_{n} \mp z_{\alpha / 2} \sqrt{\frac{1}{n I_{1}\left(\hat{\theta}_{n}\right)}} $$
\end{observación}

\ejemplo{
    Comprobar que si $X \sim \operatorname{Bin}(1, \theta)$, entonces
    $$\begin{gathered}
        I C_{1-\alpha}(\theta)=\bar{x} \mp z_{\alpha / 2} \sqrt{\frac{\bar{x}(1-\bar{x})}{n}} \\
        I C_{1-\alpha}(\theta)=\bar{x} \mp z_{\alpha / 2} \frac{1}{2 \sqrt{n}}
    \end{gathered}$$
    son intervalos de confianza para $\theta$ basados en el estimador de mínima varianza para muestras grandes.
    
}

\begin{teorema}[Desigualdad de Tchebychev]
    $$P\left(|Y-E[Y]|>k \sqrt{V(Y)}\right) \leq \frac{1}{k^{2}}$$
\end{teorema}

\ejemplo{
    Comprobar que si $X \sim \operatorname{Bin}(1, \theta)$, entonces
    $$\begin{gathered}
        I C_{1-\alpha}(\theta)=\bar{x} \mp \frac{1}{\sqrt{\alpha}} \sqrt{\frac{\bar{x}(1-\bar{x})}{n}} \\
        I C_{1-\alpha}(\theta)=\bar{x} \mp \frac{1}{\sqrt{\alpha}} \frac{1}{2 \sqrt{n}}
    \end{gathered}$$
    son intervalos de confianza para $\theta$ basados en la desigualdad de Tchebychev
}

\begin{observación}
    Los intervalos que se obtienen mediante el método de la desigualdad de Tchebychev son más amplios que los construídos mediante procedimientos específicos a cada modelo de probabilidad
\end{observación}

\subsection{Regiones de confianza bayesianas}

\begin{definición}[Región creible]
    Dada una familia de distribuciones de probabilidad $\left\{f\left(x_{1}, \ldots, x_{n} \mid \theta\right), \theta \in \Theta\right\}$, si la información inicial sobre $\theta$ viene dada por la función de densidad o de masa $\pi(\theta)$, la región $C\left(x_{1}, \ldots, x_{n}\right) \subset \Theta$ es una región creíble de probabilidad $1-\alpha$ si
    $$ P\left(\theta \in C\left(x_{1}, \ldots, x_{n}\right) \mid x_{1}, \ldots, x_{n}\right) \geq 1-\alpha $$
    donde esta probabilidad se calcula mediante la distribución final, es decir
    $$ \int_{C\left(x_{1}, \ldots, x_{n}\right)} \pi\left(\theta \mid x_{1}, \ldots, x_{n}\right) d \theta \geq 1-\alpha $$
\end{definición}



\ejemplo{
Para muestras de tamaño $n=1$ de $X \sim \operatorname{Bin}(1, \theta)$, si $\theta \sim U(0,1)$ y se observa $x=1$, entonces $C_{1-\alpha}(\theta)=\left(\sqrt{\frac{\alpha}{2}}, \sqrt{1-\frac{\alpha}{2}}\right)$

En efecto, si $\left(X_{1}, \ldots, X_{n}\right)$ es una m.a.s. $(n)$ de $X \sim \operatorname{Bin}(1, \theta)$ y $\theta \sim U(0,1)$, entonces\\
$\pi\left(\theta \mid x_{1}, \ldots, x_{n}\right) \sim \operatorname{Beta}\left(\sum_{i=1}^{n} x_{i}+1, n-\sum_{i=1}^{n} x_{i}+1\right)$\\
Por lo tanto, para muestras de tamaño $n=1$\\
$\pi(\theta \mid x=0) \sim \operatorname{Beta}(1,2), \int_{\ell_{1}}^{\ell_{2}} 2(1-\theta) d \theta=1-\alpha$\\
$\pi(\theta \mid x=1) \sim \operatorname{Beta}(2,1), \int_{\ell_{1}}^{\ell_{2}} 2 \theta d \theta=1-\alpha$\\
Por ejemplo, si $x=1, \ell_{1}^{2}=\frac{\alpha}{2}, \ell_{2}^{2}=1-\frac{\alpha}{2}$\\
$C_{1-\alpha}(\theta)=(0.223,0.974)$, para $\alpha=0.1$\\
Otra posibilidad es la región creíble de más alta probabilidad final o amplitud mínima, $C_{1-\alpha}(\theta)=(\sqrt{\alpha}, 1)=(0.316,1)$\\
}

\ejemplo{
Para muestras de tamaño $n=10$ de $X \sim \operatorname{Bin}(1, \theta)$, si $\theta \sim U(0,1)$ y se observa $\sum_{i=1}^{n} x_{i}=3$, entonces la distribución final es $\operatorname{Beta}(4,8)$ y $C_{1-\alpha}(\theta)=(0.135,0.564)$, para $\alpha=0.1$

En este caso la región creíble de más alta probabilidad final o amplitud mínima es $C_{1-\alpha}(\theta)=(0.117,0.542)$, para $\alpha=0.1$
}

\ejemplo{
Para una m.a.s. ( $n$ ) de $X \sim N(\theta, 1)$, si $\theta \sim N(0,1)$ y se observa $\bar{x}$, entonces $C_{1-\alpha}(\theta)=\frac{n}{n+1} \bar{x} \mp z_{\alpha / 2} \frac{1}{\sqrt{n+1}}$

En efecto, Si $\left(X_{1}, \ldots, X_{n}\right)$ es una m.a.s. $(n)$ de $X \sim N(\mu, \sigma)$ con $\sigma$ conocida y $\mu \sim N\left(\mu_{0}, \sigma_{0}\right)$, entonces $\pi\left(\mu \mid x_{1}, \ldots, x_{n}\right) \sim N\left(\mu_{1}, \sigma_{1}\right)$,

$$
\begin{gathered}
\mu_{1}=\frac{\frac{\mu_{0}}{\sigma_{0}^{2}}+\frac{\bar{x}}{\frac{\sigma^{2}}{n}}}{\frac{1}{\sigma_{0}^{2}}+\frac{\frac{1}{\sigma^{2}}}{\frac{\sigma^{2}}{n}}}=\frac{\sigma^{2}}{\sigma^{2}+n \sigma_{0}^{2}} \mu_{0}+\frac{n \sigma_{0}^{2}}{\sigma^{2}+n \sigma_{0}^{2}} \bar{x}=\frac{n}{n+1} \bar{x} \\
\sigma_{1}=\sqrt{\frac{1}{\frac{1}{\sigma_{0}^{2}}+\frac{1}{\frac{\sigma^{2}}{n}}}}=\frac{1}{\sqrt{n+1}}
\end{gathered}
$$
}

\begin{observación}
Recordemos que el intervalo de confianza obtenido desde el punto de vista frecuentista es $IC_{1-\alpha}(\theta)=\bar{x} \mp z_{\alpha / 2} \frac{1}{\sqrt{n}}$, que tiene mayor amplitud, aparte de su diferente interpretación
\end{observación}


