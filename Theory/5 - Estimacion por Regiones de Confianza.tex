\section{Estimación por Regiones de Confianza}

\ejemplo{
    Tomemos como ejemplo de entrada a este tema un caso en el que un lanzador de jabalina quiere estimar la distancia real promedio, es decir, su media, $\mu$. Para ello realizamos varias mediciones (muestras) y medimos las distancias. Estos datos se supone que son una m.a.s. de tamaño $n$ de una distribución normal $X \sim N(\theta, \sigma^2)$ con $\sigma$ conocida. Parece normal suponer que cada $x_i$ distará de $\mu$ una cantidad aaleatoria con distribución $Normal(\theta, \sigma^2) - \theta \equiv N(0, \sigma^2)$. \\
    Queremos estimar la media (poblacional) por lo que debemos saber que la media muestral se rige por la distribución $N(\theta, \frac{\sigma^2}{n})$. De manera que si queremos saber en qué intervalo está la media poblacional $\mu$ con una probbilidad del $95\%$. \\
    Antes de ello, simplifiquemos un poco el trabajo y tomemos $Z = \frac{\bar{X} - \theta}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1)$. Ahora tengamos en cuenta que lo que queremos es calcular un intervalo de manera que: 
    $$P\{-x \leq Z \leq x\} = 0.95 \iff x \equiv 1.96 (\text{mirando la tabla de la normal})$$ $$\implies P\{-1.96 \leq Z \leq 1.96\} = 0.95 \iff P\{-1.96 \leq \frac{\bar{X} - \theta}{\frac{\sigma}{\sqrt{n}}} \leq 1.96\} = 0.95 \iff $$
    $$ \iff P\{\bar{X} - \frac{\sigma}{\sqrt{n}} \cdot 1.96 < \theta < \bar{X} + \frac{\sigma}{\sqrt{n}\cdot 1.96}\} = 0.95 \implies$$
    Cualquier valor de $\theta$ que esté dentro del intervalo aleatorio $(\bar{X} - \frac{\sigma}{\sqrt{n}} \cdot 1.96, \bar{X} + \frac{\sigma}{\sqrt{n}\cdot 1.96})$ tiene una probabilidad del $95\%$ de contener el valor real de la media poblacional $\mu$. \\ \\
    Veamos su aplicación a un caso real: \\
    Pongamos que el lanzados lanza la jabalina $6$ veces $\implies n = 6$ y que salen los siguientes valores: 
    $$89.90 \quad 90.02 \quad 89.92 \quad 91.10 \quad 88.71 \quad 90.51 \implies \begin{cases}
    \bar{X} = 90.03 \\
    \sigma = 0.8
    \end{cases}$$
    Todos estos datos nos dan lugar a que el intervalo de confianza sea: 
    $$(\bar{X} - \frac{\sigma}{\sqrt{n}} \cdot 1.96, \bar{X} + \frac{\sigma}{\sqrt{n}\cdot 1.96}) = \left(90.03 - \frac{0.8}{\sqrt{6}}\cdot 1.96, 90.03 + \frac{0.8}{\sqrt{6}\cdot 1.96}\right) = (89.39, 90.67)$$
    Hay que tener en cuenta, que una vez calculado el intervalo, éste ya es fijo y no cambia, de manera que el parámetro $\theta$ también lo es y sólo hay dos posibilidades, que esté dentro o que esté fuera y es cómo una moneda ideal, la probabilidad es del $50\%$. La probabilidad del $95\%$ es que si tomamos varias muestras y sus intervalos correspondientes, el $95\%$ de ellos contendrán el valor real del parámetro $\theta$.
}

\subsection{Intervalos de confianza}
\begin{definición}[Región de confianza]
    Sea $C(X_{1}, \ldots, X_{n}) \subset \Theta$ una región aleatoria del espacio paramétrico tal que: 
    $$P_{\theta}\left\{\theta \in C(X_{1}, \ldots, X_{n})\right\} \geq 1-\alpha, \forall \theta \in \Theta$$
    Entones para cada $x_{1}, \ldots, x_{n} \in \chi^{n}$, $C(x_{1}, \ldots, x_{n})$ se denomina región de confianza para $\theta$ de nivel $1-\alpha$
\end{definición}

\ejemplo{

}

\begin{definición}[Intervalos de confianza]
Sea $h: \Theta \rightarrow \mathbb{R}, \alpha \in(0,1)$ y $T_{1}=T_{1}\left(X_{1}, \ldots, X_{n}\right): \chi^{n} \rightarrow \mathbb{R}$ y $T_{2}=T_{2}\left(X_{1}, \ldots, X_{n}\right): \chi^{n} \rightarrow \mathbb{R}, T_{1} \leq T_{2}$, dos estadísticos unidimensionales tales que
$$P_{\theta}\left\{T_{1}\left(X_{1}, \ldots, X_{n}\right) \leq h(\theta) \leq T_{2}\left(X_{1}, \ldots, X_{n}\right)\right\} \geq 1-\alpha, \forall \theta \in \Theta$$
Entonces, para cada $\left(x_{1}, \ldots, x_{n}\right) \in \chi^{n},\left(T_{1}\left(x_{1}, \ldots, x_{n}\right), T_{2}\left(x_{1}, \ldots, x_{n}\right)\right)$ se denomina intervalo de confianza para $h(\theta)$ de nivel $1-\alpha$
\end{definición}

\begin{observación}[Explicación de la definición]
    \vspace{-\topsep}
    \vspace{-\topsep}
    \begin{itemize}
        \item $h(\theta)$ es una función del parámetro $\theta$ que se desea estimar, por ejemplo la media poblacional $\mu$
        \item $T_{1}$ y $T_{2}$ son dos estadísticos dependientes de la muestra
        \item El intervalo $(T_{1}(\vec{x}), T_{2}(\vec{x}))$ cambia cada vez que tomas una muestra diferente
        \item La probabilidad que ese intervalo contenga el valor real de $h(\theta)$ debe ser al menos $1-\alpha$
    \end{itemize}
    Se construye un intervalo usando una muestra tomada, de modo que sin importar el valor real del parámetro $\theta$, hay algmenos una probabilidad de $1-\alpha$ de probabilidad de que el alor real esté dentro del intervalo.
\end{observación}

\begin{observación}
Siempre es deseable hacer que la medida de la región de confianza sea mínima, entre todas las del mismo grado de confianza $1-\alpha$
\end{observación}

\subsection{Métodos de obtención de intervalos de confianza}
\underline{\textbf{Método de la cantidad pivotal}}\\
\begin{definición}[Cantidad pivotal]
    Una v.a. $T=T\left(X_{1}, \ldots, X_{n}, \theta\right): \chi^{n} \times \Theta
    \rightarrow \mathbb{R}$ es una cantidad pivotal sí y sólo sí su distribución en
    el muestreo no depende de $\theta$
\end{definición}


Si $T=T\left(X_{1}, \ldots, X_{n}, \theta\right)$ es una cantidad pivotal,
fijado cualquier nivel de confianza $1-\alpha, \alpha \in(0,1)$, se pueden
determinar dos constantes, $c_{1}(\alpha)$ y $c_{2}(\alpha) \in \mathbb{R}$
(que no son únicas), tales que

$$
    P_{\theta}\left\{c_{1}(\alpha) \leq T\left(X_{1}, \ldots, X_{n}, \theta\right) \leq c_{2}(\alpha)\right\} \geq 1-\alpha, \forall \theta \in \Theta
$$

Si para cada $\left(x_{1}, \ldots, x_{n}\right) \in \chi^{n}, c_{1}(\alpha)
    \leq T\left(x_{1}, \ldots, x_{n}, \theta\right) \leq c_{2}(\alpha)$
$\Leftrightarrow T_{1}\left(x_{1}, \ldots, x_{n}, \alpha\right) \leq h(\theta)
    \leq T_{2}\left(x_{1}, \ldots, x_{n}, \alpha\right)$, entonces
$\left(T_{1}\left(x_{1}, \ldots, x_{n}, \alpha\right), T_{2}\left(x_{1},
    \ldots, x_{n}, \alpha\right)\right)$ es un intervalo de confianza para
$h(\theta)$ de nivel $1-\alpha$

Intervalos de confianza asociados a la distribución normal\\

\ejemplo{
    Para una m.a.s.(n) de $X \sim N(\mu, \sigma), \sigma$ conocida,
    $$I C_{1-\alpha}(\mu)=\left(\bar{x}-z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}, \bar{x}+z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right)$$
    es un intervalo de confianza para $\mu$ de nivel $1-\alpha, \alpha \in(0,1)$\\
    Para demostrar esto, utilizaremos el teorema de fisher, segun el cual $\overline{X} \sim N(\mu, \frac{\sigma}{\sqrt{n}})$, entonces:
    $$ \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1)$$
    $$ P(z_{\frac{\alpha}{2}} \leq \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \leq z_{\frac{\alpha}{2}}) \geq 1-\alpha \implies P(\bar{X}-z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}+z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}) \geq 1-\alpha$$
    Y asi finalmente se deduce el intervalo dado.
}

\ejemplo{
    Para una m.a.s. ( $n$ ) de de $X$, si $\theta \in \mathbb{R}$ y la función de distribución de la población $F_{\theta}(x)$, como función en $x$ es continua y estrictamente monótona $\forall \theta$, y como función de $\theta$ es continua y estrictamente monótona $\forall x$, entonces $T=-2 \sum_{i=1}^{n} \ln F_{\theta}\left(X_{i}\right) \sim \chi_{2 n}^{2}$ constituye una cantidad pivotal y permite obtener un intervalo de confianza para $\theta$\\
    Para ver este primero fijemonos en que como $F_\theta (x)$ es continua y estrictamente monótona, entonces:
    $Y = F(X|\theta) \sim U(0,1)$. Ahora hagamos el cambio:
    $$z=-2\ln(F(X | \theta)) \implies y=e^{-z/2}, \quad \left| \frac{dy}{dz} \right| = \frac{1}{2} e^{-z/2} \implies g(z) = \frac{1}{2} e^{-z/2}$$
    Que coincide con la densidad de una variable aleatoria $\chi^2$ con 2 grados de libertad.\\
    Asi, sumandolas:
    $$ T = -2 \sum_{i=1}^{n} \ln F\left(X|\theta\right) \sim \chi_{2 n}^{2}$$
    Ahora podemos utilizar los cuantiles para obtener el intervalo de confianza:
    $$ P(\chi_{2 n; 1- \frac{\alpha}{2}} \leq T \leq \chi_{2 n;\frac{\alpha}{2}}) \geq 1-\alpha$$
    Por tanto utilizando la monotonia estricta en $\theta$ de $F_\theta(x)$, y la invertibilidad de las ecuaciones, podemos obtener:
    $$\chi_{2 n; 1- \frac{\alpha}{2}} = T(x_1, \ldots ,x_n; \underline{\theta}(x_1, \ldots ,x_n))$$
    $$\chi_{2 n;\frac{\alpha}{2}} = T(x_1, \ldots ,x_n; \overline{\theta}(x_1, \ldots ,x_n))$$
    Finalmente obtenemos el intervalo de confianza:
    $$\left(\underline{\theta}(x_1, \ldots ,x_n), \overline{\theta}(x_1, \ldots ,x_n)\right)$$
}

\ejemplo{
    Construir un intervalo de confianza para $\theta$ por el método de la
    cantidad pivotal basado en una m.a.s. $(n)$ de $f_{\theta}(x)=\theta x^{\theta-1} I_{(0,1)}(x)$, con $\theta>0$\\
    Utilizando el ejemplo anterior, y como:
    $$F_{\theta}(x)=
    \begin{cases}
        0 & x \leq 0 \\
        x^{\theta} & 0<x<1 \\
        1 & x \geq 1
    \end{cases}
    $$
    Es una función continua y estrictamente monótona, entonces:
    $$P\left(\chi^2_{2n;1-\frac{\alpha}{2}} < -2\theta \sum_{i=1}^{n} \ln X_i < \chi^2_{2n;\frac{\alpha}{2}} \middle| \theta \right) = 1 - \alpha \quad \forall \theta \in \Theta$$
    Y por tanto:
    $$
        P\left\{
        \frac{\chi^2_{2n;1-\frac{\alpha}{2}}}{-2 \sum_{i=1}^{n} \ln X_i}
        < \theta <
        \frac{\chi^2_{2n;\frac{\alpha}{2}}}{-2 \sum_{i=1}^{n} \ln X_i}
        \middle|
        \theta
        \right\}
        = 1 - \alpha \quad \forall \theta \in \Theta
        $$
}

\underline{\textbf{Método de Neyman}}\\
Sea $T=T\left(X_{1}, \ldots X_{n}\right): \chi^{n} \rightarrow \mathbb{R}$ un
estimador de $h(\theta)$ y denotemos por $g_{\theta}(t)$ a su función de
densidad (o de masa)

Fijado un nivel de confianza $1-\alpha, \alpha \in(0,1)$ y $\alpha_{1},
    \alpha_{2} \in(0,1)$ tales que $\alpha=\alpha_{1}+\alpha_{2}$, para cada
$\theta \in \Theta$ se pueden determinar dos valores $c_{1}\left(\theta,
    \alpha_{1}\right)$ y $c_{2}\left(\theta, \alpha_{2}\right) \in \mathbb{R}$,
tales que $P_{\theta}\left\{T<c_{1}\left(\theta, \alpha_{1}\right)\right\} \leq
    \alpha_{1}$ y $P_{\theta}\left\{T>c_{2}\left(\theta, \alpha_{2}\right)\right\}
    \leq \alpha_{2}$. Entonces, para cada $\theta \in \Theta$

$$
    P_{\theta}\left\{c_{1}\left(\theta, \alpha_{1}\right) \leq T \leq c_{2}(\theta, \alpha)\right\} \geq 1-\alpha_{2}-\alpha_{1}=1-\alpha
$$

Si para cada $t \in \mathbb{R}, c_{1}\left(\theta, \alpha_{1}\right) \leq T(t)
    \leq c_{2}(\theta, \alpha) \Leftrightarrow$\\ $T_{1}\left(t, \alpha_{1},
    \alpha_{2}\right) \leq h(\theta) \leq T_{2}\left(t, \alpha_{1},
    \alpha_{2}\right)$, entonces\\ $\left(T_{1}\left(t, \alpha_{1},
    \alpha_{2}\right), T_{2}\left(t, \alpha_{1}, \alpha_{2}\right)\right)$ es un
intervalo de confianza para $h(\theta)$ de nivel $1-\alpha, \alpha \in(0,1)$

\ejemplo{
    Construir por el método de Neyman un intervalo de confianza de longitud esperada mínima para $\theta$ basado en una m.a.s. $(n)$ de $X \sim U(0, \theta)$, con $\theta>0$. Indicación: utilizar $T=T\left(X_{1}, \ldots, X_{n}\right)=X_{(n)}$\\
    Para ello utilizaremos la función de distribución del estadistico suficiente $T = X_{(n)}$:
    $$F(t | \theta) = \begin{cases}
        0 & t \leq 0 \\
        \left(\frac{t}{\theta}\right)^{n} & 0<t<\theta \\
        1 & t \geq \theta
        \end{cases}
    $$
    El metodo de Neyman nos dice que tenemos que dos funciones $\gamma_1(\theta \text{;} \alpha)$ y $\gamma_2(\theta \text{;} \alpha)$ tales que:
    $$P(\gamma_1(\theta \text{;} \alpha) < T < \gamma_2(\theta \text{;} \alpha) | \theta) \geq 1 - \alpha, \quad \alpha \in [0,1]$$
    Asi deducimos que:
    $$F(\gamma_1(\theta \text{;} \alpha) | \theta) = \alpha_1$$
    $$F(\gamma_2(\theta \text{;} \alpha) | \theta) = \alpha - \alpha_1$$
    Donde $\alpha_1 \in [0,\alpha]$
    Despejando obtenemos que ambas funciones deben ser:
    $$\gamma_1(\theta \text{;} \alpha) = \theta \left(\alpha_1\right)^{\frac{1}{n}}$$
    $$\gamma_2(\theta \text{;} \alpha) = \theta \left(1 - (\alpha - \alpha_1)\right)^{\frac{1}{n}}$$
    Finalmente, volviendo a la probabilidad y teniendo ya nuestras funciones y nuestro estadistico:
    $$P\left(\theta \left(\alpha_1\right)^{\frac{1}{n}} < X_{(n)} < \theta \left(1 - (\alpha - \alpha_1)\right)^{\frac{1}{n}} | \theta\right) \geq 1 - \alpha \quad \implies$$
    $$P\left(\frac{X_{(n)}}{\left(1 - (\alpha - \alpha_1)\right)^{\frac{1}{n}}} < \theta < \frac{X_{(n)}}{\left(\alpha_1\right)^{\frac{1}{n}}} | \theta\right) \geq 1 - \alpha$$
    Obteniendo asi el intervalo de confianza:
    $$\left(\frac{X_{(n)}}{\left(1 - (\alpha - \alpha_1)\right)^{\frac{1}{n}}}, \frac{X_{(n)}}{\left(\alpha_1\right)^{\frac{1}{n}}}\right)$$
    El cual claramente se minimiza en $\alpha_1 = \alpha$, obteniendo finalmente el intervalo de confianza:
    $$IC_{1-\alpha}(\theta) = \left(X_{(n)}, X_{(n)} \alpha^{-\frac{1}{n}}\right)$$
}

Intervalos de confianza para muestras grandes\\ Si $T_{n}=T\left(X_{1}, \ldots,
    X_{n}\right)$ es un estimador de $h(\theta)$ tal que

$$
    \begin{gathered}
        \frac{T_{n}-h(\theta)}{\sigma_{n}(\theta)} \underset{n \rightarrow \infty}{d} N(0,1) \\
        P_{\theta}\left(-z_{\alpha / 2} \leq \frac{T_{n}-h(\theta)}{\sigma_{n}(\theta)} \leq z_{\alpha / 2}\right) \underset{n \rightarrow \infty}{\longrightarrow} 1-\alpha
    \end{gathered}
$$

Por lo tanto, si puede invertirse la desigualdad anterior, despejando
$h(\theta)$, se puede obtener un intervalo de confianza para $h(\theta)$, de
nivel aproximado $1-\alpha$, cuando el tamaño muestral es suficientemente
grande

\begin{observación}
Si se cumplen todas las condiciones de regularidad y la ecuación de verosimilitud tiene una única raíz, $\hat{\theta}_{n} \xrightarrow[n \rightarrow \infty]{\text { c.s. }} \theta$, puede tomarse $T_{n}=\hat{\theta}_{n}$ y $h(\theta)=\theta$, y como

$$
    \frac{\hat{\theta}_{n}-\theta}{\sqrt{\frac{1}{n l_{1}(\theta)}}} \xrightarrow[n \rightarrow \infty]{\stackrel{d}{\longrightarrow}} N(0,1)
$$

entonces $\sigma_{n}(\theta)=\sqrt{\frac{1}{n I_{1}(\theta)}}$, que si es una
función continua puede ser aproximada por
$\sigma_{n}\left(\hat{\theta}_{n}\right)$, lo que facilita la inversión

$$
    I C_{1-\alpha}(\theta)=\hat{\theta}_{n} \mp z_{\alpha / 2} \sqrt{\frac{1}{n I_{1}\left(\hat{\theta}_{n}\right)}}
$$
\end{observación}

\ejemplo{
Comprobar que si $X \sim \operatorname{Bin}(1, \theta)$, entonces

$$
    \begin{gathered}
        I C_{1-\alpha}(\theta)=\bar{x} \mp z_{\alpha / 2} \sqrt{\frac{\bar{x}(1-\bar{x})}{n}} \\
        I C_{1-\alpha}(\theta)=\bar{x} \mp z_{\alpha / 2} \frac{1}{2 \sqrt{n}}
    \end{gathered}
$$

son intervalos de confianza para $\theta$ basados en el EMV, para muestras
grandes

Desigualdad de Tchebychev

$$
    P(|Y-E[Y]|>k \sqrt{V(Y)}) \leq \frac{1}{k^{2}}
$$

Teniendo en cuenta que $X \sim Bin(1, \theta)$, entonces $E[X]=\theta$ y $V(X)=\theta(1-\theta)$, y por tanto:
$$ 
    P(|\bar{X}-\theta|>k \sqrt{\frac{\bar{x}(1-\bar{x})}{n}}) \leq \frac{1}{k^{2}} \quad \implies
$$
$$ 
    P\left(\bar{X}-k \sqrt{\frac{\bar{x}(1-\bar{x})}{n}}<\theta<\bar{X}+k \sqrt{\frac{\bar{x}(1-\bar{x})}{n}}\right) \geq 1-\frac{1}{k^{2}} \quad \forall k>0
$$
Ahora fijamos $k=\sqrt{\frac{1}{\alpha}}$, y obtenemos el intervalo de confianza:
$$
    I C_{1-\alpha}(\theta)=\bar{x} \mp \frac{1}{\sqrt{\alpha}} \sqrt{\frac{\bar{x}(1-\bar{x})}{n}} \quad \forall k>0
$$

    Si $\bar{x}=\frac{1}{2}$, lo cual ocurre cuando $n$ es muy grande, entonces:
    $$I C_{1-\alpha}(\theta)=\bar{x} \mp \frac{1}{\sqrt{\alpha}} \frac{1}{2 \sqrt{n}}$$
}

\ejemplo{
Comprobar que si $X \sim \operatorname{Bin}(1, \theta)$, entonces

$$
    \begin{gathered}
        I C_{1-\alpha}(\theta)=\bar{x} \mp \frac{1}{\sqrt{\alpha}} \sqrt{\frac{\bar{x}(1-\bar{x})}{n}} \\
        I C_{1-\alpha}(\theta)=\bar{x} \mp \frac{1}{\sqrt{\alpha}} \frac{1}{2 \sqrt{n}}
    \end{gathered}
$$

son intervalos de confianza para $\theta$ basados en la desigualdad de
Tchebychev    
}

\begin{observación}
Observación Los intervalos que se obtienen mediante el método de la desigualdad de Tchebychev son más amplios que los construídos mediante procedimientos específicos a cada modelo de probabilidad
\end{observación}

\underline{\textbf{Región creíble}}\\
Dada una familia de distribuciones de probabilidad $\left\{f\left(x_{1}, \ldots, x_{n} \mid \theta\right), \theta \in \Theta\right\}$, si la información inicial sobre $\theta$ viene dada por la función de densidad o de masa $\pi(\theta)$, la región $C\left(x_{1}, \ldots, x_{n}\right) \subset \Theta$ es una región creíble de probabilidad $1-\alpha$ si

$$
P\left(\theta \in C\left(x_{1}, \ldots, x_{n}\right) \mid x_{1}, \ldots, x_{n}\right) \geq 1-\alpha
$$

donde esta probabilidad se calcula mediante la distribución final, es decir

$$
\int_{C\left(x_{1}, \ldots, x_{n}\right)} \pi\left(\theta \mid x_{1}, \ldots, x_{n}\right) d \theta \geq 1-\alpha
$$

\ejemplo{
Para muestras de tamaño $n=1$ de $X \sim \operatorname{Bin}(1, \theta)$, si $\theta \sim U(0,1)$ y se observa $x=1$, entonces $C_{1-\alpha}(\theta)=\left(\sqrt{\frac{\alpha}{2}}, \sqrt{1-\frac{\alpha}{2}}\right)$

En efecto, si $\left(X_{1}, \ldots, X_{n}\right)$ es una m.a.s. $(n)$ de $X \sim \operatorname{Bin}(1, \theta)$ y $\theta \sim U(0,1)$, entonces\\
$\pi\left(\theta \mid x_{1}, \ldots, x_{n}\right) \sim \operatorname{Beta}\left(\sum_{i=1}^{n} x_{i}+1, n-\sum_{i=1}^{n} x_{i}+1\right)$\\
Por lo tanto, para muestras de tamaño $n=1$\\
$\pi(\theta \mid x=0) \sim \operatorname{Beta}(1,2), \int_{\ell_{1}}^{\ell_{2}} 2(1-\theta) d \theta=1-\alpha$\\
$\pi(\theta \mid x=1) \sim \operatorname{Beta}(2,1), \int_{\ell_{1}}^{\ell_{2}} 2 \theta d \theta=1-\alpha$\\
Por ejemplo, si $x=1, \ell_{1}^{2}=\frac{\alpha}{2}, \ell_{2}^{2}=1-\frac{\alpha}{2}$\\
$C_{1-\alpha}(\theta)=(0.223,0.974)$, para $\alpha=0.1$\\
Otra posibilidad es la región creíble de más alta probabilidad final o amplitud mínima, $C_{1-\alpha}(\theta)=(\sqrt{\alpha}, 1)=(0.316,1)$\\
}

\ejemplo{
Para muestras de tamaño $n=10$ de $X \sim \operatorname{Bin}(1, \theta)$, si $\theta \sim U(0,1)$ y se observa $\sum_{i=1}^{n} x_{i}=3$, entonces la distribución final es $\operatorname{Beta}(4,8)$ y $C_{1-\alpha}(\theta)=(0.135,0.564)$, para $\alpha=0.1$

En este caso la región creíble de más alta probabilidad final o amplitud mínima es $C_{1-\alpha}(\theta)=(0.117,0.542)$, para $\alpha=0.1$
}

\ejemplo{
Para una m.a.s. ( $n$ ) de $X \sim N(\theta, 1)$, si $\theta \sim N(0,1)$ y se observa $\bar{x}$, entonces $C_{1-\alpha}(\theta)=\frac{n}{n+1} \bar{x} \mp z_{\alpha / 2} \frac{1}{\sqrt{n+1}}$

En efecto, Si $\left(X_{1}, \ldots, X_{n}\right)$ es una m.a.s. $(n)$ de $X \sim N(\mu, \sigma)$ con $\sigma$ conocida y $\mu \sim N\left(\mu_{0}, \sigma_{0}\right)$, entonces $\pi\left(\mu \mid x_{1}, \ldots, x_{n}\right) \sim N\left(\mu_{1}, \sigma_{1}\right)$,

$$
\begin{gathered}
\mu_{1}=\frac{\frac{\mu_{0}}{\sigma_{0}^{2}}+\frac{\bar{x}}{\frac{\sigma^{2}}{n}}}{\frac{1}{\sigma_{0}^{2}}+\frac{\frac{1}{\sigma^{2}}}{\frac{\sigma^{2}}{n}}}=\frac{\sigma^{2}}{\sigma^{2}+n \sigma_{0}^{2}} \mu_{0}+\frac{n \sigma_{0}^{2}}{\sigma^{2}+n \sigma_{0}^{2}} \bar{x}=\frac{n}{n+1} \bar{x} \\
\sigma_{1}=\sqrt{\frac{1}{\frac{1}{\sigma_{0}^{2}}+\frac{1}{\frac{\sigma^{2}}{n}}}}=\frac{1}{\sqrt{n+1}}
\end{gathered}
$$
}

\begin{observación}
Recordemos que el intervalo de confianza obtenido desde el punto de vista frecuentista es $IC_{1-\alpha}(\theta)=\bar{x} \mp z_{\alpha / 2} \frac{1}{\sqrt{n}}$, que tiene mayor amplitud, aparte de su diferente interpretación
\end{observación}


