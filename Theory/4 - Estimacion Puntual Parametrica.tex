\section{Estimación Puntual Paramétrica}

\begin{definición}[Estimador]
Sea $(\Omega, \mathcal{A}, \mathcal{P})$ el espacio probabilístico asociado a un experimento aleatorio $\mathcal{E}$, una variable aleatoria observable $X: \Omega \longrightarrow \mathbb{R}$ y su modelo estadístico asociado $\left(\chi, \mathcal{B}, F_{\theta}\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}}, \mathcal{B}=\mathcal{B}(\mathbb{R})$, y $\left(X_{1}, \cdots X_{n}\right)$ m.a.s. $(n) \sim X$
\\Un estimador del parámetro $\theta$ es un estadístico $T=T\left(X_{1}, \cdots X_{n}\right): \chi^{n} \longrightarrow \Theta$ que se utiliza para determinar el valor desconocido $\theta$
\end{definición}

\begin{definición}[Estimador centrado o insesgado]
Dado un estimador $T = T(X_1, \ldots X_n): \chi^{n} \to \Theta$ se dice que es centrado para $h(\theta)$ cuando $E_{\theta}[T] = h(\theta)$. \\
Se dice asintóticamente insesgado o centrado cuando $\lim_{n \to \infty}E_{\theta}[T] = h(\theta)$
\end{definición}

\ejemplo{
  Para cualquier población que tenga media, la media muestral \(\overline{X}\) es un estimador centrado de \(h(\theta) = E[X]\), puesto que se cumple:
  \[
    E_\theta \left[ \overline{X} \right] = E[X]
  \]

  Además, se tiene que \(E[a_k] = \alpha_k\), lo que implica que el momento
  muestral de orden \(k\) respecto al origen es un estimador centrado del momento
  poblacional de orden \(k\) respecto al origen de la población. En general, no
  es cierto un resultado análogo para momentos centrales. }

\begin{definición}[Sesgo]
Se llama sesgo de un estimador a la diferencia $b(T, \theta) = E_{\theta}[T] - \theta$
\end{definición}

\ejemplo{
Veamos que el estadístico $T = (\bar{X}, S^2)$ es un estimador centrado de $\theta = (\mu, \sigma^2):$\\
$$E[\bar{X}] = E\left[\frac{1}{n}\sum_{i  = 1}^{n} X_i \right] = \frac{1}{n} \sum_{i = 1}^{n}E[X_i] = \frac{1}{n} \sum_{i = 1}^{n} \mu = \mu$$
$$E[S^2] = E\left[\frac{1}{n-1}\sum_{i  = 1}^{n} (X_i - \bar{X})^2 \right] = \frac{1}{n-1} \sum_{i = 1}^{n}E[(X_i - \bar{X})^2] = \frac{1}{n-1} \sum_{i = 1}^{n} \sigma^2 = \sigma^2$$
}

\ejemplo{
Demuestra que el estadístico $\sigma_n^2 = b_2 = \frac{1}{n} \sum_{i = 1}^{n}(X_i - \bar{X})^2$ es un estimador centrado de $h(\theta) = \frac{n-1}{n}\sigma^2$ y $b(\sigma_n^2, \sigma^2) = -\frac{\sigma^2}{n}$
$$E[\sigma_n^2] = E\left[\frac{1}{n} \sum_{i = 1}^{n}(X_i - \bar{X})^2\right] = \frac{1}{n} \sum_{i = 1}^{n}E[(X_i - \bar{X})^2] = $$ $$ = \frac{1}{n} \sum_{i = 1}^{n}E[X_i^2 - 2X_i\bar{X} + \bar{X}^2] = \frac{1}{n} \sum_{i = 1}^{n}E[X_i^2] - 2E[X_i\bar{X}] + E[\bar{X}^2] = $$
$$ = \frac{1}{n} \sum_{i = 1}^{n} E[X_i]^2 - \frac{2}{n}\sum_{i = 1}^{n}E[X_i\bar{X}] + \frac{1}{n}\sum_{i = 1}^{n}E[\bar{X}^2] = $$
Sabemos que: $
  \begin{cases}
    Var(\bar{X}) = E[\bar{X}^2] - E[\bar{X}]^2 \iff \frac{\sigma^2}{n} = E[\bar{X}^2] - \mu^2 \iff E[\bar{X}^2] = \frac{\sigma^2}{n} + \mu^2 \\
    Var(X_i) = E[X_i^2] - E[X_i]^2 \iff \sigma^2 = E[X_i^2] - \mu^2 \iff E[X_i^2] = \sigma^2 + \mu^2                                         \\
  \end{cases} $
$$ = \frac{1}{n} n(\sigma^2 + \mu ^2) + \frac{1}{n}n(\frac{\sigma^2}{n} + \mu^2) - \frac{2}{n}\sum_{i = 1}^{n}E[X_i \bar{X}] = $$
Ahora desarrollemos el término que falta:
$$E[X_i \bar{X}] = E\left[X_i \frac{1}{n}\sum_{j = 1}^{n}X_j\right] = \frac{1}{n}\sum_{j = 1}^{n}E[X_iX_j] = \frac{1}{n}\sum_{j = 1, j \neq i}^{n}E[X_iX_j] + \frac{1}{n}E[X_i^2] = $$
$$ = \frac{1}{n}\sum_{j = 1 \\ j \neq i}^{n}E[X_i]E[X_j] + \frac{1}{n}E[X_i^2] = \frac{1}{n}(n-1)\mu^2 + \frac{1}{n}(\sigma^2 + \mu^2) = \mu^2 + \frac{\sigma^2}{n}$$
$$ \implies = 2\mu^2 + \sigma^2(1 + \frac{1}{n}) - \frac{2}{n}\left(\sum_{i = 1}^{n}\mu^2 + \frac{\sigma^2}{n}\right) = 2\mu^2 + \sigma^2(1 + \frac{1}{n}) - 2(\mu^2 + \frac{\sigma^2}{n}) = \sigma^2(1 - \frac{1}{n}) \implies$$
$$ \implies E[\sigma_n^2] = \sigma^2(1 - \frac{1}{n}) = h(\theta) \text{ y } b(\sigma_n^2, \sigma^2) =  E[\sigma_n^2] - \sigma^2 = -\frac{\sigma^2}{n}$$
$$ \implies E[\sigma_n^2] = \sigma^2\left(1 - \frac{1}{n}\right) = h(\theta) \text{ y } b(\sigma_n^2, \sigma^2) =  E[\sigma_n^2] - \sigma^2 = -\frac{\sigma^2}{n}$$
}

\begin{observación}
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\begin{itemize}
  \item Puede ocurrir que no exista un estimador centrado de $\theta$
  \item Si $T$ es un estimador centrado para $\theta$, en general $h(T)$ no tiene por
        qué ser centrado para $h(\theta)$
  \item A pesar de que exista un estimador centrado para $\theta$, puede ser que no
        tenga sentido
\end{itemize}
\end{observación}

\ejemplo{
  Sea una m.a.s. de tamaño $n = 1$ de una población que sigue una distribución $Bin(1, \theta)$ demostrar que $T(X) = X^2$ no es un estimador centrado de $\theta^2$:\\
  $$ X \sim Bin(1, \theta) \equiv Bernouilli(\theta) \implies X^2 \sim Bernouilli(\theta) \implies E[X^2] = \theta \neq \theta^2$$
}

\ejemplo{
  Sea una m.a.s. de tamaño $n = 1$ de una población que sigue una distribución $Bin(1, \theta)$ demostrar que no existe un estimador centrado de $\theta^2$:\\
  Sea $X \sim Bin(1, \theta)$ y $T(X)$ un posible estimador de $\theta^2$. Para que $T(X)$ sea centrado (insesgado), debe satisfacer:
  \[
    E[T(X)] = \theta^2 \quad \forall \theta \in (0,1)
  \]
  Como $n=1$, el estimador $T(X)$ solo puede tomar dos valores:
  \begin{itemize}
    \item $T(1)$ cuando $X=1$ (éxito)
    \item $T(0)$ cuando $X=0$ (fracaso)
  \end{itemize}
  Calculamos la esperanza:
  \[
    E[T(X)] = T(1) \cdot P(X=1) + T(0) \cdot P(X=0) = T(1) \theta + T(0) (1 - \theta)
  \]
  Para que sea insesgado:
  \[
    T(1) \theta + T(0) (1 - \theta) = \theta^2 \quad \forall \theta \in (0,1)
  \]
  Reordenando términos:
  \[
    \theta^2 - (T(1) - T(0)) \theta - T(0) = 0 \quad \forall \theta \in (0,1)
  \]
  Esta es una ecuación cuadrática en $\theta$. Sin embargo, un polinomio de grado
  2 no nulo tiene como máximo 2 raíces reales distintas, mientras que la ecuación
  debe cumplirse para infinitos $\theta \in (0,1)$. La única posibilidad sería
  que los coeficientes fueran idénticamente cero:
  \[
    \begin{cases}
      1 = 0           \\
      T(1) - T(0) = 0 \\
      T(0) = 0
    \end{cases}
  \]
  Lo cual es un sistema incompatible (la primera ecuación $1=0$ es falsa). Por
  tanto, \textbf{no existe} ninguna elección de $T(0)$ y $T(1)$ que satisfaga la
  condición de insesgadez para todo $\theta \in (0,1)$. 
}

\ejemplo{
Sea una m.a.s. de tamaño $n = 1$ que sigue una distribución $X \sim Poisson(\theta)$ demuestra que $T(X) = (-2)^x$ es un estimador centrado para $h(\theta) = e^{-3\theta}$, pero $Var_{\theta}(T) = e^{4\theta} - e^{-6\theta} \to \infty$ no es un estimador de $h(\theta)$:
$$X \sim Poisson(\theta) \implies f_{\theta}(x) = \frac{e^{-\theta}\theta^x}{x!} \implies E[T] = E[(-2)^x] = \sum_{x = 0}^{+\infty} (-2)^x \frac{e^{-\theta}\theta^x}{x!} = \sum_{x = 0}^{+\infty} \frac{(-2\theta)^x}{x!}e^{-\theta} = $$
$$e^{-\theta} \sum_{x = 0}^{+\infty} \frac{(-2\theta)^x}{x!} = e^{-\theta}e^{-2\theta} = e^{-3\theta} = h(\theta)$$
$$Var_{\theta}(T) = E[T^2] - E[T]^2 = E[(-2)^{2x}] - e^{-6\theta} = \sum_{x = 0}^{n} (-2)^{2x} \frac{e^{-\theta}\theta^x}{x!} - e^{-6\theta} = \sum_{x = 0}^{n} \frac{(4\theta)^x}{x!}e^{-\theta} - e^{-6\theta} = $$
$$e^{-\theta}\sum_{x = 0}^{n} \frac{(4\theta)^x}{x!} - e^{-6\theta} = e^{-\theta}e^{4\theta} - e^{-6\theta} = e^{3\theta} - e^{-6\theta} \to \infty$$
Este procedimiento ha demostrador que $T(X)$ es centrado para $h(\theta)$, pero su varianza es demasiado grande (infinita) por lo que no es adecuado para la estimación.
}

\ejemplo{
Sea $(T_j)_{j \in \mathbb{N}}$ sucesión de estimadores centrados para $\theta$, entonces $\bar{T_k} = \frac{1}{k}\sum_{j = 1}^{k}T_j$ es un estimador centrado para $\theta$:
$$E[\bar{T_k}] = E[\frac{1}{k}\sum_{j = 1}^{k}T_j] = \frac{1}{k}\sum_{j = 1}^{k}E[T_j] = \frac{1}{k}\sum_{j = 1}^{k}\theta = \theta$$
}

\begin{definición}[Estimadores consistentes]
Una sucesión de estimadores \( T_n = T(X_1, \ldots, X_n) \) se dice \textbf{consistente} para el parámetro \( \theta \) si, para todo \( \theta \in \Theta \), se cumple que
\[
  E_{\theta}[T_n] \underset{n \to \infty}{\longrightarrow} \theta \quad \text{y} \quad  V_{\theta}(T_n) \underset{n \to \infty}{\longrightarrow} 0.
\]
Es decir, \( T_n \) es un estimador insesgado asintóticamente y su varianza
tiende a cero conforme aumenta el tamaño de la muestra, lo que garantiza que \(
T_n \) converge en probabilidad a \( \theta \).

\end{definición}

\begin{proposición}
Si $T_{n} = T(X_1, \ldots, X_n)$ es una sucesión de estimadores tales que $\forall \theta \in \Theta, \ E_{\theta}[T_n] \underset{n \to \infty}{\longrightarrow} \theta$, $V_{\theta}(T_n) \underset{n \to \infty}{\longrightarrow} 0$, entonces $T_n$ es consistente para $\theta$
\end{proposición}
\begin{proof}
  $E_{\theta}\left[\left(T_{n}-\theta\right)^{2}\right]=V_{\theta}\left(T_{n}\right)+b\left(T_{n}, \theta\right)^{2} \underset{n \rightarrow \infty}{\longrightarrow} 0, \forall \theta \in \Theta$ entonces $T_{n} \xrightarrow[n \rightarrow \infty]{\text { m.c. }} \theta, \forall \theta \in \Theta$
\end{proof}

\ejemplo{
  El estimador $a_k = \frac{1}{n}\sum_{i = 1}^{n}X_i^k$ es un estimdor consistente pra el mometo poblacional con respecto al origen de orden $k$, es decir, es estimador del parámetro $\theta = \alpha_k$.
}

\ejemplo{
Sea una m.a.s. de tamaño $n$ de una población de $Bernouilli(\theta)$ comprobemos que el estimador $T_n = \frac{1}{n+2}\left(\sum_{i = 1}^{n}X_i + 1\right)$ es un estimador consistente para $\theta$:
$$\lim_{n \to \infty} E[T_n] = \lim_{n \to \infty} E\left[\frac{1}{n+2}\sum_{i = 1}^{n}X_i + 1\right] = \lim_{n \to \infty} \frac{1}{n+2}\sum_{i = 1}^{n}(E[X_i] + 1) = \lim_{n \to \infty}\frac{n\theta + 1}{n+2} = \theta$$
$$\lim_{n \to \infty} V[T_n] = \lim_{n \to \infty} V\left[\frac{1}{n+2}\sum_{i = 1}^{n}X_i + 1\right] = \lim_{n \to \infty} \frac{1}{(n+2)^2}\sum_{i = 1}^{n}V[X_i] = \lim_{n \to \infty} \frac{n\theta(1-\theta)}{(n+2)^2} = 0$$
}

\ejemplo{
  Sea una m.a.s. de tamaño $n$ de una población con distribución $N(\mu; \sigma^2)$ siendo $\theta = (\mu, \sigma)$, el estimador $T_1(\vec{X}) = \bar{X}$ es consistente para estimar $h(\theta) = \mu$, ya que: 
  $$ \lim_{n \to \infty} E[T_1] = \lim_{n \to \infty} E[\bar{X}] = \lim_{n \to \infty} E\left[\frac{1}{n}\sum_{i = 1}^{n}X_i\right] = \lim_{n \to \infty} \frac{1}{n}\sum_{i = 1}^{n}E[X_i] = \lim_{n \to \infty} \frac{1}{n}\sum_{i = 1}^{n}\mu = \mu$$
  $$ \lim_{n \to \infty} V[T_1] = \lim_{n \to \infty} V\left[\frac{1}{n}\sum_{i = 1}^{n}X_i\right] = \lim_{n \to \infty} \frac{1}{n^2}\sum_{i = 1}^{n}V[X_i] = \lim_{n \to \infty} \frac{1}{n^2}\sum_{i = 1}^{n}\sigma^2 = 0$$
}

\subsection{Estimadores Bayesianos}

El enfoque bayesiano trata a los parámetros de las distribuciones como
variables aleatorias con su propia función de distribución, a diferencia de
considerar que toma valores fijos desconocidos. Para desarrollar este punto de
vista, se asigna una distribución a $\theta$ llamada \underline{distribución
  inicial o a priori} $\pi(\theta)$ y se actualiza esta distribución con la
información de la muestra para obtener la \underline{distribución final o a
  posteriori} $\pi(\theta | x_1, \ldots, x_n)$ $$\pi\left(\theta \mid x_{1},
  \ldots, x_{n}\right)=\frac{\pi(\theta) f\left(x_{1}, \ldots, x_{n} \mid
    \theta\right)}{m\left(x_{1}, \ldots, x_{n}\right)}$$ donde m es la
\underline{distribución predictiva}, dada por $$m(x_1, \ldots, x_n) =
  \int_{\Theta} \pi(\theta)f(x_1, \ldots, x_n | \theta)d\theta$$

A la función $f(x_1, \ldots, x_n | \theta)$ se le llama \underline{función de
  verosimilitud} y es la función de densidad de la muestra condicionada al
parámetro $\theta$.\\

\begin{observación}
Antes de tomar la muestra, la información sobre $\theta$ viene dada por $\pi(\theta)$ y tras la experimentación se debe utilizar $\pi\left(\theta \mid x_{1}, \ldots, x_{n}\right)$. El estimador bayesiano de $\theta$ es toda la distribución final y por extensión cualquier medida de centralización correspondiente a esta distribución
\end{observación}

\ejemplo{
  Sea una muestra aleatoria simple (m.a.s.) de tamaño \( n \) proveniente de una población que sigue una distribución \( Bin(1, \theta) \)
  \[
    X_i \sim Bernoulli(\theta), \quad i = 1, \dots, n.
  \]
  Esto significa que cada \( X_i \) puede tomar valores 0 o 1 con probabilidades: \[
    P(X_i = 1 | \theta) = \theta, \quad P(X_i = 0 | \theta) = 1 - \theta.
  \]
  Buscamos encontrar la distribución a posteriori de \( \theta \) usando
  inferencia Bayesiana.

  Antes de observar los datos, suponemos que \( \theta \) sigue una distribución
  uniforme en el intervalo \( (0,1) \):
  \[
    \theta \sim U(0,1).
  \]
  Dado que la densidad de la distribución uniforme en \( (0,1) \) es:
  \[
    \pi(\theta) = \frac{1}{1-0} = 1, \quad 0 \leq \theta \leq 1,
  \]
  esto implica que cualquier valor de \( \theta \) dentro del intervalo \([0,1]\)
  es igualmente probable antes de ver los datos.

  Dado que la muestra es independiente, la función de verosimilitud se obtiene
  como el producto de las probabilidades individuales:
  \[
    f(x_1, \dots, x_n | \theta) = \prod_{i=1}^{n} P(X_i = x_i | \theta).
  \]
  Usando la función de masa de probabilidad de la distribución Bernoulli:
  \[
    P(X_i = x_i | \theta) = \theta^{x_i} (1-\theta)^{1-x_i},
  \]
  tenemos:
  \[
    f(x_1, \dots, x_n | \theta) = \prod_{i=1}^{n} \theta^{x_i} (1-\theta)^{1-x_i}.
  \]
  Factorizando los exponentes:
  \[
    f(x_1, \dots, x_n | \theta) = \theta^{\sum_{i=1}^{n} x_i} (1-\theta)^{n - \sum_{i=1}^{n} x_i}.
  \]
  Definiendo \( S = \sum_{i=1}^{n} x_i \), que es la cantidad de éxitos en la
  muestra, la función de verosimilitud se expresa como:
  \[
    f(x_1, \dots, x_n | \theta) = \theta^S (1-\theta)^{n-S}.
  \]
  Aplicando el Teorema de Bayes, la distribución a posteriori es proporcional a:
  \[
    \pi(\theta | x_1, \dots, x_n) \propto \pi(\theta) f(x_1, \dots, x_n | \theta).
  \]
  Sustituyendo \( \pi(\theta) = 1 \) y la función de verosimilitud:
  \[
    \pi(\theta | x_1, \dots, x_n) \propto \theta^S (1-\theta)^{n-S}.
  \]
  Reconociendo que esta es la forma de la función de densidad de una distribución
  Beta:
  \[
    Beta(a, b) \propto \theta^{a-1} (1-\theta)^{b-1},
  \]
  identificamos que la posteriori es una distribución Beta con parámetros:
  \[
    \pi(\theta | x_1, \dots, x_n) \sim Beta(S+1, n-S+1).
  \]
  En general, si en lugar de una uniforme usamos una distribución a priori Beta
  con parámetros \( \alpha \) y \( \beta \):
  \[
    \theta \sim Beta(\alpha, \beta),
  \]

  cuya función de densidad es:
  \[
    \pi(\theta) = \frac{\theta^{\alpha - 1} (1-\theta)^{\beta - 1}}{B(\alpha, \beta)},
  \]
  siguiendo el mismo procedimiento, obtenemos que la distribución a posteriori
  también sigue una Beta con parámetros actualizados:
  \[
    \pi(\theta | x_1, \dots, x_n) \sim Beta(\alpha + S, \beta + n - S).
  \]
  Esto muestra que \textbf{la familia de distribuciones Beta es conjugada para la
    distribución Bernoulli-Binomial}.

  El valor esperado de \( \theta \) dado los datos es:
  \[
    E[\theta | x_1, \dots, x_n] = \frac{S + \alpha}{n + \alpha + \beta}.
  \]
  Si tomamos la esperanza como una combinación convexa:
  \[
    E[\theta | x_1, \dots, x_n] = \frac{n}{n + \alpha + \beta} \bar{x} + \frac{\alpha + \beta}{n + \alpha + \beta} \frac{\alpha}{\alpha + \beta}.
  \]
  Esto indica que la esperanza de la distribución a posteriori es una mezcla
  entre la media muestral \( \bar{x} \) y la esperanza a priori \(
  \frac{\alpha}{\alpha + \beta} \), ponderada por los tamaños relativos de la
  muestra y la información previa. }

\ejemplo{
Sea una m.a.s. de tamaño $n$ de una población $N(\mu, \sigma)$ y con $\mu \sim N(\mu_0, \sigma_0)$ y $\sigma$ conocida entonces $\pi(\mu | x_1, \ldots, x_n) \sim N(\mu_1, \sigma_1)$:
$$\mu \sim N(\mu_0, \sigma_0) \implies \pi(\mu) = \frac{1}{\sqrt{2\pi}\sigma_0}e^{-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}}$$
$$X \sim N(\mu, \sigma) \implies f(x | \mu) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x - \mu)^2}{2\sigma^2}} \implies f(x_1, \ldots, x_n | \mu) = \prod_{i = 1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i - \mu)^2}{2\sigma^2}} = $$
$$ = \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{\sum_{i = 1}^{n}(x_i - \mu)^2}{2\sigma^2}}$$
$$\pi(\theta | x_1, \ldots, x_n) = \frac{\frac{1}{\sqrt{2\pi}\sigma_0}e^{-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}} \cdot \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{\sum_{i = 1}^{n}(x_i - \mu)^2}{2\sigma^2}}}{\int_{\mathbb{R}}\frac{1}{\sqrt{2\pi}\sigma_0}e^{-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}} \cdot \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{\sum_{i = 1}^{n}(x_i - \mu)^2}{2\sigma^2}}d\mu} = $$
$$ = \frac{e^{-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}} \cdot e^{-\frac{\sum_{i = 1}^{n}(x_i - \mu)^2}{2\sigma^2}}}{\int_{\mathbb{R}}e^{-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}} \cdot e^{-\frac{\sum_{i = 1}^{n}(x_i - \mu)^2}{2\sigma^2}}d\mu} = \frac{e^{\frac{-\mu^2 - \mu_0^2 + 2\mu\mu_0}{2\sigma_0^2}} \cdot e^{\frac{-\sum_{i = 1}^{n}x_i^2 - n\mu^2 + 2\mu\sum_{i = 1}^{n}x_i}{2\sigma^2}}}{\int_{\mathbb{R}}e^{\frac{-\mu^2 - \mu_0^2 + 2\mu\mu_0}{2\sigma_0^2}} \cdot e^{\frac{-\sum_{i = 1}^{n}x_i^2 - n\mu^2 + 2\mu\sum_{i = 1}^{n}x_i}{2\sigma^2}}d\mu} = $$
$$ = \frac{e^{\frac{-\mu^2 + 2\mu\mu_0}{2\sigma_0^2}} \cdot e^{\frac{-n\mu^2 + 2\mu\bar{X}n}{2\sigma^2}}}{\int_{\mathbb{R}} e^{\frac{-\mu^2 + 2\mu\mu_0}{2\sigma_0^2}} \cdot e^{\frac{-n\mu^2 + 2\mu\bar{X}n}{2\sigma^2}}d\mu} = \frac{e^{-\frac{1}{2}\left(\frac{1}{\sigma_0^2} + \frac{1}{\frac{\sigma^2}{n}}\right)\mu^2 + \left(\frac{\mu_0}{\sigma_0^2} + \frac{\bar{X}}{\frac{\sigma^2}{n}}\right)\mu}}{\int_{\mathbb{R}} e^{-\frac{1}{2}\left(\frac{1}{\sigma_0^2} + \frac{1}{\frac{\sigma^2}{n}}\right)\mu^2 + \left(\frac{\mu_0}{\sigma_0^2} + \frac{\bar{X}}{\frac{\sigma^2}{n}}\right)\mu}d} = \frac{e^{-\frac{1}{2V}\mu^2 + \frac{E}{V}\mu}}{\int_{\mathbb{R}} e^{-\frac{1}{2V}\mu^2 + \frac{E}{V}\mu}d\mu}$$
donde $V = \frac{1}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}$ y $E = \frac{\mu_0}{\sigma_0^2} + \frac{\bar{X}}{\frac{\sigma^2}{n}}$.\\
Considerando la normal $N(E, \sqrt{V})$ y la integral de su función de densidad que es igual a 1:
$$\int_{\mathbb{R}} \frac{1}{\sqrt{2\pi V}}e^{-\frac{1}{2V}(\mu - E)^2}d\mu = 1$$
entonce factorizando el exponente obtenemos:
$$1 = \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi V}}e^{-\frac{1}{2V}(\mu^2 - 2E\mu + E^2)}d\mu = \frac{1}{\sqrt{2\pi V}}e^{-\frac{E^2}{2V}}\int_{\mathbb{R}} e^{-\frac{1}{2V}\mu^2 + \frac{E}{V}\mu}d\mu$$
$$ \implies \int_{\mathbb{R}} e^{-\frac{1}{2V}\mu^2 + \frac{E}{V}\mu}d\mu = \sqrt{2\pi V}e^{\frac{E^2}{2V}}$$
Luego entonces tenemos que
$$\pi(\mu | x_1, \ldots, x_n) = \frac{e^{-\frac{1}{2V}\mu^2 + \frac{E}{V}\mu}}{\sqrt{2\pi V}e^{\frac{E^2}{2V}}} = \frac{1}{\sqrt{2\pi V}}e^{-\frac{1}{2V}\left(\mu - E\right)^2}$$
}

\begin{definición}[Estadístico suficiente bayesiano]
$T = T(X_1, \ldots, X_n)$ es un estadístico suficiente bayesiano para $\theta$ para la familia $\mathcal{P} = \{f(\vec{x}|\theta) : \theta \in \Theta\}$ si cualquiera que sea la distribución inicial $\pi(\theta)$, se tiene que la distribución final dada por la muestra y por el valor del estadístico son la misma. Es decir:
$$\pi(\theta | x_1, \ldots, x_n) = \pi(\theta | t) \ \text{ tal que } \ t = T(x_1, \ldots, x_n)$$
\end{definición}

\begin{teorema}[Versión bayesiana del Teorema de Factorización de Fisher]
  $T = T(X_1, \ldots, X_n)$ es un estadístico suficiente para $\theta$ si y sólo si $T$ es un estadístico suficiente bayesiano para $\theta$ respecto a $\pi(\theta)$, cualquiera que sea la distribución inicial $\pi(\theta)$
\end{teorema}

\begin{proof}
  $$\begin{aligned}
       & \Rightarrow \pi(\theta | x_1, \ldots, x_n) = \frac{\pi(\theta)f(x_1, \ldots, x_n | \theta)}{m(x_1, \ldots, x_n)} = \frac{\pi(\theta)g(t | \theta)f(x_1, \ldots, x_n | t, \theta)}{m(x_1, \ldots, x_n)} = \frac{\pi(\theta)g(t | \theta)}{\int_{\Theta}\pi(\theta)g(t | \theta)d\theta} = \pi(\theta | t) \\
       & \Rightarrow f(x_1, \ldots, x_n | \theta) = \frac{\pi(\theta | x_1, \ldots, x_n)m(x_1, \ldots, x_n)}{\pi(\theta)} = \frac{\pi(\theta | t)}{\pi(\theta)}m(x_1, \ldots, x_n)
    \end{aligned}$$
\end{proof}

\subsection{Criterios de comparación de estimadores}

\begin{definición}[Error cuadrático medio]
Dado un estimador $T(X_1, \ldots, X_n)$ de $\theta$, se denomina error cuadrático medio $ECM$ a la expresión en función de $\theta$:
$$ECM_T(\theta) = E_{\theta}[(T - \theta)^2]$$
Conceptualmente, el error cuadrático medio es una medida que indica qué tan cerca está un estadístico del parámetro verdadero que se intenta estimar.
\end{definición}

\begin{proposición}
Dado un estimador $T(X_1, \ldots, X_n)$ de $\theta$, se tiene que:
$$ECM_T(\theta) = V_{\theta}(T) + b(T, \theta)^2$$
\end{proposición}
\begin{proof}
  $$ECM_T(\theta) = E_{\theta}[(T - \theta)^2] = E_{\theta}[T^2 - 2T\theta + \theta^2] = E_{\theta}[T^2] - 2\theta E_{\theta}[T] + \theta^2 = V_{\theta}(T) + b(T, \theta)^2$$
\end{proof}

\begin{observación}
El sesgo mide qué tanto se desvía, en promedio, el estimador del valor verdadero del parámetro. \\
La varianza del estimador mide cómo varían las estimaciones (del estimador) si tomamos diferentes muestras. \\
Es decir, responden a las preguntas de ¿Apunta al lugar correcto? y ¿Qué tan dispersas están las estimaciones? respectivamente.
\end{observación}

\ejemplo{
Dada una m.a.s. de tamaño $n$ de una población $X \sim Bernouilli(\theta)$ el error cuadrático medio del estimador bayesiano $\bar{X}$:
\[
E_{\theta}[(T - \theta)^2] = E_{\theta}[(\bar{X} - \theta)^2] = E_{\theta}[\bar{X}^2 - 2\bar{X}\theta + \theta^2] = E_{\theta}[\bar{X}^2] - 2\theta E_{\theta}[\bar{X}] + \theta^2 =
\]
\[
= \frac{1}{n^2}\sum_{i = 1}^{n}E_{\theta}[X_i^2] - 2\theta \frac{1}{n} \sum_{i = 1}^{n}E_{\theta}[X_i] + \theta^2 = \frac{1}{n^2}n(\theta^2 + \theta(1-\theta)) - 2\theta^2 + \theta^2 = \frac{\theta(1-\theta)}{n} = \frac{\theta}{n} - \frac{\theta^2}{n} = \frac{\theta(1-\theta)}{n}
\]
}

\ejemplo{
  Dada una m.a.s. de tamaño $n$ de una población $X \sim N(\mu, \sigma)$, se sab quelos estimadores centrados de ambos paráetros son $\bar{X}$ para $\mu$ y $S^2$ para $\sigma^2$, respectivamente. Y sus errores cuadráticos medios son:
  $$ECM_{\bar{X}}(\mu) = Var[\bar{X}] = \frac{\sigma^2}{n} \quad ECM_{S^2}(\sigma^2) = Var[S^2] = \frac{2\sigma^4}{n-1}$$
  Sea $b_2 = \sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n}(X_i - \bar{X})^2$ otro estimador centrado para $\sigma^2$, calculemos su varianza y sesgo:
  Gracias al cálculo realizado en un ejemplo anterior tenemos que $b(\sigma_n^2, \sigma) = -\frac{\sigma^2}{n}$, por lo que sólo queda calcular la varianza, la cual es de la forma
  $$V_{\theta}(\sigma_n^2) = \frac{2\sigma^4}{n} \implies ECM_{\sigma_n^2}(\sigma^2) = V_{\theta}(\sigma_n^2) + b(\sigma_n^2, \sigma)^2 = \frac{2\sigma^4}{n} - \frac{\sigma^4}{n^2} = \frac{2n - 1}{n^2}\sigma^4 < \frac{2\sigma^4}{n-1} = ECM_{S^2}(\sigma^2)$$
  Por lo que se puede concluir que $S^2$ es un estimador más eficiente que $\sigma_n^2$ para $\sigma^2$
  A pesar de que matemáticamente se parezcan mas, la corrección para mejorar la eficiencia se la conoce como \underline{corrección de Bessel}.
}

\begin{observación}
En general, si $T_1$ y $T_2$ son dos estimadores de $\theta$ y $ECM_{T_1}(\theta) < ECM_{T_2}(\theta)$, entonces $T_1$ es un estimador más eficiente que $T_2$ para $\theta$
\end{observación}



\begin{definición}[Pérdida final esperada]
Dado un estimador $T(X_1, \ldots, X_n)$, la distribución inicial $\pi(\theta)$ y la función de pérdida $\mathcal{L}(\theta, t)$ donde $t$ son los valores que toma el estimador, se define la Pérdida Final Esperada o $PFE$ o el riesgo a posteriori como:
$$PFE_T = E[\mathcal{L}(t, \theta) | X_1 = x_1, \ldots, X_n = x_n] = \int_{\Theta} \mathcal{L}(\theta, t)\pi(\theta | x_1, \ldots, x_n)d\theta$$
\end{definición}

\begin{proposición}
Puede darse que existan varias funciones de pérdida, veamos las más comunes:
\begin{enumerate}
  \item \textbf{Pérdida cuadrática}: Si $\mathcal{L}(\theta, t) = (\theta - t)^2$ entonces $PFE(t) = V(\theta | x_1,
          \ldots, x_n) + b(T, \theta)^2$ y la pérdida final esperada se minimimia en $t^*
          = E[\theta | x_1, \ldots, x_n]$
  \item \textbf{Pérdida absoluta}:Si $\mathcal{L}(\theta, t) = |\theta - t|$ entonces $PFE(t) = E[|\theta - t| |
              x_1, \ldots, x_n]$ y la pérdida final esperada se minimiza en la mediana de la
        distribución final (estimador bayesiano)
    \item \textbf{Pérdida 0-1}: Si $\mathcal{L}(\theta, t) = \begin{cases} 0 & \text{si } t = \theta \\ 1 & \text{si } t \neq \theta \end{cases}$ entonces $PFE_T = P(\theta \neq t | x_1, \ldots, x_n) = 1 - P(\theta = t | x_1, \ldots, x_n)$. Se usa en problemas de decisioón o clasificación donde sólo importa acertar o errar, no cuán lejos se está del valor correcto, es por ello que es mas difícil saber dónde se encuentra el valor que la minimiza.
\end{enumerate}
\end{proposición}

\begin{proof}
  \begin{enumerate}
    \item Si  $\mathcal{L}(\theta, t) = (\theta - t)^2 \implies$ $$PFE_T = E[(\theta -
                t)^2 | x_1, \ldots, x_n] = E[((\theta - E[\theta | x_1, \ldots, x_n]) +
                (E[\theta | x_1, \ldots, x_n] - t))^2 | x_1, \ldots, x_n] $$ Si expandimos el
          cuadrado: $$ (\theta - E[\theta | x_1, \ldots, x_n])^2 + 2(\theta - E[\theta |
                x_1, \ldots, x_n])(E[\theta | x_1, \ldots, x_n] - t) + (E[\theta | x_1, \ldots,
                x_n] - t)^2 \implies$$ Calculemos cada una de las esperanzas por separado:
          $$E[(\theta - E[\theta | x_1, \ldots, x_n])^2 | x_1, \ldots, x_n] = V(\theta |
            x_1, \ldots, x_n)$$ $$E[(\theta - E[\theta | x_1, \ldots x_n] | x_1, \ldots,
                x_n)] = 0 \text{ por propiedades de la esperanza condicional}$$ $$E[(E[\theta |
                    x_1, \ldots, x_n] - t)^2 | x_1, \ldots, x_n] = (E[\theta | x_1, \ldots, x_n] -
            t)^2 \text{ dada una muestra, se vuelve una constante}$$ Por lo que se puede
          concluir que $PFE_T = V(\theta | x_1, \ldots, x_n) + (E[\theta | x_1, \ldots,
                x_n] - t)^2$ y se minimiza en $t^* = E[\theta | x_1, \ldots, x_n]$
    \item  Si $\mathcal{L}(\theta, t) = |\theta - t| \implies$ $$PFE_T = E[|\theta - t| |
                x_1, \ldots, x_n] = \int_{\Theta} |\theta - t|F(\theta | x)d\theta \implies$$
          Dividiendo la integral entre los valores positivos y los negativos de $\theta$,
          nos queda que la integral es la suma de: $$\int_{-\infty}^{t} (t -
            \theta)f(\theta | x)d\theta + \int_{t}^{+\infty} (\theta - t)f(\theta |
            x)d\theta$$ Si derivamos con respecto a $t$ y obtenemos $0$ podemos ver un
          posible punto máximo o mínimo de la función: $$\frac{d}{dt}PFE_T = \frac{d}{dt}
            \int_{-\infty}^{t} (t - \theta)f(\theta | x)d\theta + \frac{d}{dt}
            \int_{t}^{+\infty} (\theta - t)f(\theta | x)d\theta = $$ $$ =
            \int_{-\infty}^{t} f(\theta | x)d\theta - \int_{t}^{+\infty} f(\theta |
            x)d\theta = 0 \iff$$ $$F_X(t) = 1 - F_X(t) \iff F_X(t) = \frac{1}{2} \implies
            t^* = \text{mediana de la distribución final}$$\

  \end{enumerate}
\end{proof}


\begin{observación}
  $$PFE(t) = \int_{\Theta} \underbrace{\mathcal{L}(\theta, t)}_{\text{pérdida si fuera } \theta}
  \cdot 
  \underbrace{\pi(\theta \mid x)}_{\text{qué tan probable es } \theta}
  \, d\theta$$
\end{observación}

\ejemplo{
  Sea una m.a.s. de tamaño $n$ de una población $X \sim Bernoulli(\theta)$ y pérdida cruadrática e informaci`´on inicial dadas por na distribución $Beta(\alpha_0, \beta_0)$. Por un ejemplo anterior en el apartado de \textbf{Estimadores bayesianos}, que la distribución final o a posteriori es $Beta(\alpha_0 + \sum_{i = 1}^{n}X_i, \beta_0 + n - \sum_{i = 1}^{n}X_i)$ 
  $$\implies PFE_T = E[(t - \theta)^2 | \vec{x}] = \int_{0}^{1} (t - \theta)^2 \cdot \frac{\theta^{\alpha_0 + \sum_{i = 1}^{n}X_i - 1}(1 - \theta)^{\beta_0 + n - \sum_{i = 1}^{n}X_i - 1}}{\beta(\alpha_0 + \sum_{i = 1}^{n}X_i, \beta_0 + n - \sum_{i = 1}^{n}X_i)}d\theta$$
  $$ = \int_{0}^{1} (t^2 - 2t\theta + \theta^2) \cdot Beta(\theta; \alpha_1, \beta_1) \, d\theta = t^2\int_{0}^{1} Beta(\theta)d\theta - 2t\int_{0}^{1} \theta\cdot Beta(\theta) d\theta + \int_{0}^{1} \theta^2 \cdot Beta(\theta)$$
  $$ = t^2 - 2t\frac{\alpha_1}{\alpha_1 + \beta_1} + \frac{\alpha_1(\alpha_1 + 1)}{(\alpha_1 + \beta_1)(\alpha_1 + \beta_1 + 1)} = t^2 - 2t \cdot \mu + E[\theta^2]$$
  Expresión que se minimiza cuando $t = E[\theta | \vec{x}] = \frac{\alpha_0 + \sum X_i}{\alpha_0 + \beta_0 + n} = \frac{\alpha_1}{\alpha_1 + \beta_1}$
}

\begin{definición}[Estimador centrado uniformemente de mínima varianza]
$T^{*} = T^{*}(X_1, \ldots, X_n)$ es un estimador centrado uniformemente de mínima varianza para $\theta$ si y sólo si $E_{\theta}[T^{*}] = \theta$ y para cualquier otro estimador $T = T(X_1, \ldots, X_n)$ con $E_{\theta}[T] = \theta$, se tiene que $V_{\theta}(T^{*}) \leq V_{\theta}(T)$, $\forall \theta \in \Theta$
\end{definición}

\begin{proposición}[Unicidad de los estimadores centrados uniformemente de mínima varianza]
Si existe un estimador centrado uniformemente de mínima varianza para $\theta$, entonces es único c.s.
\end{proposición}

\begin{proof}
  Sean $T_1$ y $T_2$ dos estimadores centrados uniformemente de mínima varianza para $\theta$, demostremos que entones $T_1 \overset{c.s.}{\equiv} T_2$. \\
  Sea $T = \frac{T_1 + T_2}{2} \implies E_{\theta}[T] = \theta$
  $$V_{\theta}(T) = V_{\theta}(\frac{T_1 + T_2}{2}) = \frac{1}{4}(V_{\theta}(T_1) + V_{\theta}(T_2) + 2Cov_{\theta}(T_1, T_2)) = \frac{V_{\theta}(T_1)}{2} + \frac{Cov(T_1, T_2)}{2}$$
  Sabemos que la correlación de dos variables aleatorias está acotada por 1, entonces:
  $$\rho_{\theta}(T_1, T_2) = \frac{Cov(T_1, T_2)}{\sqrt{V_{\theta}(T_1)V_{\theta}(T_2)}} \leq 1 \iff Cov(T_1, T_2) \leq V_{\theta}(T_1) \implies$$
  $$\implies V_{\theta}(T) \leq V_{\theta}(T_1) $$
  Pero además como $T_1$ es un estimador centrado uniformemente de mínima varianza, ningún otro estimador centrado puede tener un avarianza más pequeña: $V_{\theta}(T) \leq V_{\theta}(T_1)$
  Por lo tanto $V_{\theta}(T) = V_{\theta}(T_1) = Cov_{\theta}(T_1, T_2) = V_{\theta}(T_2) \implies \rho_{\theta}(T_1, T_2) = 1 \implies \exists a, b : T_1 = aT_2 + b \iff E[T_1] = aE[T_2] + b \iff \theta = a\theta + b \iff a = 1, b = 0 \implies T_1 \overset{c.s.}{\equiv} T_2$
\end{proof}

\ejemplo{
Sea una m.a.s. de tamaño $n$ de una población $X \sim N(\mu, \sigma^2)$ vamos a trabajar con la familia de estimadores $T_k = \{k \cdot S_n^2\}$. Calculemos cuál es el menor error cuadrático medio. Y tomemos como función del estimador $d(\theta) = \theta^2$
$$ECM_{T_k}(\theta) = V_{\theta}(T_k) + b(T_k, \theta)^2 \implies$$ $$ \begin{cases} b(T_k, \theta) = E_{\theta}[T_k] - d(\theta)  = \frac{k\theta^2}{n} \cdot E_{\theta}[\frac{n}{\theta^2}S_n^2] =  \frac{k\theta^2}{n} \cdot E_{\theta}[\chi_{n}] - \theta^2 = \frac{k\theta^2}{n}\cdot n - \theta^2  = k\theta^2 - \theta = (k-1)\theta^2\\ V_{\theta}(T_k) = V_{\theta}[k \cdot S_n^2] = \frac{k^2\theta^4}{n^2}V_{\theta}[\frac{n}{\theta^2}S_n^2] = \frac{k^2\theta^4}{n^2}V_{\theta}[\chi_n^2] = \theta^4 \cdot \frac{k^2}{n^2} \cdot 2n = \frac{2\theta^4k^2}{n}\end{cases} \implies $$ $$ \implies ECM_{T_k}(\theta) = \frac{2\theta^4k^2}{n} + (k-1)^2\theta^4 = \theta^4 \left(\frac{2k^2}{n} + (k-1)^2\right)$$
Para encontrar el valor de $k$ que minimiza el error cuadrático medio, derivamos con respecto a $k$ e igualamos a 0:
$$\frac{d}{dk}ECM_{T_k}(\theta) = 0 \iff \frac{d}{dk}\left(\theta^4 \left(\frac{2k^2}{n} + (k-1)^2\right)\right) = 0 \iff \theta^4 \left(\frac{4k}{n} + 2(k-1)\right) = 0 \iff$$
$$\iff \frac{4k}{n} + 2(k-1) = 0 \iff 4k + 2n(k-1) = 0 \iff 4k + 2nk - 2n = 0 \iff 4k + 2nk = 2n \iff$$  $$\iff k(4 + 2n) = 2n \iff k = \frac{2n}{4 + 2n} = \frac{n}{2 + n}$$
Por lo que el estimador que minimiza el error cuadrático medio es $T_{\frac{n}{2 + n}} = \frac{n}{2 + n}S_n^2$
}

\begin{teorema}
  El estimador centrado uniformemente de mínima varianza es función simétrica de las observaciones
\end{teorema}

\ejemplo{
  Sea una m.a.s. de tamaño $n = 2$, entonces el estimador $T_1 = \frac{X_1}{X_2}$ no puede ser un estimador centrado uniformemente de mínima varianza, ya que si lo fuera, entonces para el nuevo estimador $T_2 = \frac{X_2}{X_1}$ se tendría que $E_{\theta}[T_2] = E_{\theta}[T_1]$ y $V_{\theta}(T_2) = V_{\theta}(T_1)$ con lo que el estimador $T = \frac{T_1 + T_2}{2} = \frac{X_1^2 + X_2^2}{X_1X_2}$ sería tal que $V_{\theta} < V_{\theta}(T_1)$, lo cual es una contradicción
}

\begin{observación}
En general si tienes un estimador $T$ que no es simétrico, puedes promediar sobre todas las permutaciones posibles para crear un nuevo estimador $\bar{T}$:
$$\bar{T} = \frac{1}{n!}\sum_{i = 1}^{n!} T_j$$
Este nuevo estimador es simétrico respecto a las observaciones y $V_{\theta}(\bar{T}) \leq V_{\theta}(T_j) \forall j$ donde se cumple que $V_{\theta}(\bar{T}) < V_{\theta}(T_j)$ si $T_j$ no es un estimador simétrico. Además, se cumple que $E_{\theta}[\bar{T}] = E_{\theta}[T]$
\end{observación}

\begin{teorema}[Teorema de caracterización del ECUMV]
  Sea $T_1 = T_1(X_1, \ldots, X_n)$ un estimador centrado para $\theta$ ($E_{\theta}[T_1] = \theta$) y $V_{\theta}(T_1) < \infty$ entonces $T_1$ es el ECUMV para $\theta$ si y sólo si para cualquier otro estimador $T_2 = T_2(X_1, \ldots, X_n)$ con $E_{\theta}[T_2] = 0$ y $V_{\theta}(T_2) < \infty$ se tiene que $E_{\theta}[T_1T_2] = 0$
\end{teorema}

\begin{corolario}
  Si $T_1 = T_2(X_1, \ldots X_n)$ y $T_2 = T_2(X_1, \ldots, X_n)$ son ECUMV para $h_1(\theta)$ y $h_2(\theta)$ respectivamente, entonces $b_1T_1 + b_2T_2$ es el ECUMV para $b_1h_1(\theta) + b_2h_2(\theta)$
\end{corolario}

\begin{teorema}[Teorema de Rao-Blackwell]
  Si $T = T(X_1, \ldots, X_n)$ es un estimador centrado para $\theta$ con $V_{\theta}(T) < \infty$ y $S(X_1, \ldots, X_n)$ es un estadístico suficiente, entonces $g(S) = E[T | S]$ es un estimador centrado para $\theta$ con $V_{\theta}(g(S)) \leq V_{\theta}(T)$
\end{teorema}

\begin{proof}
  Si $S$ es una estadística suficiente para el parámetro $\theta$, entonces $E[T \mid S]$ no depende de $\theta$. Por las propiedades de la esperanza condicionada, se tiene que:

  \[
    E_{\theta}[g(S)] = E_{\theta}[E[T \mid S]] = E_{\theta}[T] = \theta
  \]

  Ahora, considerando la varianza de $T$:

  \[
    V_{\theta}(T) = E_{\theta}[(T - \theta)^2] = E_{\theta}[(T - g(S) + g(S) - \theta)^2]
  \]

  Expandiendo el cuadrado y usando la linealidad de la esperanza:

  \[
    V_{\theta}(T) = E_{\theta}[(T - g(S))^2] + E_{\theta}[(g(S) - \theta)^2] + 2E_{\theta}[(T - g(S))(g(S) - \theta)]
  \]

  El último término se anula debido a la siguiente propiedad de la esperanza
  condicionada:

  \[
    E_{\theta}[(T - g(S))(g(S) - \theta)] = \iint (t - g(s))(g(s) - \theta) dF_{\theta}(t, s)
  \]

  Descomponiendo en términos de la distribución condicional:

  \[
    = \int (g(s) - \theta) \left( \int (t - g(s)) dF(t \mid s) \right) dF_{\theta}(s) = 0
  \]

  Ya que $E[T \mid S] = g(S)$, la esperanza condicional centrada es cero.

  Por lo tanto,

  \[
    V_{\theta}(T) = V_{\theta}(g(S)) + E_{\theta}[(T - g(S))^2] \geq V_{\theta}(g(S))
  \]

  donde se alcanza la igualdad si y solo si $T = g(S)$ casi seguramente.
\end{proof}

\begin{teorema}[Teorema de Lehmann-Schefeé]
  Si $S(X_1, \ldots, S_n)$ es un estadístico suficiente y completo para $\theta$ y $T = T(X_1, \ldots, X_n)$ es un estimador centrado para $\theta$ tal que $T = h(S)$, entonces $T$ es ECUMV para $\theta$
\end{teorema}

\begin{proof}
  $\begin{cases} S \text{ suficiente } \\ T \text{ centrado } \end{cases} \implies g(S) = E[T|S] \text{ es centrado para } \theta \text{ y } V_{\theta}(g(S)) \leq V_{\theta}(T)$ \\
  Además, se tiene que para cualquier otro estimador $T_1$ centrado para $\theta$, $g_1(S) = E[T_1 | S]$ es centrado para $\theta$ y $V_{\theta}(g_1(S)) \leq V_{\theta}(T_1)$ \\
  Por lo tanto al ser $S$ completo y $E_{\theta}[g(S) - g_1(S)] = \theta - \theta = 0$ se tiene que $g(S) \stackrel{c.s.}{=} g_1(S)$. En particular, para $T = h(S)$, $g_1(S) = E[h(S) | S] = h(S) = T$ y $V_{\theta}(T) \leq V_{\theta}(T_1)$, cualquiera que sea $T_1$ centrado para $\theta$
\end{proof}

\ejemplo{
  Sean  una m.a.s. de tamaño $n$ de una población con $X \sim Bin(1, \theta)$ y un estadístico $T = \sum_{i = 1}^{n} X_i$, comprueba que es suficiente y completo y además, si $h(T) = \bar{X}$, comprueba entonces que $h(T)$ es el ECUMV para $\theta$: \\
  Veamos primero que $T = \sum_{i = 1}^{n} X_i$ es suficiente y completo para $\theta$:
  $$X \sim Bin(1, \theta) \equiv Bernouilli(\theta) \implies f_{\theta}(x) = \theta^x(1 - \theta)^{1-x} \implies f_{\theta}(x_1, \ldots, x_n) = \theta^{\sum_{i = 1}^{n} x_i}(1 - \theta)^{n - \sum_{i = 1}^{n} x_i}$$
  Entones por el Teorema de Factorización de Fisher, tenemos que $T$ es suficiente para $\theta$. Veamos ahora su completitud: \\
  $$ X \sim Bin(1, \theta) \implies T \sim Bin(n, \theta) \implies$$
  Sea $g$ función real tal que: $E_{\theta}[g(T)] = 0, \forall \theta \in [0, 1]$, entonces:
  $$E_{\theta}[g(T)] = \sum_{t = 0}^{n} g(t) \binom{n}{t} \theta^t(1 - \theta)^{n-t} = 0, \forall \theta \in [0, 1] \iff$$
  $$\iff (1 - \theta)^n \cdot \sum_{i = 1}^{n} g(t) \binom{n}{t} \left(\frac{\theta}{1 - \theta}\right)^t = 0 \iff$$
  $$\iff \sum_{t = 0}^{n} g(t) \binom{n}{t} x^t = 0, \forall x \in \mathbb{R} \iff g(t) = 0, \forall t \in \{0, \ldots, n\} $$  ya que los coeficientes binomiales son no nulos. \\
  Por último queda ver que $S$ es un estadístico centrado para $\theta$, i.e. $E_{\theta}[h(S)] = \theta$ y $V_{\theta}(h(S)) < \infty$:
  $$E_{\theta}[h(T)] = E_{\theta}[\bar{X}] = \frac{1}{n} \cdot \sum_{i = 1}^{n} E_{\theta}[X_i] = \frac{1}{n} \cdot n\theta = \theta$$
  $$V_{\theta}(h(T)) = V_{\theta}(\bar{X}) = \frac{1}{n^2} \cdot \sum_{i = 1}^{n} V_{\theta}(X_i) = \frac{1}{n^2} \cdot n\theta(1 - \theta) = \frac{\theta(1 - \theta)}{n} < \infty$$
  Por lo tanto, $T = n\bar{X}$ es el ECUMV para $\theta$
}

\ejemplo{
  Sea una m.a.s. de tamaño $n$ de una población con distribución $Poisson(\theta)$ y dado un estadistico $d(\theta) = e^{-2\theta}$. Encuentra el ECUMV para $d(\theta)$:\\
}

\ejemplo{
  Sea una m.a.s. de tamaño $n$ de una población que sigue una distribución $Poisson(\theta)$, tenemos que encontrar el ECUMV para $\theta$: \\
  Si tomamos el estadístico $S = \sum_{i = 1}^{n} X_i$, para poder aplicar el Teorema de Lehmann-Scheffé necesitamos ver que el estadístico sea completo y suficiente: \\

  Veamos primero que $S$ es suficiente para $\theta$: \\ $$ X \sim
    Poisson(\theta) \implies f_{\theta}(x) = \frac{e^{-\theta}\theta^x}{x!}
    \implies f_{\theta}(x_1, \ldots, x_n) = \frac{e^{-n\theta}\theta^{\sum_{i =
          1}^{n} x_i}}{\prod_{i = 1}^{n} x_i!} $$

  Entonces, por el Teorema de Factorización de Fisher, tenemos que $S$ es
  suficiente para $\theta$. Veamos ahora su completitud: \\

  Siguiendo con lo anterior: $$ f_{\theta}(x_1, \ldots, x_n) =
    \frac{e^{-n\theta}\theta^{\sum_{i = 1}^{n} x_i}}{\prod_{i = 1}^{n} x_i!} =
    \frac{e^{-n\theta} e^{\ln(\theta) \sum x_i}}{\prod_{i = 1}^{n} x_i!} \implies
    \begin{cases}
      c(\theta)^n = e^{-n\theta}                        \\
      \prod_{i = 1}^{n} h(x_i) = \prod_{i = 1}^{n} x_i! \\
      q_1(\theta) = \ln(\theta)                         \\
      T_1(\vec{x}) = \sum_{i = 1}^{n} x_i
    \end{cases}
  $$
  Entonces, debemos ver que $ln(\theta)$ contiene un abierto  $(0, +\infty) \subset \mathbb{R}$, por lo tanto $s$ es completo para $\theta$.
  Además, tomando el estadístico $T = \frac{1}{n} \sum_{i = 1}^{n}x_i$ tenemos que:
  $$E[T] = \frac{1}{n} \sum_{i = 1}^{n} E[X_i] = \frac{1}{n} \cdot n\theta = \theta \implies \text{ T es centrado para } \theta \implies $$
  Tomando la función $h(x) = \frac{1}{n}\cdot x$ tenemos que: $$h(S) = \frac{1}{n} \sum_{i = 1}^{n} X_i = \frac{S}{n} \implies$$
  Por el Teorema de Lehmann-Scheffé, $T = h(S)$ es el ECUMV para $\theta$.
  \\
  Si en lugar de haber tomado $d(\theta) = \theta$ hubieramos querido el estimador centrado uniformemente de mínima varianza para $d(\theta) = e^{-\theta}$ HAY QUE INSERTAR LO DE DIEGO
}

\ejemplo{
  Sea una m.a.s. de tamaño $n$ de una población que sigue una distribución exponencial con parámetro $\theta$, queremos encontrar el estimador centrado uniformemente de mínima varianza para $d(\theta) = \theta$. \\
  $$X \sim Exponencial(\theta) \implies f(x|\theta) = \theta e^{-\theta x} \implies f(x_1, \ldots, x_n | \theta) = \theta^n e^{-\theta \sum_{i = 1}^{n} x_i}$$
  Por el Teorema de Factorización de Fisher, $S = \sum_{i = 1}^{n} X_i$ es suficiente para $\theta$. Veamos ahora su completitud: \\
  Como se sigue una distribución exponencial, podemos ver que pertenece a una familia exponencial uniparamétrica:
  $$f(x_1, \ldots, x_n | \theta) = \theta^n e^{-\theta \sum_{i = 1}^{n} x_i} \implies \begin{cases} c(\theta)^n = \theta^n \\ h(\vec{x}) = 1 \\ q_1(\theta) = -\theta \\ T_1(\vec{x}) = \sum_{i = 1}^{n} x_i \end{cases}$$
  Por lo que evidentemente en la imagen de la función $q_1(\theta) = -\theta$ tiene un abierto en su imagen por lo que el estadístico $S$ es completo para $\theta$. \\
  Por último, sabemos que $\bar{X} = \frac{S}{n} \implies E[\bar{X}] = \frac{1}{\theta} \implies \bar{X}$ es centrado para $\frac{1}{\theta}$ y por el Teorema de Lehmann-Scheffé, $\bar{X}$ es el ECUMV para $\frac{1}{\theta}$.\\
  Pero nosotros lo que queríamos es un estimador centrado para $\theta$. Por lo que puede parecer intuitivo pensar que el estadístico que podría estar centrado para $\theta$ es $\frac{1}{\bar{X}}$:
  $$E[\frac{1}{\bar{X}}] = n \cdot E[\frac{1}{S}] = n \cdot E[\frac{1}{\sum_{i = 1}^{n}X_i}] = n \cdot \frac{n-1}{\theta} \implies S' = \frac{n(n-1)}{\sum_{i = 1}^{n}X_i}$$
  Por lo que $S'$ es el ECUMV para $\theta$
}

\ejemplo{
  Sea una m.a.s. de tamaño $n$ de una población que sigue una distribución $Bernouilli(\theta)$, busquemos cual es el estimador centrado uniformemente de mínima varianza para $d_1(\theta) = \theta$ y para $d_2(\theta) = \theta(1 - \theta)$: \\
  $$X \sim Bernouilli(\theta) \implies f(x|\theta) = \theta^x(1 - \theta)^{1-x} \implies f(x_1, \ldots, x_n | \theta) = \theta^{\sum_{i = 1}^{n} x_i}(1 - \theta)^{n - \sum_{i = 1}^{n} x_i}$$
  Por el Teorema de Factorización de Fisher, $S = \sum_{i = 1}^{n} X_i$ es suficiente para $\theta$. Veamos ahora su completitud: \\
  $$f(x_1, \ldots, x_n | \theta) = \theta^{\sum_{i = 1}^{n} x_i}(1 - \theta)^{n - \sum_{i = 1}^{n} x_i} = (1 - \theta)^n \cdot e^{\sum x_i \cdot ln(\frac{\theta}{1 - \theta})} \implies \begin{cases}
      c(\theta)^n = (1 - \theta)^n                \\
      h(\vec{x}) = 1                              \\
      q_1(\theta) = ln(\frac{\theta}{1 - \theta}) \\
      T_1(\vec{x}) = \sum_{i = 1}^{n} x_i
    \end{cases}$$
  Por lo que evidentemente en la imagen de la función $q_1(\theta) = ln(\frac{\theta}{1 - \theta})$ tiene un abierto en su imagen por lo que el estadístico $S = \sum_{i = 1}^{n} X_i$ es completo para $\theta$.
  \begin{enumerate}
    \item Primero veamos el caso para $d_1(\theta) = \theta$: Sabemos que $E[S] = \sum_{i
              = 1}^{n}E[X_i] = n\theta \implies \bar{X} = \frac{S}{n} \implies E[\bar{X}] =
            \theta \implies \bar{X}$ es centrado para $\theta$. Por el Teorema de
          Lehmann-Scheffé, $\bar{X}$ es el ECUMV para $\theta$
    \item Ahora veamos el caso para $d_2(\theta) = \theta(1 - \theta) = \theta -
            \theta^2$: A pesar de que pueda parecer lógico tomar la varianza muestral $S^2$
          como estimador, no se cumplen las condiciones del Teorema de Lehmann-Scheffé
          para que sea un ECUMV, ya que tiene que depender únicamente de $S = \sum X_i$ y
          en $S^2$ aparecen términos cuadráticos, por lo que no es un ECUMV. Podemos
          intentar buscar otro:\\ NO LO ENTIENDO
  \end{enumerate}
}

\ejemplo{
  Sea una m.a.s. de tamao $n$ de una población que sigue una distribución $Normal(\mu, \sigma^2)$, veamos distintas casuísticas:
  \begin{enumerate}
    \item Tomemos uque $\mu$ es conocida y $\sigma^2$ no, por lo que queremos buscar el
          ECUMV para $\sigma^2$: $$X \sim Normal(\mu, \sigma^2) \implies f(x|\sigma^2) =
            \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x - \mu)^2}{2\sigma^2}} \implies f(x_1,
            \ldots, x_n | \sigma^2) =
            \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{\sum_{i = 1}^{n}(x_i -
              \mu)^2}{2\sigma^2}}$$ Por el Teorema de Factorización de Fisher, $S = \sum_{i =
              1}^{n} (X_i - \mu)^2$ es suficiente para $\sigma^2$. Veamos ahora su
          completitud: \\ Sabemos que la distribución normal, en este caso, pertenece a
          la familia exponencial uniparamétrica. Entonces sabemos que los estadísticos
          naturales asociados a las funciónes paramétricas $q_i(\theta)$ son completos,
          si éstas ultimas contienen un abierto en su imagen. En este caso,
          $q_1(\sigma^2) = \frac{1}{2\sigma^2}$ contiene un abierto en su imagen, por lo
          que $S$ es completo para $\sigma^2$. \\ Veamos la esperanza de $S$: $$E[S] =
            E[\sum_{i = 1}^{n} (X_i - \mu)^2] = \sum_{i = 1}^{n} E[(X_i^2 + \mu^2 -
                2X_i\mu)] = \sum_{i = 1}^{n} E[X_i^2] + \sum_{i = 1}^{n} \mu^2 - \sum_{i =
              1}^{n} 2\mu E[X_i] = $$ $$ = n(\sigma^2 + \mu^2) + n\mu^2 - 2n\mu^2 = n\sigma^2
            \implies \frac{S}{n} \text{ es el ECUMV para } \sigma^2 \text{ por el T. de
              Lehmann-Schefeé}$$
    \item Ahora supongamos que $\mu$ y $\sigma^2$ son desconocidos, por lo que queremos
          encontrar el ECUMV para $\mu$ y $\sigma^2$: $$f(x|\mu, \sigma^2) =
            \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x - \mu)^2}{2\sigma^2}} \implies f(x_1,
            \ldots, x_n | \mu, \sigma^2) =
            \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{\sum_{i = 1}^{n}(x_i -
              \mu)^2}{2\sigma^2}} = $$ $$ = f(x_1, \ldots, x_n | \mu, \sigma^2) =
            \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{\sum x_i^2 + n\mu^2 - \mu\sum
              x_i}{2\sigma^2}}$$ Por el Teorema de Factorización de Fisher, $T(\vec{x}) =
            (\sum x_i, \sum x_i^2)$ es suficiente para $(\mu, \sigma^2)$. Veamos ahora su
          completitud: \\ Sabemos que la distribución normal, en este caso, pertenece a
          la familia exponencial uniparamétrica. Entonces sabemos que los estadísticos
          naturales asociados a las funciónes paramétricas $q_i(\theta)$ son completos,
          si éstas ultimas contienen un abierto en su imagen. En este caso, $q_1(\mu,
            \sigma^2) = \mu$ y $q_2(\mu, \sigma^2) = \frac{1}{2\sigma^2}$ contienen un
          abierto en su imagen, por lo que $T$ es completo para $(\mu, \sigma^2)$. \\
          Veamos la esperanza de $T$: $$E[T] = E[(\sum X_i, \sum X_i^2)] = (\sum E[X_i],
            \sum E[X_i^2]) = (n\mu, n\sigma^2 + n\mu^2)$$ $$\implies \frac{\sum X_i}{n} =
            \bar{X} \text{ es el ECUMV para } \mu \text{ y } S^2 = \frac{1}{n-1}\sum_{i =
              1}^{n}(X_i - \bar{X})^2 \text{ es el ECUMV para } \sigma^2$$
    \item Ahora suponamos que $\mu$ es desconocido y $\sigma^2$ es conocido, por lo que
          queremos encontrar el ECUMV para $\mu$: Por lo visto en el apartado anterior,
          podemos que por el Teorema de Fisher, podemos ver que el estadítico $T =
            \sum_{i = 1}^{n} X_i$ es suficiente para $\mu$. \\También podríamos
          preguntarnos porqué no sería el estadístico $\vec{T} = (\sum X_i, \sum X_i^2)$,
          la respuesta está en que esta distribución normal pertenece a la familia
          exponencial uniparamétrica y por tanto tomamos la función dependiente del
          parámetro $q_1(\mu) = \mu$ y su estadístico natural asociado $T_1(\vec{x}) =
            \sum X_i$, el cual por las propiedades que vimos de las familias exponenciales
          es suficiente, y además completo. Veamos ahora si es insesgado: $$E[T] = E[\sum
                X_i] = \sum E[X_i] = n\mu \implies \bar{X} = \frac{T}{n} \text{ es el ECUMV
              para } \mu$$
  \end{enumerate}
}

\subsection{Cota para la varianza de un estimador}
\begin{definición}[Score]
  Sea $X$ variable aleatoria que sigue una distribución uniparamétrica continua o discreta y sea una m.a.s. de tamaño $n$ tales que $f_{\theta}(x_1, \ldots, x_n)$ es su función de densidad o masa. Entonces se define el \underline{score} como: 
  $$S_{\theta}(x_1, \ldots, x_n) = \frac{\partial}{\partial \theta} \log f_{\theta}(x_1, \ldots, x_n) = \frac{1}{f_{\theta}(x_1, \ldots, x_n)}\frac{\partial}{\partial \theta} f_{\theta}(x_1, \ldots, x_n) $$
\end{definición}

\begin{observación}
  El score mide cuánta información hay en una observación sobre $\theta$. Si el score es muy sensible a cambios sobre $\theta$ entonces hay una poco variación en $\theta$ hace que la verosimilitud cambie mucho y por tanto hay mucha información para estimar $\theta$. 
\end{observación}

\begin{definición}[Información de Fisher]
  Sea $X$ variable aleatoria que sigue una distribución uniparamétrica continua o discreta y sea una m.a.s. de tamaño $n$ tales que $f_{\theta}(x_1, \ldots, x_n)$ es su función de densidad o masa. Entonces se define la \underline{información de Fisher} como:
  $$I_n(\theta) = E_{\theta}\left[S_{\theta}(X_1, \ldots, X_n)^2\right] = E_{\theta}\left[\left(\frac{\partial}{\partial \theta} \log f_{\theta}(X_1, \ldots, X_n)\right)^2\right]$$
\end{definición}

Hemos visto hasta ahora el \underline{estimador centrado uniformemente de
  mínima varianza}, que cómo su nombre indica, es el que tiene varianza más
pequeña. \\ Si definieramos una varianza mínima para los estimadores de un
parámetro, es decir, si encontrásemos una cota para los estimadores, podríamos
encontrar más fácilmente un estimador que tenga dicha varianza y por tanto
seria el centrado uniformemente de mínima varianza.
\begin{definición}[Condiciones de regularidad de Wolfowitz]
Sea $X \approx\left(\chi, \beta_{\chi}, F_{\theta}\right)_{\theta \in \Theta \subset \mathbb{R}}$ modelo estadístico uniparamétrico contínuo (o discreto) y sea ( $X_{1}, \cdots X_{n}$ ) muestra de $\left\{F_{\theta}, \theta \in \Theta\right\}$ siendo $f_{\theta}\left(x_{1}, \cdots, x_{n}\right)$ su función de densidad (o de masa). Supongamos que se verifican las siguientes condiciones de regularidad:
\begin{enumerate}
  \item $\Theta$ es un intervalo abierto de $\mathbb{R}$
  \item $\operatorname{Sop}\left(f_{\theta}\right)=\left\{\left(x_{1}, \cdots, x_{n}\right) \in \chi^{n}: f_{\theta}\left(x_{1}, \cdots, x_{n}\right)>0\right\}$ no depende de $\theta$
  \item $\forall\left(x_{1}, \cdots, x_{n}\right) \in \chi^{n}$ y $\forall \theta \in \Theta, \exists \frac{\partial}{\partial \theta} f_{\theta}\left(x_{1}, \cdots, x_{n}\right)$
  \item $\int_{\chi^{n}} \frac{\partial}{\partial \theta} f_{\theta}\left(x_{1}, \cdots, x_{n}\right) d x_{1} \cdots d x_{n}=0$
  \item $I_{n}(\theta)=E\left[\left(\frac{\partial}{\partial \theta} \log f_{\theta}\left(x_{1}, \cdots, x_{n}\right)\right)^{2}\right]<\infty($ cantidad de información de Fisher)
\end{enumerate}
\end{definición}

\begin{teorema}[Cota de Fréchet-Cramér-Rao]
  Si $T=T\left(X_{1}, \cdots, X_{n}\right)$ es un estadístico unidimensional tal que $E_{\theta}\left[T^{2}\right]<\infty, \quad E_{\theta}[T]=d(\theta)$ y $$d^{\prime}(\theta)=\int_{\chi^{n}} T\left(x_{1}, \cdots, x_{n}\right) \frac{\partial}{\partial \theta} f_{\theta}\left(x_{1}, \cdots, x_{n}\right) d x_{1} \cdots d x_{n}$$ entonces $d^{\prime}(\theta)^{2} \leq V_{\theta}(T) I_{n}(\theta), \quad \forall \theta \in \Theta$, con igualdad si y solo si existe una función $k(\theta)$ tal que

  $$
    P_{\theta}\left(\left(x_{1}, \cdots, x_{n}\right) \in x^{n}: T\left(x_{1}, \cdots, x_{n}\right)=d(\theta)+k(\theta) \frac{\partial}{\partial \theta} f_{\theta}\left(x_{1}, \cdots, x_{n}\right)\right)=1, \forall \theta \in \theta
  $$
\end{teorema}

\begin{proof}
  $\exists d^{\prime}(\theta)$ puesto que
  $$
    \begin{gathered}
      d^{\prime}(\theta)=\int_{\chi^{n}} T\left(x_{1}, \cdots, x_{n}\right) \frac{\partial}{\partial \theta} \log \left(f_{\theta}\left(x_{1}, \cdots, x_{n}\right)\right) f_{\theta}\left(x_{1}, \cdots, x_{n}\right) d x_{1} \cdots d x_{n}=E_{\theta}\left[T \frac{\partial}{\partial \theta} \log f_{\theta}\right] \\
      \left|d^{\prime}(\theta)\right| \leq E_{\theta}\left[\left|T \frac{\partial}{\partial \theta} \log f_{\theta}\right|\right] \leq \sqrt{E_{\theta}\left[T^{2}\right] E_{\theta}\left[\left(\frac{\partial}{\partial \theta} \log f_{\theta}\right)^{2}\right]}<\infty \text { (desigualdad de Cauchy-Swartz) }
    \end{gathered}
  $$
  Además, $E_{\theta}\left[\frac{\partial}{\partial \theta} \log f_{\theta}\left(x_{1}, \cdots, x_{n}\right)\right]=0$ y por lo tanto,\\
  $V_{\theta}\left[\frac{\partial}{\partial \theta} \log f_{\theta}\left(x_{1}, \cdots, x_{n}\right)\right]=E_{\theta}\left[\left(\frac{\partial}{\partial \theta} \log f_{\theta}\left(x_{1}, \cdots, x_{n}\right)\right)^{2}\right]=I_{n}(\theta)$\\
  En efecto, $0=\int_{x^{n}} \frac{\partial}{\partial \theta} f_{\theta}\left(x_{1}, \cdots, x_{n}\right) d x_{1} \cdots d x_{n}=\int_{x^{n}} \frac{\partial}{\partial \theta} \log \left(f_{\theta}\left(x_{1}, \cdots, x_{n}\right)\right) f_{\theta}\left(x_{1}, \cdots, x_{n}\right) d x_{1} \cdots d x_{n}$

  $$
    =E_{\theta}\left[\frac{\partial}{\partial \theta} \log f_{\theta}\left(x_{1}, \cdots, x_{n}\right)\right]
  $$

  Entonces, $\operatorname{Cov}_{\theta}\left[T, \frac{\partial}{\partial \theta}
      \log f_{\theta}\right]=E\left[T \frac{\partial}{\partial \theta} \log
      f_{\theta}\right]=d^{\prime}(\theta)$, y como $\rho_{\theta}^{2}\left(T,
    \frac{\partial}{\partial \theta} \log
    f_{\theta}\right)=\frac{d^{\prime}(\theta)^{2}}{V_{\theta}(T)
      V_{\theta}\left(\frac{\partial}{\partial \theta} \log f_{\theta}\right)} \leq
    1$, se tiene que\\ $d^{\prime}(\theta)^{2} \leq V_{\theta}(T) I_{n}(\theta)$,
  con igualdad si y sólo si $\rho_{\theta}^{2}\left(T, \frac{\partial}{\partial
      \theta} \log f_{\theta}\right)=1$, es decir, si y sólo si $T \stackrel{\text {
        c.s. }}{=} a+b \frac{\partial}{\partial \theta} \log f_{\theta}$, es decir, si
  y sólo si existe una función $k(\theta)$ tal que
  $P_{\theta}\left(T=d(\theta)+k(\theta) \frac{\partial}{\partial \theta}
    f_{\theta}\right)=1$ En efecto, si $T \stackrel{\text { c.s. }}{=} a+b
    \frac{\partial}{\partial \theta} \log f_{\theta}$, entonces
  $d(\theta)=E_{\theta}[T]=a y$

  $$
    d^{\prime}(\theta)=E_{\theta}\left[T \frac{\partial}{\partial \theta} \log f_{\theta}\right]=E_{\theta}\left[a \frac{\partial}{\partial \theta} \log f_{\theta}+b\left(\frac{\partial}{\partial \theta} \log f_{\theta}\right)^{2}\right]=b I_{n}(\theta),
  $$

  y $b=\frac{d^{\prime}(\theta)}{l_{n}(\theta)}=k(\theta)$\\
\end{proof}


\ejemplo{
  Supongamos que tenemos un termómetro que mide la temperatura siguiendo una distribución normal con media $\mu$ y varianza $\sigma^2$. Tomemos que $\sigma^2 = 0.01$, de manera que tenemos que: 
  \begin{center}
    \begin{tabular}{ c c c }
     Medición & Valor \\ 
     1 & 21.0  \\  
     2 & 21.1  \\
     3 & 20.9
    \end{tabular}
    \end{center}
  Se puede ver que las mediciones están mu cerca entre ellas, por lo que podríamos confiar en la media de las observaciones. \\
  Veamos la información de Fisher: 
  $$S_{\mu}(x_1, \ldots, x_n) = \frac{\partial}{\partial \mu} \log f_{\mu}(x_1, \ldots, x_n) = \frac{\partial}{\partial\mu}\left( -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(x - \mu)^2}{2\sigma^2}\right) = \frac{x - \mu}{\sigma^2} \implies$$
  \begin{observación}
    Dado que se da que: 
    $$E_{\mu} \left[ \frac{x - \mu}{\sigma^2}\right] = \frac{1}{\sigma^2}E_{\mu}\left[x - \mu\right] = \frac{1}{\sigma^2}(\mu - \mu) = 0 \implies I_n(\mu) = Var(S_{\theta}(\vec{x})) = E_{\mu}\left[S_{\mu}(\vec{x})\right]$$
  \end{observación}
  $$I_n(\mu) = E_{\mu}\left[S_{\mu}(X_1, \ldots, X_n)^2\right] = E_{\mu}\left[\left(\frac{{X - \mu}}{{\sigma^2}}\right)^2\right] = Var(\frac{X - \mu}{\sigma^2}) =$$ $$= \frac{1}{\sigma^2} \cdot Var(X - \mu) = \frac{1}{\sigma^4} \cdot Var(X) = \frac{1}{\sigma^4} \cdot \sigma^2 = \frac{1}{\sigma^2} $$
  Entonces, la información de Fisher es:
  $$I_n(\mu) = \frac{1}{\sigma^2} = \frac{1}{0.01} = 100$$
  La cual es bastante alta lo que conlleva que podremos realiar una estimación mucho más precisa de $\mu$. Asimismo, también podemos deducir por la cota de Fréchet-Cramér-Rao que ningun estimador centrado o insesgado puede tener varianza menor que: 
  $$Var(T) \geq \frac{1}{I_n(\mu)} = \frac{1}{100} = 0.01$$
  Para poder estimar $\theta = \mu$ con precisión.
}

\begin{corolario}
Bajo las suposiciones anteriores, si T es un estadístico tal que
$E_{\theta}[T]=d(\theta)$ y $V_{\theta}(T)=\frac{d^{\prime}(\theta)^{2}}{l_{n} \theta)}$, entonces $T$ es ECUMV para d $(\theta)$
\end{corolario}

\begin{corolario}
Bajo las suposiciones anteriores, si ( $X_{1}, \cdots X_{n}$ ) es
m.a.s. (n) de $\left\{F_{\theta}, \theta \in \Theta\right\}$, entonces
$I_{n}(\theta)=n I_{1}(\theta)$ Indicación: $f_{\theta}\left(x_{1}, \cdots
  x_{n}\right)=\prod_{i=1}^{n} f_{\theta}\left(x_{i}\right)$
\end{corolario}

\begin{proposición}
Bajo las suposiciones anteriores, si la distribución de $X$ pertenece a la familia exponencial uniparamétrica, es decir, $f_{\theta}(x)=c(\theta) h(x) e^{q_{1}(\theta) T_{1}(x)}$, con $q_{1}^{\prime}(\theta)$ no nula, entonces el estadístico $\frac{1}{n} \sum_{i=1}^{n} T_{1}\left(X_{i}\right)$ alcanza la cota de FCR para $d(\theta)=-\frac{c^{\prime}(\theta)}{c(\theta) q_{1}^{\prime}(\theta)}$
\end{proposición}

\begin{proof}
  Sea una función de verosimilitud de la forma:
  \[
  f_{\theta}\left(x_{1}, \ldots, x_{n}\right)
  = c(\theta)^n \prod_{i=1}^{n} h(x_i) \cdot \exp\left( q_1(\theta) \sum_{i=1}^{n} T_1(x_i) \right)
  \]
  
  Entonces, la derivada del logaritmo de la verosimilitud respecto de \(\theta\) (el **score**) es:
  \[
  \frac{\partial}{\partial \theta} \log f_{\theta}
  = n \cdot \frac{c'(\theta)}{c(\theta)} + q_1'(\theta) \sum_{i=1}^{n} T_1(x_i)
  \]
  
  Dividiendo ambos lados entre \(n\), se obtiene:
  \[
  \frac{1}{n} \sum_{i=1}^{n} T_1(x_i)
  = -\frac{c'(\theta)}{c(\theta) q_1'(\theta)} + \frac{1}{n q_1'(\theta)} \cdot \frac{\partial}{\partial \theta} \log f_{\theta}
  \]
  
  Es decir:
  \[
  \frac{1}{n} \sum_{i=1}^{n} T_1(x_i)
  = a(\theta) + b(\theta) \cdot \frac{\partial}{\partial \theta} \log f_{\theta}
  \]
  donde:
  \[
  a(\theta) = -\frac{c'(\theta)}{c(\theta) q_1'(\theta)}, \quad
  b(\theta) = \frac{1}{n q_1'(\theta)}
  \]
  
  Como la esperanza del score es cero bajo condiciones regulares, se tiene:
  \[
  \mathbb{E}_\theta\left[ \frac{1}{n} \sum_{i=1}^{n} T_1(x_i) \right] = a(\theta)
  \]
  por lo que el estadístico \(\frac{1}{n} \sum_{i=1}^{n} T_1(x_i)\) es **centrado** para el parámetro:
  \[
  d(\theta) = -\frac{c'(\theta)}{c(\theta) q_1'(\theta)}
  \]
  
  Además, como está escrito como combinación lineal del score, alcanza la **cota de Cramér-Rao** bajo condiciones regulares.
  \end{proof}
  

\ejemplo{
  Si $X \sim Bin(1, \theta), T=\bar{X}$ alcanza la cota de FCR para $d(\theta)=\theta$
  Primero calculemos el score: 
  $$S_{\theta}(x_1, \ldots, x_n) = \frac{\partial}{\partial \theta} \log f_{\theta}(x_1, \ldots, x_n) = \frac{\partial}{\partial\theta}\left(ln\left(\theta^{\sum x_i} \cdot (1 - \theta)^{n - \sum x_i}\right)\right) = $$ $$ = \frac{\partial}{\partial\theta}\left((\sum x_i)\cdot ln(\theta) + (n - \sum x_i)\cdot ln(1 - \theta)\right) = \frac{\sum x_i}{\theta} + \frac{\sum x_i - n}{1 - \theta} $$

  $$ = \frac{\sum x_i - n\theta}{\theta(1 - \theta)} = \frac{n\bar{X} - n\theta}{\theta(1 - \theta)}$$
  Ahora calculemos la información de Fisher: 
  $$I_n(\theta) = E_{\theta}\left[S_{\theta}(X_1, \ldots, X_n)^2\right] = E_{\theta}\left[\left(\frac{n\bar{X} - n\theta}{\theta(1 - \theta)}\right)^2\right] = \frac{n^2}{\theta^2(1 - \theta)^2}E_{\theta}\left[(\bar{X} - \theta)^2\right] = $$

  $$ = \frac{n^2}{\theta^2(1 - \theta)^2} \cdot E_{\theta}\left[\bar{X}^2 + \theta^2 - 2\theta\bar{X}\right] = \frac{n^2}{\theta^2(1 - \theta)^2}\cdot\left(E_{\theta}[\bar{X}^2] + \theta^2 -2\theta E_{\theta}[\bar{X}]\right)= $$ $$= \frac{n^2}{\theta^2(1 - \theta)^2}\cdot(\frac{\sigma^2}{n} + \mu^2 + \theta^2 -2\theta\mu)$$
  
  Ahora sustituimos $\mu = \theta$ y $\sigma^2 = \theta(1 - \theta)$:
  $$ = \frac{n^2}{\theta^2(1 - \theta)^2}\cdot(\frac{\theta(1 - \theta)}{n} + \theta^2 + \theta^2 - 2\theta^2) = \frac{n^2}{\theta^2(1 - \theta)^2} \cdot \frac{\theta(1 - \theta)}{n} = \frac{n}{\theta(1 - \theta)}$$
  Entonces tenemos que la cota de FCR es:
  $$Var(T) \geq \frac{1}{I_n(\theta)} = \frac{\theta(1 - \theta)}{n}$$
  Por lo que el estimador $\bar{X}$ es el ECUMV para $d(\theta)=\theta$.
}

\ejemplo{
  Sea una población que sigue una distribución $Poisson(\theta)$ y una m.a.s. de tamaño $n$ de esa población. Se quiere estimar $\theta$ veamos cual debería ser la varianza del supuesto estimador: 
  Primero calculemos el score:
  $$S_{\theta}(x_1, \ldots, x_n) = \frac{\partial}{\partial\theta}ln(f_{\theta}(x_1, \ldots, x_n)) = \frac{\partial}{\partial\theta}\left(ln\left(\frac{e^{-n\theta} \cdot \theta^{\sum x_i}}{\prod x_i!}\right)\right) = $$ $$ = \frac{\partial}{\partial\theta}\left(-n\theta + (\sum x_i)ln(\theta) - \sum ln(x_i!)\right) = \frac{\sum x_i}{\theta} - n = \frac{n\bar{X}}{\theta} - n$$
  Ahora veamos la información de Fisher:
  $$I_{n}(\theta) = E_{\theta}\left[S_{\theta}(X_1, \ldots, X_n)^2\right] = E_{\theta}\left[\left(\frac{n\bar{X}}{\theta} - n\right)^2\right] = $$
  $$ = E_{\theta}\left[n^2\left(\frac{\bar{X}^2}{\theta^2} + 1 - \frac{2\bar{X}}{\theta}\right)\right] = n^2\left(\frac{E_{\theta}[\bar{X}^2]}{\theta^2} + 1 - \frac{2E_{\theta}[\bar{X}]}{\theta}\right) = n^2\left(\frac{\mu^2 + \frac{\sigma^2}{n}}{\theta^2} + 1 - \frac{2\mu}{\theta}\right) = $$
  Sustituyendo $\mu = \theta$ y $\sigma^2 = \theta$:
  $$ = n^2\left(\frac{\theta^2}{\theta^2} + \frac{1}{n\theta} + 1 \frac{\theta}{\theta}\right) = n^2\left(\frac{1}{n\theta}\right) = \frac{n}{\theta}$$

  Por tanto la cota de FCR es: $$Var(T) \geq \frac{\theta}{n}$$ Luego el estimador $\bar{X}$ coincide con la varianza y por tanto es el ECUMV para $d(\theta)=\theta$.

  Asimismo, existe una forma alternativa de calcular la información de Fisher, y es a través de la fórmula: 
  $$I_n(\theta) = -E_{\theta}\left[\frac{\partial^2}{\partial \theta^2} \log f_{\theta}(X_1, \ldots, X_n)\right]$$
  En nuestro caso sería: 
  $$I_n(\theta) = -E_{\theta}\left[\frac{\partial^2}{\partial \theta^2} \log f_{\theta}(X_1, \ldots, X_n)\right] = -E_{\theta}\left[\frac{\partial}{\partial\theta}\left(\frac{n\bar{X}}{\theta} - n\right)\right] = -E_{\theta}\left[\frac{- n\bar{X}}{\theta^2}\right]  = $$
  $$ = \frac{n}{\theta^2}E_{\theta}\left[\bar{X}\right] = \frac{n}{\theta^2}\cdot\theta = \frac{n}{\theta}$$
  Por lo que la información de Fisher coincide con la que hemos calculado anteriormente.
}

\begin{definición} [Estimador Eficiente]
Diremos que un estimador es eficiente para $d(\theta)$ si es centrado para $d(\theta)$ y su varianza alcanza la cota de FCR\\
En general, se llama eficiencia de un estimador centrado de $d(\theta)$ a

$$
  e f(T, d(\theta))=\frac{d^{\prime}(\theta)^{2}}{I_{n}(\theta) V_{\theta}(T)} \leq 1
$$
\end{definición}

\ejemplo{
  Sea $X \sim Poisson(\theta)$ y $T=\bar{X}$, entonces $T$ es un estimador eficiente para $\theta$\\
  Primero calculemos la varianza de $T$: 
  $$V_{\theta}(T) = E_{\theta}\left[\bar{X}^2\right] - E_{\theta}\left[\bar{X}\right]^2 = \frac{\sigma^2}{n} + \mu^2 - \mu^2 = \frac{\sigma^2}{n} = \frac{\theta}{n}$$
  Por lo que la eficiencia de $T$ es: 
  $$ef(T, d(\theta)) = \frac{d^{\prime}(\theta)^{2}}{I_{n}(\theta) V_{\theta}(T)} = \frac{\theta^2}{\frac{n}{\theta} \cdot \frac{\theta}{n}} = 1$$
  Por lo que el estimador $\bar{X}$ es eficiente para $\theta$. 
}

\subsection{Métodos de construcción de estimadores}
\subsubsection{Método de los momentos}
\begin{definición}[Método de los momentos]
  Sea $X$ una variable aleatoria que sigue una distribución uniparamétrica continua o discreta y sea una m.a.s. de tamaño $n$ tales que $f_{\theta}(x_1, \ldots, x_n)$ es su función de densidad o masa. Entonces se define el \underline{método de los momentos} como el método que consiste en igualar los momentos muestrales a los momentos poblacionales.
  \begin{enumerate}
    \item Si $E\left[X^{k}\right]$ es el momento poblacional respecto al origen de orden $k$, entonces el estimador por el método de los momentos del momento poblacional respecto al origen de orden $k$ es:
    $$\hat{\theta}_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}$$
    \item Si $E\left[\left(X-\alpha_{1}\right)^{k}\right]$ es el momento poblacional respecto a la media de orden $k$, entonces el estimador por el método de los momentos del momento poblacional respecto a la media de orden $k$ es:
    $$\hat{\theta}_{k}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{k}$$
  \end{enumerate}
\end{definición}

\ejemplo{
  Supongamos que \(X \sim \text{Gamma}(a, p)\), donde \(a > 0\) es el parámetro de forma y \(p > 0\) es el parámetro de escala. La función de densidad es:

  \[
  f(x) = \frac{1}{\Gamma(a)p^a} x^{a-1} e^{-x/p}, \quad x > 0
  \]
  
  Sean \(X_1, X_2, \dots, X_n\) una muestra aleatoria simple de esta distribución. Aplicamos el método de los momentos, que consiste en igualar los momentos teóricos con los empíricos.
  
  \subsection*{Momentos teóricos}
  
  Sabemos que:
  
  \[
  \mathbb{E}[X] = a p, \qquad \text{Var}(X) = a p^2
  \]
  
  \subsection*{Momentos muestrales}
  
  Calculamos los momentos muestrales:
  
  \[
  \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i, \qquad S^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
  \]
  
  \subsection*{Igualación de momentos}
  
  Igualamos los momentos teóricos y muestrales:
  
  \[
  \begin{cases}
  \bar{X} = a p \\
  S^2 = a p^2
  \end{cases}
  \]
  
  Despejamos \(a\) y \(p\):
  
  De la primera ecuación:
  
  \[
  a = \frac{\bar{X}}{p}
  \]
  
  Sustituimos en la segunda:
  
  \[
  S^2 = \left( \frac{\bar{X}}{p} \right) p^2 = \bar{X} p \Rightarrow p = \frac{S^2}{\bar{X}}
  \]
  
  Luego, sustituimos en la expresión de \(a\):
  
  \[
  a = \frac{\bar{X}}{p} = \frac{\bar{X}^2}{S^2}
  \]
  
  \subsection*{Conclusión}
  
  El estimador de \(\theta = (a, p)\) por el método de los momentos es:
  
  \[
  \boxed{
  \hat{a} = \frac{\bar{X}^2}{S^2}, \quad \hat{p} = \frac{S^2}{\bar{X}}
  }
  \]

}

\ejemplo{
  Para una m.a.s. de tamaño $n$ de una poblacion de $Bernouilli(\theta)$ en este caso $r = 1$, $\alpha_1(\theta) = \theta$ y $a_1 = \bar{X}$ por lo tanto el estimador $\bar{X}$ es el estimador de los momentos para $\theta$.
}

\ejemplo{
  Para una población con distribución de $Poisson(\theta)$ se necesita una sola ecuación, $\alpha_1(\theta) = \theta$ y $a_1 = \bar{X}$ por lo tanto el estimador $\bar{X}$ es el estimador de los momentos para $\theta$.

}

\ejemplo{
  Para una población $X$ con distribución $N(\mu, \sigma)$ con ambos parámetros desconocidos $\vec{\theta} = (\mu, \sigma)$ es $r = 2$ y se debe resolver el sistema:
  $$\begin{cases}
    \mu = \bar{X} \\
    \sigma^2 + \mu^2 = \frac{1}{n} \sum_{i=1}^{n} X_i^2
  \end{cases}$$
  Lo que lleva a que los estimadores por el método de los momentos son: $\bar{X}$ y $\sqrt{b_2}$
}

\subsubsection{Método de máxima verosimilitud}

\begin{definición}[Estimador de máxima verosimilitud]
  Dada $C(X_1, \ldots, X_n) \subset \Theta$ región aleatoria del espacio paramétrico tal que $P_{\theta}\{\theta \in C(X_1, \ldots, X_n)\} \geq 1 - \alpha \forall \theta \in \Theta$, entonces para cada posible valor $(x_1, \ldots, x_n) \in \chi^n$, $C(x_1, \ldots, x_n)$ se denomina región de confianza para $\theta$ de nivel $1 - \alpha$
\end{definición}

\textbf{Método de máxima verosimilitud}\\
Supongamos que una urna contiene 6 bolas entre blancas y negras, no todas del mismo color, pero se ignora cuantas hay de cada uno. Para tratar de adivinar la composición de la urna se permiten dos extracciones con reemplazamiento de la misma y resultó que ninguna de ellas fue blanca. Dar una estimación de la probabilidad $\theta$ de que una bola extraída aleatoriamente de dicha urna sea blanca

$$
  \theta \in \Theta=\left\{\frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}, \frac{5}{6}\right\}
$$

$T=X_{1}+X_{2} \equiv \mathrm{n}^{\circ}$ de blancas en las dos extracciones C.R. de la urna $\sim \operatorname{Bin}(2, \theta)$ y $f_{\theta}(t)=\binom{2}{t} \theta^{t}(1-\theta)^{2-t}, t=0,1,2$

\begin{center}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    $\theta$                       & $1 / 6$ & $2 / 6$ & $3 / 6$ & $4 / 6$ & $5 / 6$ \\
    \hline
    $f_{\theta}(0)=(1-\theta)^{2}$ & 0.694   & 0.444   & 0.25    & 0.111   & 0.027   \\
    \hline
  \end{tabular}
\end{center}

Por lo tanto, la estimación $\hat{\theta}(0)=\frac{1}{6}$ y el estimador

$$
  \hat{\theta}=\hat{\theta}(T)=\left\{\begin{array}{lll}
    1 / 6 & \text { si } & T=0 \\
    1 / 2 & \text { si } & T=1 \\
    5 / 6 & \text { si } & T=2
  \end{array}\right.
$$

es el estimador de máxima verosimilitud (EMV)

Sea $\left(X_{1}, \cdots X_{n}\right)$ una muestra con $f_{\theta}\left(x_{1},
  \cdots, x_{n}\right)=f\left(x_{1}, \cdots, x_{n} \mid \theta\right)$ función de
densidad (o de masa), $\theta \in \Theta \subset \mathbb{R}^{\ell}$ Denotemos
por $L\left(\theta \mid x_{1}, \cdots, x_{n}\right)=f\left(x_{1}, \cdots, x_{n}
  \mid \theta\right)$ a la función de verosimilitud de la muestra Un estimador
$\hat{\theta}=\hat{\theta}\left(X_{1}, \cdots, X_{n}\right)$ se denomina
estimador de máxima verosimilitud (EMV) de $\theta$, sí y sólo sí\\ (1)
$\hat{\theta}\left(x_{1}, \cdots, x_{n}\right) \in \Theta, \forall\left(x_{1},
  \cdots, x_{n}\right) \in \chi^{n}$\\ (2) $L\left(\hat{\theta} \mid x_{1},
  \cdots, x_{n}\right)=\sup _{\theta \in \Theta} L\left(\theta \mid x_{1},
  \cdots, x_{n}\right), \forall\left(x_{1}, \cdots, x_{n}\right) \in \chi^{n}$\\
o equivalentemente, sí y sólo sí\\ (1) $\hat{\theta}\left(x_{1}, \cdots,
  x_{n}\right) \in \Theta, \forall\left(x_{1}, \cdots, x_{n}\right) \in
  \chi^{n}$\\ (2) $\log L\left(\hat{\theta} \mid x_{1}, \cdots, x_{n}\right)=\sup
  _{\theta \in \Theta} \log L\left(\theta \mid x_{1}, \cdots, x_{n}\right),
  \forall\left(x_{1}, \cdots, x_{n}\right) \in \chi^{n}$

Si $f_{\theta}$ es una función derivable respecto a $\theta$ en el interior del
espacio paramétrico $\Theta$, la forma usual de determinar el estimador de
máxima verosimilitud es examinar primero los máximos relativos de $f_{\theta}$,
para compararlos después, con los valores sobre la frontera de $\Theta$. Ello
conduce a resolver las ecuaciones de verosimilitud:

$$
  \frac{\partial}{\partial \theta_{j}} \log L\left(\theta \mid x_{1}, \cdots, x_{n}\right)=0, j=1, \cdots, \ell
$$

(en el supuesto de que $\theta=\left(\theta_{1}, \cdots, \theta_{\ell}\right)$ sea un parámetro $\ell$-dimensional), seleccionando las soluciones correspondientes a un máximo de $f_{\theta}$, es decir aquellas en las que la matriz hessiana $H=\left(\frac{\partial}{\partial \theta_{i}} \frac{\partial}{\partial \theta_{j}} \log L\left(\theta \mid x_{1}, \cdots, x_{n}\right)\right)_{i, j=1, \cdots, \ell}$ sea definida negativa.\\

\begin{observación}
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\begin{enumerate}
  \item  EI EMV $\hat{\theta}$ no tiene por qué existir
  \item El EMV $\hat{\theta}$ no tiene por qué ser único
  \item El EMV $\hat{\theta}$ no tiene por qué ser centrado
  \item El EMV $\hat{\theta}$ no tiene por qué ser suficiente, pero si $S=S\left(X_{1},
          \cdots, X_{n}\right)$ es suficiente para $\theta$, entonces
        $\hat{\theta}=\hat{\theta}(S)$
  \item Invariancia: Si $\hat{\theta}$ es el EMV de $\theta$, entonces
        $h(\hat{\theta}s)$ es el EMV de $h(\theta)$
  \item Bajo ciertas condiciones de regularidad, si $\left(X_{1}, \cdots X_{n}\right)$
        es m.a.s. $(\mathrm{n})$ y $\theta \in \mathbb{R}$, entonces
        $\sqrt{n}(\hat{\theta}-\theta) \underset{n \rightarrow \infty}{d} N\left(0,
          \frac{1}{\sqrt{l_{1}(\theta)}}\right)$ y por lo tanto, $\hat{\theta}$ es
        asintóticamente insesgado para $\theta$ y asintóticamente eficiente
\end{enumerate}
\end{observación}

\ejemplo{
  Si $X \sim \operatorname{Bin}(1, \theta), \hat{\theta}=\bar{X}$ es el EMV para $\theta \in[0,1]$ basado en una m.a.s.(n)
}

\ejemplo{
  Si $X \sim N(\mu, \sigma), \hat{\theta}=\left(\bar{X}, \sigma_{n}\right)=\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}, \sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}\right)$ es el EMV para $\theta=(\mu, \sigma) \in \mathbb{R} \times \mathbb{R}^{+}$basado en una m.a.s. $(\mathrm{n})$. Además, si $n=1$, no existe el EMV para $\sigma$
}

\ejemplo{
Si $X \sim U(0, \theta), \hat{\theta}=X_{(n)}$ es el EMV para $\theta>0$ basado en una m.a.s. $(\mathrm{n})$ y $E_{\theta}\left[X_{(n)}\right]=\frac{n}{n+1} \theta$
}

\ejemplo{
Si $X \sim U\left(\theta-\frac{1}{2}, \theta+\theta-\frac{1}{2}\right)$, entonces cualquier valor en el intervalo $\left(X_{(n)}-\frac{1}{2}, X_{(1)}+\frac{1}{2}\right)$ es un EMV para $\theta$. En particular, $\frac{X_{(1)}+X_{(n)}}{2}$ no es suficiente y es EMV para $\theta$
}

\textbf{Propiedades del EMV}

\begin{proposición}
Si $S=S\left(X_{1}, \cdots, X_{n}\right)$ es suficiente para $\theta$, entonces el EMV $\hat{\theta}=\hat{\theta}(S)$ es función de $S$
\end{proposición}

\begin{proof}
  $f_{\theta}\left(x_{1}, \ldots, x_{n}\right)=h\left(x_{1}, \ldots, x_{n}\right) g_{\theta}\left(S\left(x_{1}, \ldots, x_{n}\right)\right)$ (teorema de factorización)
\end{proof}

\begin{proposición}
Si $\hat{\theta}$ es el EMV de $\theta$ y $h: \Theta \longrightarrow \Lambda$, entonces $\hat{\lambda}=h(\hat{\theta})$ es el EMV de $\lambda=h(\theta)$ respecto de la función de verosimilitud inducida,

$$
  \begin{gathered}
    M\left(\lambda \mid x_{1}, \ldots, x_{n}\right)=\operatorname{Sup}_{\theta \in \Theta_{\lambda}} L\left(\theta \mid x_{1}, \ldots, x_{n}\right) \\
    \Theta_{\lambda}=\{\theta \in \Theta: h(\theta)=\lambda\}
  \end{gathered}
$$
\end{proposición}

\begin{proof}
  $$
    \begin{aligned}
       & \operatorname{Sup}_{\lambda \in \Lambda} M\left(\lambda \mid x_{1}, \ldots, x_{n}\right)=\operatorname{Sup}_{\lambda \in \Lambda} \operatorname{Sup}_{\theta \in \Theta_{\lambda}} L\left(\theta \mid x_{1}, \ldots, x_{n}\right)=\operatorname{Sup}_{\theta \in \Theta} L\left(\theta \mid x_{1}, \ldots, x_{n}\right) \\
       & =L\left(\hat{\theta} \mid x_{1}, \ldots, x_{n}\right)
    \end{aligned}
  $$

  Finalmente, como $\hat{\theta} \in \Theta_{\hat{\lambda}}$, se tiene que\\
  $M\left(\hat{\lambda} \mid x_{1}, \ldots,
    x_{n}\right)=\operatorname{Sup}_{\theta \in \Theta_{\hat{\lambda}}}
    L\left(\theta \mid x_{1}, \ldots, x_{n}\right) \geq L\left(\hat{\theta} \mid
    x_{1}, \ldots, x_{n}\right)$, y por lo\\ tanto $\operatorname{Sup}_{\lambda \in
      \Lambda} M\left(\lambda \mid x_{1}, \ldots, x_{n}\right)=M\left(\hat{\lambda}
    \mid x_{1}, \ldots, x_{n}\right)$

\end{proof}

\subsection{Propiedades asintóticas de los estimadores de máxima verosimilitud}

\begin{proposición} [Desigualdad de Yelsin]
Sea $X$ una v.a. integrable y $g$ una función convexa tal que $g(X)$ es integrable. Entonces $g(E[X]) \leq E[g(X)]$ Además, si $g$ es estrictamente convexa entonces $g(E[X])<E[g(X)]$
\end{proposición}

Consideremos $X \approx\left(\chi, \beta_{\chi}, F_{\theta}\right)_{\theta \in
  \Theta \subset \mathbb{R}}$ modelo estadístico uniparamétrico contínuo (o
discreto) y sea $\left(X_{1}, \cdots X_{n}\right)$ muestra de
$\left\{F_{\theta}, \theta \in \Theta\right\}$ siendo $f_{\theta}\left(x_{1},
  \cdots, x_{n}\right)$ su función de densidad (o de masa). Supongamos que se
verifican las condiciones de regularidad

Denotemos por $\theta_{0}$ el verdadero valor del parámetro (desconocido).

\begin{lema}
  (1) $E_{\theta_{0}}\left[\log f_{\theta}\left(x_{1}, \ldots, x_{n}\right)\right]<E_{\theta_{0}}\left[\log f_{\theta_{0}}\left(x_{1}, \ldots, x_{n}\right)\right], \forall \theta \neq \theta_{0}$\\
  (2) Existe un entorno de $\theta_{0}, E\left(\theta_{0}\right)$, donde la función de $\theta$ $E_{\theta_{0}}\left[\frac{\partial}{\partial \theta} \log f_{\theta}\left(x_{1}, \ldots, x_{n}\right)\right]=J\left(\theta_{0}, \theta\right)$ es estrictamente decreciente
\end{lema}

\begin{proof}
  \begin{enumerate}
    \item Como el logaritmo es una función estrictamente cóncava y el soporte de la
          distribución es independiente del parámetro, si $\theta \neq \theta_{0},
            E_{\theta_{0}}\left[\log \frac{f_{\theta}\left(x_{1}, \ldots,
                x_{n}\right)}{f_{\theta_{0}}\left(x_{1}, \ldots, x_{n}\right)}\right]<\log
            E_{\theta_{0}}\left[\frac{f_{\theta}\left(x_{1}, \ldots,
                x_{n}\right)}{f_{\theta_{0}}\left(x_{1}, \ldots, x_{n}\right]}\right]=0$. Por
          lo tanto,\\ $E_{\theta_{0}}\left[\log f_{\theta}\left(x_{1}, \ldots,
              x_{n}\right)\right]-E_{\theta_{0}}\left[\log f_{\theta_{0}}\left(x_{1}, \ldots,
              x_{n}\right)\right]<0, y$\\ $E_{\theta_{0}}\left[\log f_{\theta}\left(x_{1},
              \ldots, x_{n}\right)\right]$ presenta un máximo extricto en $\theta_{0}$
    \item Como $f_{\theta}\left(x_{1}, \ldots, x_{n}\right)$ es derivable respecto a
          $\theta$, $\frac{\partial}{\partial \theta} E_{\theta_{0}}\left[\log
              f_{\theta}\left(x_{1}, \ldots, x_{n}\right)\right]=0$ en $\theta=\theta_{0}
            \Rightarrow J\left(\theta_{0}, \theta_{0}\right)=0$
          $\frac{\partial^{2}}{\partial \theta^{2}} E_{\theta_{0}}\left[\log
              f_{\theta}\left(x_{1}, \ldots, x_{n}\right)\right]<0$ en $\theta=\theta_{0}
            \Rightarrow \frac{\partial}{\partial \theta} J\left(\theta_{0},
            \theta\right)<0$ en $\theta=\theta_{0}$ Por lo tanto, existe un entorno de
          $\theta_{0}, E\left(\theta_{0}\right)$, donde la función $J\left(\theta_{0},
            \theta\right)$ es estrictamente decreciente
  \end{enumerate}
\end{proof}

\begin{teorema} [Primer teorema de convergencia del EMV (consistencia)]
  Para una m.a.s. $(n)$, existe una sucesión de v.a. $\hat{\theta}_{n}$, para cada $n$ solución de la ecuación de verosimilitud, $\frac{\partial}{\partial \theta} \log L\left(\theta \mid x_{1}, \ldots, x_{n}\right)=0$, que converge c.s. al verdadero valor del parámetro $\theta_{0}$.
\end{teorema}

\begin{proof}
  Para una m.a.s. $(n), f_{\theta}\left(x_{1}, \ldots, x_{n}\right)=\prod_{i=i}^{n} f_{\theta}\left(x_{i}\right)$\\
  Entonces, $\frac{\partial}{\partial \theta} \log L\left(\theta \mid x_{1}, \ldots, x_{n}\right)=\sum_{i=1}^{n} \frac{\partial}{\partial \theta} \log f_{\theta}\left(x_{i}\right)=n z_{n}(\theta)$\\
  Además, $z_{n}(\theta) \xrightarrow[n \rightarrow \infty]{\text { c.s. }} E_{\theta_{0}}\left[\frac{\partial}{\partial \theta} \log f_{\theta}(x)\right]=J\left(\theta_{0}, \theta\right)$ que es estrictamente decreciente en el entorno $E\left(\theta_{0}\right)$ y $J\left(\theta_{0}, \theta_{0}\right)=0$\\
  Por lo tanto, podemos elegir $\varepsilon>0$ tal que $\left[\theta_{0}-\varepsilon, \theta_{0}+\varepsilon\right] \subset E\left(\theta_{0}\right)$ y $J\left(\theta_{0}-\varepsilon, \theta_{0}\right)>0, J\left(\theta_{0}+\varepsilon, \theta_{0}\right)<0$\\
  Entonces,\\
  $z_{n}\left(\theta_{0}-\varepsilon\right) \xrightarrow[n \rightarrow \infty]{\text { c.s. }} J\left(\theta_{0}, \theta_{0}-\varepsilon\right)>0$ y $z_{n}\left(\theta_{0}+\varepsilon\right) \xrightarrow[n \rightarrow \infty]{\text { c.s. }} J\left(\theta_{0}, \theta_{0}+\varepsilon\right)<0$\\
  Como $\frac{\partial}{\partial \theta} f_{\theta}$ es continua, $\exists \hat{\theta}_{n} \in\left(\theta_{0}-\varepsilon, \theta_{0}+\varepsilon\right)$ tal que $z_{n}\left(\hat{\theta}_{n}\right)=0$. Por lo tanto, $\hat{\theta}_{n}$ es una solución de la ecuación de verosimilitud tal que $\hat{\theta}_{n} \xrightarrow[n \rightarrow \infty]{\text { c.s. }} \theta_{0}$
\end{proof}

\begin{observación}
El teorema anterior es útil, sólo cuando existe una única raíz de la ecuación de verosimilitud, ya que entonces esa raíz ha de ser el estimador consistente. En caso contrario, no se sabrá, cual de las raíces al variar n, va a dar lugar a la sucesión consistente\\
Además, bajo ciertas condiciones de regularidad,\\
$Y_{n}(\theta)=\frac{1}{n} \sum_{i=1}^{n} \frac{\partial^{2}}{\partial \theta^{2}} \log f_{\theta}\left(x_{i}\right) \xrightarrow[n \rightarrow \infty]{\text { c.s. }} E_{\theta_{0}}\left[\frac{\partial^{2}}{\partial \theta^{2}} \log f_{\theta}(x)\right]=I\left(\theta_{0}, \theta\right)$, con $I\left(\theta_{0}, \theta_{0}\right)=-I_{1}\left(\theta_{0}\right)<0$\\
Como $\hat{\theta}_{n} \xrightarrow[n \rightarrow \infty]{\text { c.s. }} \theta_{0}$, si $\frac{\partial^{2}}{\partial \theta^{2}} f_{\theta}(x)$ es continua en $\theta$ uniformemente $\forall x$, entonces $Y_{n}\left(\hat{\theta}_{n}\right)-Y_{n}\left(\theta_{0}\right) \xrightarrow[n \rightarrow \infty]{\text { c.s. }} 0$, y por lo tanto $Y_{n}\left(\hat{\theta}_{n}\right) \xrightarrow[n \rightarrow \infty]{\text { c.s. }}-I_{1}\left(\theta_{0}\right)<0$ y a partir de un $n$ en adelante la sucesión de v.a. solución de la ecuación de verosimilitud son una sucesión de máximos
\end{observación}

\begin{teorema} [Segundo teorema de convergencia del EMV (convergencia en ley)]
  Si se cumplen las condiciones de regularidad y $\frac{\partial^{2}}{\partial \theta^{2}} f_{\theta}(x)$ es continua en $\theta$ uniformemente $\forall x$, entonces

  $$
    \sqrt{n}\left(\hat{\theta}_{n}-\theta\right) \underset{n \rightarrow \infty}{\stackrel{d}{\longrightarrow}} N\left(0, \frac{1}{\sqrt{I_{1}(\theta)}}\right)
  $$

\end{teorema}

\begin{proof}
  Si denotamos por $\varphi(x, \theta)=\frac{\partial}{\partial \theta} \log f_{\theta}(x)$, entonces la ecuación de verosimilitud\\
  $\frac{\partial}{\partial \theta} \log L\left(\theta \mid x_{1}, \ldots, x_{n}\right)=\sum_{i=1}^{n} \frac{\partial}{\partial \theta} \log f_{\theta}\left(x_{i}\right)=\sum_{i=1}^{n} \varphi\left(x_{i}, \theta\right)=n z_{n}(\theta)=$ 0 en $\hat{\theta}_{n}$, con $\hat{\theta}_{n} \xrightarrow[n \rightarrow \infty]{\text { c.s. }} \theta_{0}$, y por lo tanto $\sum_{i=1}^{n} \varphi\left(x_{i}, \hat{\theta}_{n}\right)=0$

  $\sum_{i=1}^{n} \varphi\left(x_{i}, \theta_{0}\right)=\sum_{i=1}^{n} \varphi\left(x_{i}, \hat{\theta}_{n}\right)+\left(\theta_{0}-\hat{\theta}_{n}\right) \sum_{i=1}^{n} \frac{\partial}{\partial \theta} \varphi\left(x_{i}, \theta^{*}\right)$ $\operatorname{con} \theta^{*} \in\left(\theta_{0}, \hat{\theta}_{n}\right)$\\
  Entonces, $\hat{\theta}_{n}-\theta_{0}=\frac{\sum_{i=1}^{n} \varphi\left(x_{i}, \theta_{0}\right)}{-\sum_{i=1}^{n} \frac{\partial}{\partial \theta} \varphi\left(x_{i}, \theta^{*}\right)}$\\
  $\sqrt{n}\left(\hat{\theta}_{n}-\theta_{0}\right)=\frac{\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \varphi\left(x_{i}, \theta_{0}\right)}{-\frac{1}{n} \sum_{i=1}^{n} \frac{\partial}{\partial \theta} \varphi\left(x_{i}, \theta^{*}\right)}=\frac{A_{n}}{B_{n}}$\\
  $E_{\theta_{0}}\left[A_{n}\right]=\frac{1}{\sqrt{n}} E_{\theta_{0}}\left[\sum_{i=1}^{n} \varphi\left(x_{i}, \theta_{0}\right)\right]=\frac{1}{\sqrt{n}} n E_{\theta_{0}}\left[\varphi\left(x, \theta_{0}\right)\right]=0$\\
  $V_{\theta_{0}}\left(A_{n}\right)=\frac{1}{n} V_{\theta_{0}}\left(\sum_{i=1}^{n} \varphi\left(x_{i}, \theta_{0}\right)\right)=\frac{1}{n} n V_{\theta_{0}}\left(\varphi\left(x, \theta_{0}\right)\right)$\\
  $=E_{\theta_{0}}\left[\varphi\left(x, \theta_{0}\right)^{2}\right]=I_{1}\left(\theta_{0}\right)$. Por lo tanto, $A_{n} \xrightarrow[n \rightarrow \infty]{d} N\left(0, \sqrt{I_{1}\left(\theta_{0}\right)}\right)$\\
  Además, $-B_{n}-E_{\theta_{0}}\left[\frac{\partial}{\partial \theta} \varphi\left(x, \theta^{*}\right)\right] \xrightarrow[n \rightarrow \infty]{\text { c.s. }} 0$ y como $\hat{\theta}_{n} \xrightarrow[n \rightarrow \infty]{\text { c.s. }} \theta_{0}$ y $\frac{\partial^{2}}{\partial \theta^{2}} f_{\theta}(x)$ es\\
  continua en $\theta$ uniformemente $\forall x$, se tiene que $-B_{n}+I_{1}\left(\theta_{0}\right) \xrightarrow[n \rightarrow \infty]{\text { c.s. }} 0$ Por lo tanto, $\frac{A_{n}}{B_{n}} \xrightarrow[n \rightarrow \infty]{d} \frac{1}{I_{1}\left(\theta_{0}\right)} N\left(0, \sqrt{I_{1}\left(\theta_{0}\right)}\right) \sim N\left(0, \sqrt{\frac{1}{I_{1}\left(\theta_{0}\right)}}\right)$

\end{proof}

\begin{observación}[Simplificación de las condiciones de regularidad]
Las condiciones de regularidad del teorema anterior pueden simplificarse a las siguientes:\\
\begin{enumerate}
  \item $\phi (x, \theta) = \frac{\partial}{\partial \theta} \log f_{\theta}(x)$ tiene dos derivadas continuas respecto de $\theta$.
  \item $\hat{\theta}_n \xrightarrow[n \to \infty]{P} \theta_0$
  \item $E_{\theta_0}[\varphi^2(x, \theta_0)] = I_1(\theta_0) < \infty$
  \item $E_{\theta_0} \left[ \frac{\partial}{\partial \theta} \varphi(x, \theta_0) \right] = - I_1(\theta_0) \neq 0$
  \item $\exists \, \varepsilon > 0 \text{ y } M(x) \text{ tales que } \sup_{|\theta - \theta_0| \leq \varepsilon} \left| \frac{\partial^2}{\partial \theta^2} \varphi(x, \theta) \right| \leq M(x)$ con $M(x) < \infty$
\end{enumerate}
\end{observación}